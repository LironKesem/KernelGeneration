/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
INFO:__main__:Using config from TRITONBENCH_RUN_CONFIG: /home/nvme/fedora/lkesem/KernelGeneration/benchmark_helion_runner.yaml
INFO:__main__:Expecting 3 output file(s):
INFO:__main__:  - bf16_layernorm: benchmark_bf16_layernorm.json
INFO:__main__:  - bf16_matmul: benchmark_bf16_matmul.json
INFO:__main__:  - bf16_gemm_gelu: benchmark_bf16_gemm_gelu.json
INFO:__main__:Cleared 1 TorchInductor cache directory
INFO:__main__:Starting benchmark run...
INFO:tritonbench.utils.run_utils:[tritonbench] Running helion benchmark: /home/nvme/fedora/miniconda3/envs/lkesem-tritonenv/bin/python benchmarks/run.py --device=cuda --precision bf16 --kernel-config ../custom_kernel_config.yaml --kernel bf16_layernorm --output benchmark_bf16_layernorm.json
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
Loading custom kernel configuration from: ../custom_kernel_config.yaml
Loaded 3 kernel mapping(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Loaded metric mappings for 3 kernel(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Using num_inputs=20 for bf16_layernorm
Running bf16_layernorm benchmark with Helion implementation...

WARNING:tritonbench.utils.triton_op:First-k mode: Selected 3 sequential inputs starting from index 0 (total available: 3)
WARNING:tritonbench.utils.triton_op:Input IDs to run: [0, 1, 2]
Using HIP autotune config
  0%|          | 0/3 [00:00<?, ?it/s]WARNING:tritonbench.utils.triton_op:Running input ID 0:
(M, D)
----------
(512, 512)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_layernorm
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for kernelllm_layernorm
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_layernorm
INFO:tritonbench.utils.triton_op:Took 19.24ms to get benchmark function for torch_compile_max_layernorm
INFO:tritonbench.utils.triton_op:Took 0.84ms to get benchmark function for torch_compile_default_layernorm
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512, 512),
              'stride': (512, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512,),
              'stride': (1,)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512,),
              'stride': (1,)},
            1e-05),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 0.48ms to get benchmark function for helion_layer_norm_tritonbench
[0s] Autotune random seed: 662840672
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 8.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 17.3 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 35.0 configs/s
[46s] Initial random population of 100, 5 starting points: ok=100 min=0.0117 mid=0.0214 max=1.8436 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[46s] Generation 1 starting: 136 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136/136 43.1 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136/136 17.8 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[166s] Generation 1 complete: ok=141 min=0.0114 mid=0.0123 max=0.0416 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[166s] Generation 2 starting: 122 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 42.0 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 18.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.2 configs/s
[275s] Generation 2 complete: ok=127 min=0.0116 mid=0.0122 max=0.0417 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[275s] Generation 3 starting: 120 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 53.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.1 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.7 configs/s
[381s] Generation 3 complete: ok=125 min=0.0116 mid=0.0122 max=0.0307 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[381s] Generation 4 starting: 123 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 46.5 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 18.1 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.3 configs/s
[489s] Generation 4 complete: ok=128 min=0.0115 mid=0.0121 max=0.0306 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[489s] Generation 5 starting: 121 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 52.7 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 18.1 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.4 configs/s
[596s] Generation 5 complete: ok=126 min=0.0114 mid=0.0121 max=0.0304 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[596s] Generation 6 starting: 120 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 46.8 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.1 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.5 configs/s
[701s] Generation 6 complete: ok=125 min=0.0114 mid=0.0121 max=0.0305 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[701s] Generation 7 starting: 118 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 47.5 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 18.1 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.8 configs/s
[805s] Generation 7 complete: ok=123 min=0.0115 mid=0.0121 max=0.0306 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[805s] Generation 8 starting: 117 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 54.8 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 18.1 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.9 configs/s
[908s] Generation 8 complete: ok=122 min=0.0114 mid=0.0121 max=0.0305 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[908s] Generation 9 starting: 117 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 51.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 18.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.9 configs/s
[1010s] Generation 9 complete: ok=122 min=0.0115 mid=0.0121 max=0.0305 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1010s] Generation 10 starting: 120 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 47.0 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.6 configs/s
[1116s] Generation 10 complete: ok=125 min=0.0115 mid=0.0120 max=0.0304 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1116s] Generation 11 starting: 120 neighbors, 5 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 54.4 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.1 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.5 configs/s
[1221s] Generation 11 complete: ok=125 min=0.0116 mid=0.0121 max=0.0306 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1221s] Generation 12 starting: 118 neighbors, 5 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 44.8 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 18.1 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.9 configs/s
[1325s] Generation 12 complete: ok=123 min=0.0116 mid=0.0121 max=0.0306 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1325s] Generation 13 starting: 119 neighbors, 5 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 54.5 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 18.1 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.7 configs/s
[1430s] Generation 13 complete: ok=124 min=0.0115 mid=0.0122 max=0.0306 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1430s] Generation 14 starting: 119 neighbors, 5 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 46.9 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 18.1 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.8 configs/s
[1534s] Generation 14 complete: ok=124 min=0.0114 mid=0.0122 max=0.0305 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1534s] Generation 15 starting: 114 neighbors, 5 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114/114 47.9 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114/114 18.2 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.3 configs/s
[1634s] Generation 15 complete: ok=119 min=0.0115 mid=0.0120 max=0.0306 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1634s] Generation 16 starting: 113 neighbors, 5 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 45.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 18.1 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.5 configs/s
[1733s] Generation 16 complete: ok=118 min=0.0115 mid=0.0120 max=0.0308 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1733s] Generation 17 starting: 117 neighbors, 5 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 43.5 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 18.2 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.8 configs/s
[1836s] Generation 17 complete: ok=122 min=0.0115 mid=0.0120 max=0.0306 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1836s] Generation 18 starting: 120 neighbors, 5 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 55.3 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.2 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.7 configs/s
[1942s] Generation 18 complete: ok=125 min=0.0115 mid=0.0120 max=0.0305 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1942s] Generation 19 starting: 116 neighbors, 5 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 52.5 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 18.2 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.0 configs/s
[2043s] Generation 19 complete: ok=121 min=0.0114 mid=0.0120 max=0.0305 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', '', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[2043s] Generation 20 starting: 115 neighbors, 5 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 49.5 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 18.0 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.1 configs/s
[2144s] Generation 20 complete: ok=120 min=0.0115 mid=0.0121 max=0.0306 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', 'first', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[2144s] Autotuning complete in 2144.4s after searching 2485 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', 'last', 'first', 'first'], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 0:
(M, D)
----------
(512, 512)
 33%|███▎      | 1/3 [35:48<1:11:36, 2148.20s/it]WARNING:tritonbench.utils.triton_op:Running input ID 1:
(M, D)
----------
(32, 1024)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_layernorm
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for kernelllm_layernorm
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_layernorm
INFO:tritonbench.utils.triton_op:Took 1.25ms to get benchmark function for torch_compile_max_layernorm
INFO:tritonbench.utils.triton_op:Took 0.60ms to get benchmark function for torch_compile_default_layernorm
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (32, 1024),
              'stride': (1024, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024,),
              'stride': (1,)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024,),
              'stride': (1,)},
            1e-05),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 2.11ms to get benchmark function for helion_layer_norm_tritonbench
[0s] Autotune random seed: 662840672
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 37.6 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 17.7 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 20.3 configs/s
[59s] Initial random population of 100, 5 starting points: ok=100 min=0.0074 mid=0.0104 max=0.0340 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[59s] Generation 1 starting: 128 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 37.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 18.0 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 8.9 configs/s
[185s] Generation 1 complete: ok=133 min=0.0071 mid=0.0074 max=0.0084 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[185s] Generation 2 starting: 120 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 43.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[303s] Generation 2 complete: ok=125 min=0.0070 mid=0.0074 max=0.0084 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', 'first', 'last', 'first'], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[303s] Generation 3 starting: 117 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 50.4 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 18.2 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.7 configs/s
[418s] Generation 3 complete: ok=122 min=0.0070 mid=0.0075 max=0.0084 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[418s] Generation 4 starting: 118 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 49.6 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 18.2 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[534s] Generation 4 complete: ok=123 min=0.0070 mid=0.0075 max=0.0084 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', 'first', 'last', 'first'], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[534s] Generation 5 starting: 118 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 46.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 18.2 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[650s] Generation 5 complete: ok=123 min=0.0070 mid=0.0075 max=0.0084 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[650s] Generation 6 starting: 95 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95/95 42.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95/95 18.2 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.8 configs/s
[744s] Generation 6 complete: ok=100 min=0.0070 mid=0.0075 max=0.0084 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', 'first', 'last', 'first'], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[744s] Generation 7 starting: 75 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 44.2 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 18.1 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.1 configs/s
[819s] Generation 7 complete: ok=79 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', '', 'last', ''], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[819s] Generation 8 starting: 76 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 45.8 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 18.2 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 14.8 configs/s
[893s] Generation 8 complete: ok=79 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', '', 'last', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[893s] Generation 9 starting: 74 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 41.6 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 18.2 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.2 configs/s
[966s] Generation 9 complete: ok=77 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['first', '', 'last', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[966s] Generation 10 starting: 75 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 45.2 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 18.2 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.0 configs/s
[1040s] Generation 10 complete: ok=78 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['first', '', 'last', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1040s] Generation 11 starting: 75 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 49.9 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 18.2 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 14.9 configs/s
[1113s] Generation 11 complete: ok=78 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['first', '', 'first', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1113s] Generation 12 starting: 74 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 44.5 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 18.2 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.2 configs/s
[1185s] Generation 12 complete: ok=77 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', '', 'first', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1185s] Generation 13 starting: 71 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 45.7 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.2 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.0 configs/s
[1255s] Generation 13 complete: ok=74 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', '', 'first', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1255s] Generation 14 starting: 71 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 43.3 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.1 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.8 configs/s
[1325s] Generation 14 complete: ok=74 min=0.0072 mid=0.0076 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', '', '', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1325s] Generation 15 starting: 72 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 43.1 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 18.4 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.6 configs/s
[1396s] Generation 15 complete: ok=75 min=0.0072 mid=0.0076 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', 'last', '', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1396s] Generation 16 starting: 71 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 42.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.4 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.0 configs/s
[1466s] Generation 16 complete: ok=74 min=0.0072 mid=0.0076 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', 'last', 'last', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1466s] Generation 17 starting: 70 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70/70 44.4 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70/70 18.3 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.0 configs/s
[1535s] Generation 17 complete: ok=73 min=0.0072 mid=0.0076 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', 'first', 'last', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1535s] Generation 18 starting: 72 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 45.8 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 18.1 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.5 configs/s
[1606s] Generation 18 complete: ok=75 min=0.0072 mid=0.0075 max=0.0084 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', 'first', 'last', 'first'], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1606s] Generation 19 starting: 71 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 45.8 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.2 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.9 configs/s
[1676s] Generation 19 complete: ok=74 min=0.0072 mid=0.0076 max=0.0085 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', 'first', 'last', ''], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1676s] Generation 20 starting: 66 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 21.0 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66/66 18.2 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.9 configs/s
[1744s] Generation 20 complete: ok=69 min=0.0075 mid=0.0078 max=0.0087 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', 'first', 'first', ''], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1744s] Autotuning complete in 1744.7s after searching 1809 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', 'first', 'first', ''], num_stages=3, num_warps=32, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 1:
(M, D)
----------
(32, 1024)
 67%|██████▋   | 2/3 [1:04:55<31:52, 1912.53s/it]WARNING:tritonbench.utils.triton_op:Running input ID 2:
(M, D)
------------
(2048, 2048)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_layernorm
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_layernorm
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_layernorm
INFO:tritonbench.utils.triton_op:Took 2.64ms to get benchmark function for torch_compile_max_layernorm
INFO:tritonbench.utils.triton_op:Took 1.04ms to get benchmark function for torch_compile_default_layernorm
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048, 2048),
              'stride': (2048, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048,),
              'stride': (1,)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048,),
              'stride': (1,)},
            1e-05),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 3.65ms to get benchmark function for helion_layer_norm_tritonbench
[0s] Autotune random seed: 662840672
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[64s] Timeout after 60s compiling Config(block_sizes=[1024], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['first', 'last', '', 'first'], num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[False], range_multi_buffers=[None], range_num_stages=[2], range_unroll_factors=[4], range_warp_specializes=[False], reduction_loops=[64])
[65s] Timeout after 60s compiling Config(block_sizes=[256], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'last'], num_stages=3, num_warps=8, pid_type='persistent_blocked', range_flattens=[True], range_multi_buffers=[None], range_num_stages=[3], range_unroll_factors=[3], range_warp_specializes=[None], reduction_loops=[512])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 16.4 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 23.9 configs/s
[113s] Initial random population of 100, 5 starting points: timeout=2 ok=98 min=0.0891 mid=0.1397 max=14.1833 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[113s] Generation 1 starting: 124 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 39.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 17.6 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.2 configs/s
[235s] Generation 1 complete: ok=129 min=0.0889 mid=0.0904 max=0.6890 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[235s] Generation 2 starting: 115 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 34.9 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 17.7 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.3 configs/s
[354s] Generation 2 complete: ok=120 min=0.0884 mid=0.0897 max=0.6658 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[354s] Generation 3 starting: 112 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 41.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 17.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.4 configs/s
[472s] Generation 3 complete: ok=117 min=0.0882 mid=0.0895 max=0.1204 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[472s] Generation 4 starting: 111 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111/111 41.2 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111/111 17.9 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[588s] Generation 4 complete: ok=116 min=0.0882 mid=0.0895 max=0.1207 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[588s] Generation 5 starting: 112 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 45.6 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 18.0 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[705s] Generation 5 complete: ok=117 min=0.0881 mid=0.0894 max=0.1203 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[705s] Generation 6 starting: 105 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 41.1 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 17.9 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.1 configs/s
[816s] Generation 6 complete: ok=110 min=0.0881 mid=0.0890 max=0.1236 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[816s] Generation 7 starting: 104 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104/104 46.3 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104/104 18.0 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.1 configs/s
[925s] Generation 7 complete: ok=109 min=0.0881 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[925s] Generation 8 starting: 106 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 45.1 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 18.0 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.9 configs/s
[1036s] Generation 8 complete: ok=111 min=0.0881 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1036s] Generation 9 starting: 103 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 44.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 18.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.1 configs/s
[1145s] Generation 9 complete: ok=108 min=0.0880 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['last', '', 'first', 'first'], num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1145s] Generation 10 starting: 100 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 43.4 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 18.0 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.5 configs/s
[1250s] Generation 10 complete: ok=105 min=0.0880 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1250s] Generation 11 starting: 106 neighbors, 5 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 46.8 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 18.0 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.9 configs/s
[1361s] Generation 11 complete: ok=111 min=0.0880 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['last', 'last', 'first', 'first'], num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1361s] Generation 12 starting: 103 neighbors, 5 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 45.8 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 18.0 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.1 configs/s
[1469s] Generation 12 complete: ok=108 min=0.0880 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1469s] Generation 13 starting: 103 neighbors, 5 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 47.4 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 18.0 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.1 configs/s
[1577s] Generation 13 complete: ok=108 min=0.0880 mid=0.0891 max=0.1233 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1577s] Generation 14 starting: 102 neighbors, 5 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 46.1 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 18.0 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.3 configs/s
[1684s] Generation 14 complete: ok=107 min=0.0880 mid=0.0891 max=0.1233 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1684s] Generation 15 starting: 102 neighbors, 5 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 45.5 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 18.0 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.3 configs/s
[1791s] Generation 15 complete: ok=107 min=0.0881 mid=0.0891 max=0.1235 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1791s] Generation 16 starting: 103 neighbors, 5 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 43.7 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 18.0 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.2 configs/s
[1900s] Generation 16 complete: ok=108 min=0.0880 mid=0.0891 max=0.1235 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[1900s] Generation 17 starting: 102 neighbors, 5 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 46.0 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 18.0 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.4 configs/s
[2007s] Generation 17 complete: ok=107 min=0.0881 mid=0.0892 max=0.1234 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[2007s] Generation 18 starting: 103 neighbors, 5 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 44.2 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 18.0 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.2 configs/s
[2115s] Generation 18 complete: ok=108 min=0.0881 mid=0.0891 max=0.1234 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'first'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[2115s] Generation 19 starting: 102 neighbors, 5 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 44.0 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 18.0 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.3 configs/s
[2222s] Generation 19 complete: ok=107 min=0.0881 mid=0.0891 max=0.1235 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'last'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[2222s] Generation 20 starting: 101 neighbors, 5 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101/101 43.3 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101/101 18.0 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.5 configs/s
[2328s] Generation 20 complete: ok=106 min=0.0881 mid=0.0891 max=0.1236 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'last'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None])
[2328s] Autotuning complete in 2329.0s after searching 2217 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'last'], num_stages=6, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[None], reduction_loops=[None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 2:
(M, D)
------------
(2048, 2048)
100%|██████████| 3/3 [1:43:49<00:00, 2104.83s/it]100%|██████████| 3/3 [1:43:49<00:00, 2076.48s/it]
INFO:tritonbench.utils.run_utils:[tritonbench] Output result csv to /tmp/tmpsyo46l96.csv
INFO:__main__:ignoring torch_layernorm-latency
INFO:__main__:ignoring torch_layernorm-best_config
INFO:__main__:ignoring torch_layernorm-gbps
INFO:__main__:ignoring triton_layernorm-best_config
INFO:__main__:ignoring kernelllm_layernorm-best_config
INFO:__main__:ignoring mako_layernorm-best_config
INFO:__main__:ignoring torch_compile_max_layernorm-best_config
INFO:__main__:ignoring torch_compile_default_layernorm-best_config
INFO:__main__:ignoring helion_layer_norm_tritonbench-best_config
INFO:__main__:ignoring torch_layernorm-latency
INFO:__main__:ignoring torch_layernorm-best_config
INFO:__main__:ignoring torch_layernorm-gbps
INFO:__main__:ignoring triton_layernorm-best_config
INFO:__main__:ignoring kernelllm_layernorm-best_config
INFO:__main__:ignoring mako_layernorm-best_config
INFO:__main__:ignoring torch_compile_max_layernorm-best_config
INFO:__main__:ignoring torch_compile_default_layernorm-best_config
INFO:__main__:ignoring helion_layer_norm_tritonbench-best_config
INFO:__main__:ignoring torch_layernorm-latency
INFO:__main__:ignoring torch_layernorm-best_config
INFO:__main__:ignoring torch_layernorm-gbps
INFO:__main__:ignoring triton_layernorm-best_config
INFO:__main__:ignoring kernelllm_layernorm-best_config
INFO:__main__:ignoring mako_layernorm-best_config
INFO:__main__:ignoring torch_compile_max_layernorm-best_config
INFO:__main__:ignoring torch_compile_default_layernorm-best_config
INFO:__main__:ignoring helion_layer_norm_tritonbench-best_config
      (M, D)    torch_layernorm-latency    torch_layernorm-best_config    torch_layernorm-gbps    triton_layernorm-latency    triton_layernorm-accuracy                                         triton_layernorm-best_config    triton_layernorm-gbps    kernelllm_layernorm-latency    kernelllm_layernorm-accuracy    kernelllm_layernorm-best_config    kernelllm_layernorm-gbps    mako_layernorm-latency    mako_layernorm-accuracy    mako_layernorm-best_config    mako_layernorm-gbps    torch_compile_max_layernorm-latency    torch_compile_max_layernorm-accuracy    torch_compile_max_layernorm-best_config    torch_compile_max_layernorm-gbps    torch_compile_default_layernorm-latency    torch_compile_default_layernorm-accuracy    torch_compile_default_layernorm-best_config    torch_compile_default_layernorm-gbps    helion_layer_norm_tritonbench-latency    helion_layer_norm_tritonbench-accuracy    helion_layer_norm_tritonbench-best_config    helion_layer_norm_tritonbench-gbps
------------  -------------------------  -----------------------------  ----------------------  --------------------------  ---------------------------  -------------------------------------------------------------------  -----------------------  -----------------------------  ------------------------------  ---------------------------------  --------------------------  ------------------------  -------------------------  ----------------------------  ---------------------  -------------------------------------  --------------------------------------  -----------------------------------------  ----------------------------------  -----------------------------------------  ------------------------------------------  ---------------------------------------------  --------------------------------------  ---------------------------------------  ----------------------------------------  -------------------------------------------  ------------------------------------
  (512, 512)          0.015000 (±3.73%)                                                69.9051           0.014000 (±2.57%)                            1  {'BLOCK_SIZE': 256, 'num_warps': 4, 'num_ctas': 1, 'num_stages': 2}                  74.8983             0.027000 (±44.00%)                               1                                                        38.8361         0.012480 (±3.21%)                          1                                              84.0205                      0.031360 (±3.83%)                                       1                                                                       33.4367                           0.024720 (±8.74%)                                           1                                                                                42.4181                       0.011960 (±10.03%)                                         1                                                                            87.6736
  (32, 1024)          0.007960 (±3.02%)                                                16.4663           0.007840 (±3.57%)                            1  {'BLOCK_SIZE': 512, 'num_warps': 8, 'num_ctas': 1, 'num_stages': 2}                  16.7184             0.011040 (±21.01%)                               1                                                        11.8725        0.007521 (±14.35%)                          1                                              17.4275                   0.030080 (±5598.75%)                                       1                                                                        4.35745                          0.007520 (±7.46%)                                           1                                                                                17.4298                        0.007360 (±6.52%)                                         1                                                                            17.8087
(2048, 2048)          0.091401 (±1.36%)                                               183.556            0.096041 (±1.12%)                            1  {'BLOCK_SIZE': 512, 'num_warps': 8, 'num_ctas': 1, 'num_stages': 2}                 174.688               0.093121 (±0.95%)                               1                                                       180.166          0.091401 (±0.96%)                          1                                             183.556                    0.145362 (±2883.35%)                                       1                                                                      115.417                            0.089641 (±1.34%)                                           1                                                                               187.16                          0.088361 (±0.77%)                                         1                                                                           189.871
     average       0.038120334347089134                                                89.9758        0.039293667301535606                            1                                                                                       88.7682            0.04372033352653185                               1                                                        76.9581       0.03713400118673841                          1                                              95.0014                    0.06893400164941947                                       1                                                                       51.0703                         0.04062699914599458                                           1                                                                                82.336                       0.03589366748929024                                         1                                                                            98.4512
INFO:tritonbench.utils.run_utils:[tritonbench] Running helion benchmark: /home/nvme/fedora/miniconda3/envs/lkesem-tritonenv/bin/python benchmarks/run.py --device=cuda --precision bf16 --kernel-config ../custom_kernel_config.yaml --kernel bf16_matmul --num-inputs 30 --output benchmark_bf16_matmul.json
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
Loading custom kernel configuration from: ../custom_kernel_config.yaml
Loaded 3 kernel mapping(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Loaded metric mappings for 3 kernel(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Running bf16_matmul benchmark with Helion implementation...

WARNING:tritonbench.utils.triton_op:First-k mode: Selected 10 sequential inputs starting from index 0 (total available: 10)
WARNING:tritonbench.utils.triton_op:Input IDs to run: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
Using HIP autotune config
Using HIP autotune config
Using HIP autotune config
  0%|          | 0/10 [00:00<?, ?it/s]WARNING:tritonbench.utils.triton_op:Running input ID 0:
(M, K, N)
------------------
(2944, 3072, 3200)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_14", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4", "best_time": 1.0282939672470093, "best_triton_pos": 0}
AUTOTUNE mm(2944x3072, 3072x3200)
strides: [3072, 1], [3200, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_14 1.0283 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_27 1.0451 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_26 1.0453 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_30 1.0717 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_28 1.1312 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_23 1.1741 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_34 1.1756 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_25 1.2691 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_21 1.2907 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_24 1.2923 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.7328 seconds and 1.1787 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3833.99ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 37.55ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2944, 3072),
              'stride': (3072, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3072, 3200),
              'stride': (3200, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 0.47ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[61s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 3.0 configs/s
[1995s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=1.6312 mid=115.9433 max=106334.7031 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[1995s] Generation 1 starting: 199 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 24.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 8.2 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 14.0 configs/s
[2039s] Generation 1 complete: error=13 ok=191 min=1.3421 mid=3.8711 max=302.0560 best=Config(block_sizes=[256, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2039s] Generation 2 starting: 173 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 31.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 8.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 7.4 configs/s
[2089s] Generation 2 complete: error=5 ok=173 min=1.3407 mid=2.2802 max=344.0001 best=Config(block_sizes=[256, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2089s] Generation 3 starting: 168 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 15.1 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 13.7 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196/196 8.1 configs/s
[2137s] Generation 3 complete: error=6 ok=167 min=1.0195 mid=1.7212 max=18.8658 best=Config(block_sizes=[64, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[2137s] Generation 4 starting: 181 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181/181 18.1 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181/181 13.5 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 8.3 configs/s
[2185s] Generation 4 complete: error=12 ok=174 min=1.0014 mid=1.6456 max=16.8357 best=Config(block_sizes=[64, 128, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[2185s] Generation 5 starting: 178 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178/178 18.8 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178/178 13.4 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 7.0 configs/s
[2238s] Generation 5 complete: error=6 ok=177 min=0.9774 mid=1.6490 max=17.8325 best=Config(block_sizes=[64, 128, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[2238s] Generation 6 starting: 187 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187/187 18.5 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 187/187 13.4 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 6.9 configs/s
[2294s] Generation 6 complete: error=8 ok=184 min=0.9722 mid=1.6350 max=18.2067 best=Config(block_sizes=[64, 128, 16], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[2294s] Generation 7 starting: 106 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 20.1 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 13.7 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207/207 9.7 configs/s
[2329s] Generation 7 complete: error=2 ok=107 min=0.9673 mid=1.4695 max=14.5657 best=Config(block_sizes=[64, 128, 16], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2329s] Generation 8 starting: 105 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 19.7 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 13.1 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208/208 9.4 configs/s
[2365s] Generation 8 complete: ok=108 min=0.9670 mid=1.4836 max=14.4912 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2365s] Generation 9 starting: 107 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 28.2 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 13.2 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 10.5 configs/s
[2399s] Generation 9 complete: ok=110 min=0.9647 mid=1.4344 max=14.5813 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2399s] Generation 10 starting: 106 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 20.5 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 13.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 10.0 configs/s
[2434s] Generation 10 complete: ok=109 min=0.9667 mid=1.4525 max=14.5559 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2434s] Generation 11 starting: 108 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108/108 27.5 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108/108 13.4 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 9.9 configs/s
[2469s] Generation 11 complete: ok=111 min=0.9693 mid=1.4345 max=14.5511 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2469s] Generation 12 starting: 72 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 19.9 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 13.6 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 16.7 configs/s
[2492s] Generation 12 complete: ok=74 min=0.9744 mid=1.5450 max=12.5040 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2492s] Generation 13 starting: 36 neighbors, 1 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 22.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 14.1 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 35.8 configs/s
[2503s] Generation 13 complete: ok=38 min=1.3253 mid=1.6390 max=5.5712 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2503s] Generation 14 starting: 35 neighbors, 1 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 18.5 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 14.0 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 38.6 configs/s
[2514s] Generation 14 complete: ok=37 min=1.3295 mid=1.6486 max=4.9361 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2514s] Autotuning complete in 2514.9s after searching 1860 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 0:
(M, K, N)
------------------
(2944, 3072, 3200)
 10%|█         | 1/10 [42:02<6:18:21, 2522.36s/it]WARNING:tritonbench.utils.triton_op:Running input ID 1:
(M, K, N)
------------------
(3072, 3200, 3328)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_62", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 1.1384170055389404, "best_triton_pos": 0}
AUTOTUNE mm(3072x3200, 3200x3328)
strides: [3200, 1], [3328, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_62 1.1384 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_59 1.1649 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_63 1.1667 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_69 1.1751 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_66 1.1761 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_50 1.1878 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_64 1.2478 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_60 1.2621 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_61 1.2662 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_70 1.2889 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.9774 seconds and 1.0610 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3520.91ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 31.83ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3072, 3200),
              'stride': (3200, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3200, 3328),
              'stride': (3328, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 2.77ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[61s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 2.4 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99/99 - configs/s
[2460s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=1.9581 mid=133.5153 max=140734.1562 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2460s] Generation 1 starting: 205 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 10.1 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 7.4 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148/148 17.9 configs/s
[2515s] Generation 1 complete: error=12 ok=198 min=1.1925 mid=4.4854 max=177.4601 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2515s] Generation 2 starting: 184 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 24.7 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 13.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167/167 14.9 configs/s
[2549s] Generation 2 complete: error=8 ok=181 min=1.1809 mid=2.5086 max=20.2187 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2549s] Generation 3 starting: 143 neighbors, 4 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 23.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 12.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 9.4 configs/s
[2585s] Generation 3 complete: error=4 ok=144 min=1.1881 mid=2.3241 max=25.5483 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2585s] Generation 4 starting: 154 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154/154 23.4 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154/154 13.1 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 8.1 configs/s
[2627s] Generation 4 complete: error=2 ok=157 min=1.1962 mid=1.8785 max=17.0450 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2627s] Generation 5 starting: 145 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145/145 14.0 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145/145 12.0 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 6.2 configs/s
[2683s] Generation 5 complete: error=1 ok=149 min=1.2002 mid=1.5477 max=33.4145 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2683s] Generation 6 starting: 143 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 13.5 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 11.4 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 5.1 configs/s
[2744s] Generation 6 complete: ok=148 min=1.0303 mid=1.4183 max=35.1072 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2744s] Generation 7 starting: 108 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108/108 11.8 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108/108 11.3 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 7.2 configs/s
[2791s] Generation 7 complete: error=1 ok=111 min=1.3197 mid=1.3760 max=34.8361 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2791s] Generation 8 starting: 104 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104/104 11.5 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104/104 11.8 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 7.5 configs/s
[2836s] Generation 8 complete: error=1 ok=106 min=1.0274 mid=1.3849 max=21.6727 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2836s] Generation 9 starting: 72 neighbors, 2 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 22.5 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 11.7 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 10.9 configs/s
[2864s] Generation 9 complete: ok=75 min=1.3234 mid=1.4221 max=21.8873 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2864s] Generation 10 starting: 72 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 9.6 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 11.4 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 11.0 configs/s
[2897s] Generation 10 complete: ok=75 min=1.3254 mid=1.4198 max=22.2446 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2897s] Generation 11 starting: 71 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 24.5 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 11.5 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 11.5 configs/s
[2925s] Generation 11 complete: ok=74 min=1.3282 mid=1.4516 max=22.3049 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2925s] Generation 12 starting: 73 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73/73 15.7 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73/73 11.4 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 11.1 configs/s
[2955s] Generation 12 complete: ok=76 min=1.3281 mid=1.4297 max=22.0245 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2955s] Generation 13 starting: 69 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 23.9 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 11.4 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 11.8 configs/s
[2982s] Generation 13 complete: ok=72 min=1.3289 mid=1.4594 max=22.1420 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2982s] Generation 14 starting: 71 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 23.2 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 11.5 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 11.1 configs/s
[3010s] Generation 14 complete: ok=74 min=1.3307 mid=1.4591 max=22.0217 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3010s] Generation 15 starting: 69 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 24.2 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 11.4 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 11.6 configs/s
[3038s] Generation 15 complete: ok=72 min=1.3320 mid=1.4614 max=22.0676 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3038s] Generation 16 starting: 33 neighbors, 1 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 33.7 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 10.8 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 26.7 configs/s
[3050s] Generation 16 complete: ok=35 min=1.3282 mid=1.5965 max=22.2453 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3050s] Generation 17 starting: 36 neighbors, 1 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 10.3 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 11.0 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 27.8 configs/s
[3065s] Generation 17 complete: ok=38 min=1.3305 mid=1.3955 max=22.6676 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3065s] Generation 18 starting: 34 neighbors, 1 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 31.8 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 11.0 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 23.7 configs/s
[3078s] Generation 18 complete: ok=36 min=1.3336 mid=1.4787 max=22.5815 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3078s] Generation 19 starting: 33 neighbors, 1 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 32.7 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 10.9 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 30.1 configs/s
[3090s] Generation 19 complete: ok=35 min=1.3316 mid=1.4764 max=22.6374 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3090s] Generation 20 starting: 36 neighbors, 1 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 9.2 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 10.9 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 19.2 configs/s
[3106s] Generation 20 complete: ok=38 min=1.3313 mid=1.4019 max=24.4338 best=Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3106s] Autotuning complete in 3106.1s after searching 1954 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[64, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 1:
(M, K, N)
------------------
(3072, 3200, 3328)
 20%|██        | 2/10 [1:33:54<6:22:33, 2869.17s/it]WARNING:tritonbench.utils.triton_op:Running input ID 2:
(M, K, N)
------------------
(3200, 3328, 3456)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_98", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 1.3012590408325195, "best_triton_pos": 0}
AUTOTUNE mm(3200x3328, 3328x3456)
strides: [3328, 1], [3456, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_98 1.3013 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_99 1.3188 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_86 1.3362 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_95 1.3518 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_105 1.3549 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_102 1.3618 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_97 1.4345 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_100 1.4359 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_96 1.4484 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_106 1.4730 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.9631 seconds and 0.9732 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3418.78ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 32.19ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3200, 3328),
              'stride': (3328, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3328, 3456),
              'stride': (3456, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 3.34ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[61s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 2.3 configs/s
[3408s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=1.9269 mid=150.2815 max=226166.0469 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3408s] Generation 1 starting: 205 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 16.9 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 4.8 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 11.1 configs/s
[3467s] Generation 1 complete: error=13 ok=197 min=1.6657 mid=5.0905 max=383.1503 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3467s] Generation 2 starting: 181 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181/181 19.4 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181/181 7.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 6.6 configs/s
[3523s] Generation 2 complete: error=7 ok=179 min=1.5495 mid=2.9111 max=451.8714 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3523s] Generation 3 starting: 171 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 18.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 13.3 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138/138 4.9 configs/s
[3574s] Generation 3 complete: error=11 ok=165 min=1.3931 mid=2.3070 max=21.2255 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3574s] Generation 4 starting: 167 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167/167 16.0 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167/167 13.1 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 4.3 configs/s
[3632s] Generation 4 complete: error=10 ok=162 min=1.3913 mid=1.9629 max=18.8861 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3632s] Generation 5 starting: 153 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153/153 16.6 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153/153 12.9 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 4.2 configs/s
[3688s] Generation 5 complete: error=12 ok=146 min=1.4024 mid=1.8662 max=18.8536 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3688s] Generation 6 starting: 137 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137/137 14.6 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137/137 12.5 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 4.9 configs/s
[3740s] Generation 6 complete: error=11 ok=131 min=1.3990 mid=1.7776 max=25.3165 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3740s] Generation 7 starting: 147 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147/147 15.2 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147/147 12.4 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 4.8 configs/s
[3793s] Generation 7 complete: error=11 ok=141 min=1.3999 mid=1.8701 max=26.6894 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3793s] Generation 8 starting: 146 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146/146 15.1 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146/146 12.3 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 4.9 configs/s
[3846s] Generation 8 complete: error=11 ok=140 min=1.3996 mid=1.8716 max=27.4451 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3846s] Generation 9 starting: 109 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 14.5 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 12.6 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 6.7 configs/s
[3886s] Generation 9 complete: error=11 ok=102 min=1.4010 mid=1.8667 max=27.4821 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3886s] Generation 10 starting: 35 neighbors, 1 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 20.2 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 10.5 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 17.4 configs/s
[3899s] Generation 10 complete: ok=37 min=1.4130 mid=1.5328 max=25.8852 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3899s] Autotuning complete in 3899.6s after searching 1550 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 2:
(M, K, N)
------------------
(3200, 3328, 3456)
 30%|███       | 3/10 [2:38:59<6:29:56, 3342.33s/it]WARNING:tritonbench.utils.triton_op:Running input ID 3:
(M, K, N)
------------------
(3328, 3456, 3584)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_134", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 1.420220971107483, "best_triton_pos": 0}
AUTOTUNE mm(3328x3456, 3456x3584)
strides: [3456, 1], [3584, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_134 1.4202 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_141 1.4502 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_135 1.4611 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_131 1.4643 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_122 1.4849 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_138 1.4971 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_133 1.5793 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_132 1.5807 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_136 1.6010 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_142 1.6595 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 2.2076 seconds and 0.9695 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3658.18ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 31.55ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3328, 3456),
              'stride': (3456, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3456, 3584),
              'stride': (3584, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 2.91ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[61s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 1.8 configs/s
[3035s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=1.6615 mid=179.9649 max=167640.8125 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3035s] Generation 1 starting: 199 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 8.4 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 7.8 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126/126 12.4 configs/s
[3095s] Generation 1 complete: error=13 ok=191 min=1.5528 mid=5.6623 max=108.9296 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3095s] Generation 2 starting: 142 neighbors, 4 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142/142 16.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142/142 11.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 - configs/s
[3120s] Generation 2 complete: error=6 ok=141 min=1.5652 mid=4.1671 max=49.2481 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3120s] Generation 3 starting: 142 neighbors, 4 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142/142 14.8 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142/142 11.9 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 7.5 configs/s
[3160s] Generation 3 complete: error=4 ok=143 min=1.5683 mid=2.7506 max=32.4347 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3160s] Generation 4 starting: 138 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138/138 9.5 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138/138 12.0 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 3.9 configs/s
[3220s] Generation 4 complete: error=6 ok=137 min=1.5775 mid=1.9951 max=26.1904 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3220s] Generation 5 starting: 140 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140/140 9.5 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140/140 10.9 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 4.4 configs/s
[3278s] Generation 5 complete: error=6 ok=138 min=1.5569 mid=2.0492 max=31.9958 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3278s] Generation 6 starting: 151 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 9.8 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 10.3 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 4.3 configs/s
[3340s] Generation 6 complete: error=5 ok=150 min=1.5558 mid=1.9997 max=35.8201 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3340s] Generation 7 starting: 104 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104/104 8.4 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104/104 11.0 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 6.5 configs/s
[3383s] Generation 7 complete: ok=108 min=1.5666 mid=1.8854 max=28.7545 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3383s] Generation 8 starting: 105 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 8.3 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 10.7 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 6.0 configs/s
[3427s] Generation 8 complete: error=1 ok=108 min=1.5712 mid=1.8919 max=28.7083 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3427s] Generation 9 starting: 113 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 8.4 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━ 113/113 10.6 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━ 129/129 6.4 configs/s
[3472s] Generation 9 complete: error=1 ok=116 min=1.5736 mid=1.9866 max=29.0588 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3472s] Generation 10 starting: 113 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 8.4 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━ 113/113 10.4 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 6.5 configs/s
[3517s] Generation 10 complete: error=1 ok=116 min=1.5706 mid=1.9886 max=44.0000 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3517s] Generation 11 starting: 111 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111/111 8.3 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━ 111/111 10.5 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 6.5 configs/s
[3562s] Generation 11 complete: error=1 ok=114 min=1.5727 mid=2.1054 max=29.0121 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3562s] Generation 12 starting: 111 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111/111 8.2 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━ 111/111 10.5 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 6.3 configs/s
[3606s] Generation 12 complete: error=1 ok=114 min=1.5784 mid=2.1039 max=29.0611 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3606s] Generation 13 starting: 114 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114/114 8.3 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━ 114/114 10.5 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 6.4 configs/s
[3652s] Generation 13 complete: error=1 ok=117 min=1.5728 mid=1.9781 max=29.0188 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3652s] Generation 14 starting: 110 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110/110 8.2 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━ 110/110 10.5 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 6.3 configs/s
[3695s] Generation 14 complete: error=1 ok=113 min=1.5790 mid=2.1062 max=29.0466 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3695s] Generation 15 starting: 72 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 14.3 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 11.5 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 9.6 configs/s
[3721s] Generation 15 complete: error=1 ok=74 min=1.6006 mid=1.8585 max=23.7788 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3721s] Generation 16 starting: 69 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 14.6 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 10.8 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 9.2 configs/s
[3748s] Generation 16 complete: ok=71 min=1.5779 mid=1.7867 max=24.4323 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3748s] Generation 17 starting: 76 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 14.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 10.8 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 9.7 configs/s
[3777s] Generation 17 complete: ok=78 min=1.5684 mid=1.8112 max=26.9159 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3777s] Generation 18 starting: 77 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 14.4 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 10.8 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 7.7 configs/s
[3806s] Generation 18 complete: ok=79 min=1.5649 mid=1.7946 max=27.0335 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3806s] Generation 19 starting: 75 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 14.4 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 10.7 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 9.6 configs/s
[3834s] Generation 19 complete: ok=77 min=1.5655 mid=1.7979 max=27.0405 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[3834s] Generation 20 starting: 72 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 13.7 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 10.7 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━ 129/129 9.8 configs/s
[3861s] Generation 20 complete: ok=74 min=1.5670 mid=1.8025 max=27.0450 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[3861s] Autotuning complete in 3862.0s after searching 2333 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 3:
(M, K, N)
------------------
(3328, 3456, 3584)
 40%|████      | 4/10 [3:43:27<5:54:59, 3549.90s/it]WARNING:tritonbench.utils.triton_op:Running input ID 4:
(M, K, N)
------------------
(3456, 3584, 3712)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_171", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 1.6308629512786865, "best_triton_pos": 0}
AUTOTUNE mm(3456x3584, 3584x3712)
strides: [3584, 1], [3712, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_171 1.6309 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_158 1.6580 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_170 1.7117 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_174 1.7432 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_172 1.8111 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_178 1.9765 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_167 1.9807 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_164 2.0513 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_175 2.0612 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_177 2.0672 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 2.1582 seconds and 0.9904 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3485.50ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 31.44ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3456, 3584),
              'stride': (3584, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3584, 3712),
              'stride': (3712, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 4.08ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[61s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 1.7 configs/s
[3221s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=2.8163 mid=191.2428 max=177575.0625 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3221s] Generation 1 starting: 199 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 12.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 5.2 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 9.6 configs/s
[3281s] Generation 1 complete: error=12 ok=192 min=2.0583 mid=6.1580 max=461.0455 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3281s] Generation 2 starting: 180 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 13.2 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 6.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 6.0 configs/s
[3344s] Generation 2 complete: error=5 ok=180 min=1.8942 mid=3.5912 max=571.2734 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3344s] Generation 3 starting: 168 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 8.9 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 11.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 3.5 configs/s
[3411s] Generation 3 complete: error=6 ok=167 min=1.7497 mid=2.5464 max=33.4977 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3411s] Generation 4 starting: 185 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 9.0 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 10.7 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 3.8 configs/s
[3482s] Generation 4 complete: error=12 ok=178 min=1.6728 mid=2.5410 max=33.3376 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3482s] Generation 5 starting: 184 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 9.0 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 10.8 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 3.6 configs/s
[3555s] Generation 5 complete: error=12 ok=177 min=1.6661 mid=2.4718 max=34.9311 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3555s] Generation 6 starting: 149 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 8.6 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 11.2 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 4.5 configs/s
[3614s] Generation 6 complete: error=13 ok=141 min=1.6686 mid=2.4743 max=33.4639 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3614s] Generation 7 starting: 148 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148/148 8.4 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148/148 10.7 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 4.5 configs/s
[3674s] Generation 7 complete: error=10 ok=143 min=1.6703 mid=2.4777 max=33.5731 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3674s] Generation 8 starting: 152 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152/152 8.3 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152/152 10.9 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 4.2 configs/s
[3735s] Generation 8 complete: error=11 ok=146 min=1.6728 mid=2.4858 max=33.3577 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3735s] Generation 9 starting: 78 neighbors, 2 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 6.0 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 11.3 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 14.7 configs/s
[3762s] Generation 9 complete: error=11 ok=70 min=1.6970 mid=2.5638 max=33.5781 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3762s] Generation 10 starting: 79 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 10.1 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 11.3 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 29.1 configs/s
[3786s] Generation 10 complete: error=11 ok=71 min=1.6945 mid=2.5804 max=33.1073 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3786s] Generation 11 starting: 77 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 10.0 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 11.4 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 25.3 configs/s
[3808s] Generation 11 complete: error=11 ok=69 min=1.6893 mid=2.6419 max=33.0820 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3808s] Generation 12 starting: 78 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 10.1 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 11.3 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 2409.3 configs/s
[3830s] Generation 12 complete: error=11 ok=70 min=1.6960 mid=2.6434 max=33.1845 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3830s] Generation 13 starting: 37 neighbors, 1 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 3.2 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 8.7 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 1.6 configs/s
[3848s] Generation 13 complete: ok=39 min=1.6953 mid=2.7067 max=33.6127 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3848s] Generation 14 starting: 39 neighbors, 1 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 3.1 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 8.8 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 2662.5 configs/s
[3868s] Generation 14 complete: ok=41 min=1.6956 mid=2.6203 max=33.5289 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3868s] Generation 15 starting: 38 neighbors, 1 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 3.3 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 8.7 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 20.3 configs/s
[3887s] Generation 15 complete: ok=40 min=1.6965 mid=2.7100 max=33.3835 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[3887s] Autotuning complete in 3887.6s after searching 1890 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 4:
(M, K, N)
------------------
(3456, 3584, 3712)
 50%|█████     | 5/10 [4:48:21<5:06:09, 3673.86s/it]WARNING:tritonbench.utils.triton_op:Running input ID 5:
(M, K, N)
------------------
(3584, 3712, 3840)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_206", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 1.779865026473999, "best_triton_pos": 0}
AUTOTUNE mm(3584x3712, 3712x3840)
strides: [3712, 1], [3840, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_206 1.7799 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_203 1.8059 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_213 1.8317 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_207 1.8329 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_194 1.8345 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_210 1.8751 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_205 1.9360 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_204 1.9509 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_208 1.9749 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_214 1.9925 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 3.2028 seconds and 1.0271 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 4717.83ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 31.79ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3584, 3712),
              'stride': (3712, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3712, 3840),
              'stride': (3840, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 2.36ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[62s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 1.6 configs/s
[2157s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=2.4723 mid=212.5115 max=47163.9922 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2157s] Generation 1 starting: 199 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 7.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 6.8 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 17.5 configs/s
[2223s] Generation 1 complete: error=13 ok=191 min=1.9343 mid=6.6412 max=134.8188 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2223s] Generation 2 starting: 183 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 11.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 11.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 8.3 configs/s
[2269s] Generation 2 complete: error=6 ok=182 min=1.9420 mid=3.7161 max=35.7193 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2269s] Generation 3 starting: 177 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 8.2 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 10.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 3.4 configs/s
[2339s] Generation 3 complete: error=4 ok=178 min=1.9563 mid=2.6808 max=40.1540 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2339s] Generation 4 starting: 149 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 10.7 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 11.3 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105/105 3.6 configs/s
[2399s] Generation 4 complete: error=2 ok=151 min=1.8699 mid=2.3408 max=29.1846 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2399s] Generation 5 starting: 140 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140/140 10.6 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140/140 11.5 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 3.4 configs/s
[2458s] Generation 5 complete: error=1 ok=143 min=1.8658 mid=2.2681 max=28.5774 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2458s] Generation 6 starting: 137 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137/137 10.0 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137/137 10.5 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 3.6 configs/s
[2516s] Generation 6 complete: ok=141 min=1.8787 mid=2.0392 max=30.0899 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2516s] Generation 7 starting: 141 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141/141 10.0 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141/141 10.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 3.7 configs/s
[2576s] Generation 7 complete: error=1 ok=144 min=1.8259 mid=1.9814 max=35.2151 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2576s] Generation 8 starting: 109 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 9.5 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 9.5 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 4.7 configs/s
[2623s] Generation 8 complete: error=1 ok=111 min=1.8190 mid=1.9721 max=35.2160 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2623s] Generation 9 starting: 36 neighbors, 1 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 8.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 10.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 17.6 configs/s
[2641s] Generation 9 complete: ok=38 min=1.8142 mid=2.0156 max=28.2488 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2641s] Generation 10 starting: 37 neighbors, 1 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 8.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 10.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110/110 15.8 configs/s
[2658s] Generation 10 complete: ok=39 min=1.8226 mid=2.0186 max=28.2122 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2658s] Generation 11 starting: 37 neighbors, 1 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 12.2 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 10.3 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110/110 13.3 configs/s
[2675s] Generation 11 complete: ok=39 min=1.8276 mid=2.0229 max=28.1266 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2675s] Generation 12 starting: 38 neighbors, 1 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 12.2 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 10.4 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110/110 12.8 configs/s
[2692s] Generation 12 complete: ok=40 min=1.8314 mid=2.0185 max=28.1541 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2692s] Generation 13 starting: 36 neighbors, 1 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 8.9 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 10.0 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110/110 16.4 configs/s
[2709s] Generation 13 complete: ok=38 min=1.8311 mid=2.0266 max=28.2520 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[2709s] Autotuning complete in 2709.7s after searching 1518 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 5:
(M, K, N)
------------------
(3584, 3712, 3840)
 60%|██████    | 6/10 [5:33:38<3:43:14, 3348.54s/it]WARNING:tritonbench.utils.triton_op:Running input ID 6:
(M, K, N)
------------------
(3712, 3840, 3968)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_242", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 1.973628044128418, "best_triton_pos": 0}
AUTOTUNE mm(3712x3840, 3840x3968)
strides: [3840, 1], [3968, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_242 1.9736 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_239 1.9915 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_230 2.0264 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_243 2.0330 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_249 2.1111 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_246 2.1491 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_240 2.1904 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_241 2.1905 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_244 2.2064 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_236 2.2464 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 2.4138 seconds and 1.0066 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3906.17ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 32.03ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3712, 3840),
              'stride': (3840, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3840, 3968),
              'stride': (3968, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 3.08ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[62s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 1.4 configs/s
[2434s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=2.8846 mid=235.3452 max=56995.0508 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2434s] Generation 1 starting: 205 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 10.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 2.9 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86/86 15.4 configs/s
[2514s] Generation 1 complete: error=13 ok=197 min=2.2514 mid=7.4862 max=568.8234 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2514s] Generation 2 starting: 185 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 10.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 10.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93/93 11.8 configs/s
[2562s] Generation 2 complete: error=8 ok=182 min=2.0497 mid=4.5679 max=38.7458 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2562s] Generation 3 starting: 171 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 10.2 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 11.0 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97/97 4.6 configs/s
[2619s] Generation 3 complete: error=2 ok=174 min=2.0806 mid=3.6841 max=44.3326 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2619s] Generation 4 starting: 172 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172/172 9.5 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172/172 10.8 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97/97 4.0 configs/s
[2678s] Generation 4 complete: error=7 ok=170 min=2.0787 mid=3.2281 max=39.1841 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2678s] Generation 5 starting: 177 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 9.6 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 10.2 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97/97 2.9 configs/s
[2750s] Generation 5 complete: error=4 ok=178 min=2.0888 mid=2.7254 max=39.5976 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2750s] Generation 6 starting: 132 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132/132 9.0 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132/132 9.2 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97/97 3.8 configs/s
[2807s] Generation 6 complete: ok=136 min=2.0242 mid=2.3805 max=43.7193 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2807s] Generation 7 starting: 139 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139/139 9.2 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139/139 8.5 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 3.5 configs/s
[2868s] Generation 7 complete: ok=143 min=2.0103 mid=2.2309 max=45.3756 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2868s] Generation 8 starting: 145 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145/145 9.9 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145/145 8.9 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 3.6 configs/s
[2929s] Generation 8 complete: ok=149 min=2.0140 mid=2.3962 max=45.6412 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2929s] Generation 9 starting: 109 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 9.5 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 9.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 4.9 configs/s
[2974s] Generation 9 complete: ok=112 min=2.0128 mid=2.4146 max=42.4955 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2974s] Generation 10 starting: 113 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 8.8 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 9.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101/101 4.9 configs/s
[3023s] Generation 10 complete: ok=116 min=2.0195 mid=2.3896 max=42.5283 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[None, False])
[3023s] Generation 11 starting: 76 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 9.3 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 9.0 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101/101 8.3 configs/s
[3055s] Generation 11 complete: ok=78 min=2.0041 mid=2.4159 max=42.7308 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[None, False])
[3055s] Generation 12 starting: 37 neighbors, 1 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 9.8 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 37/37 9.9 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101/101 11.4 configs/s
[3072s] Generation 12 complete: ok=39 min=2.0086 mid=2.2752 max=28.9472 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[None, False])
[3072s] Autotuning complete in 3072.5s after searching 1760 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[None, False]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 6:
(M, K, N)
------------------
(3712, 3840, 3968)
 70%|███████   | 7/10 [6:24:57<2:43:01, 3260.43s/it]WARNING:tritonbench.utils.triton_op:Running input ID 7:
(M, K, N)
------------------
(3840, 3968, 4096)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_278", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 2.207350969314575, "best_triton_pos": 0}
AUTOTUNE mm(3840x3968, 3968x4096)
strides: [3968, 1], [4096, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_278 2.2074 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_279 2.2485 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_266 2.2666 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_282 2.2856 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_285 2.3706 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_286 2.3889 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_275 2.3934 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_280 2.4310 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_277 2.4912 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_276 2.4972 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 3.0164 seconds and 1.0057 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 4500.86ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 31.26ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3840, 3968),
              'stride': (3968, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3968, 4096),
              'stride': (4096, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 3.34ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[62s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 1.1 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52/52 - configs/s
[3088s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=3.6705 mid=295.7979 max=71991.3750 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3088s] Generation 1 starting: 199 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 6.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 5.0 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86/86 - configs/s
[3157s] Generation 1 complete: error=12 ok=192 min=2.3156 mid=8.1891 max=163.2469 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3157s] Generation 2 starting: 180 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 9.0 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 9.8 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/87 8.2 configs/s
[3207s] Generation 2 complete: error=6 ok=179 min=2.2962 mid=4.3905 max=55.9833 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3207s] Generation 3 starting: 173 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 7.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 10.1 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/87 3.0 configs/s
[3280s] Generation 3 complete: error=4 ok=174 min=2.3038 mid=3.0116 max=49.4628 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3280s] Generation 4 starting: 183 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 7.9 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 10.5 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 2.8 configs/s
[3355s] Generation 4 complete: error=2 ok=186 min=2.2583 mid=2.9753 max=35.8321 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3355s] Generation 5 starting: 177 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 7.6 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 10.4 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 2.3 configs/s
[3434s] Generation 5 complete: error=1 ok=181 min=2.2271 mid=2.8132 max=39.4262 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3434s] Generation 6 starting: 177 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 7.7 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 8.8 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 2.7 configs/s
[3513s] Generation 6 complete: ok=182 min=2.2325 mid=2.6690 max=39.4824 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3513s] Generation 7 starting: 182 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 7.6 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 9.3 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 2.5 configs/s
[3594s] Generation 7 complete: ok=187 min=2.2290 mid=2.6241 max=39.5305 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3594s] Generation 8 starting: 142 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142/142 7.3 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142/142 9.4 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 3.5 configs/s
[3658s] Generation 8 complete: ok=146 min=2.2407 mid=2.6828 max=39.6659 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3658s] Generation 9 starting: 103 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 5.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103/103 10.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 4.6 configs/s
[3707s] Generation 9 complete: ok=106 min=2.2162 mid=2.6065 max=31.6838 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3707s] Generation 10 starting: 69 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 7.8 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69/69 10.6 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 7.2 configs/s
[3738s] Generation 10 complete: ok=72 min=2.2567 mid=2.6125 max=31.4823 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3738s] Generation 11 starting: 35 neighbors, 1 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 9.2 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 11.5 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 19.1 configs/s
[3754s] Generation 11 complete: ok=37 min=2.2460 mid=2.9114 max=28.3649 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3754s] Generation 12 starting: 35 neighbors, 1 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 9.1 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 11.5 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 16.2 configs/s
[3770s] Generation 12 complete: ok=37 min=2.2612 mid=2.8874 max=28.5286 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3770s] Generation 13 starting: 35 neighbors, 1 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 9.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 11.5 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 13.9 configs/s
[3786s] Generation 13 complete: ok=37 min=2.2727 mid=2.9168 max=28.3820 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3786s] Generation 14 starting: 36 neighbors, 1 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 9.4 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36/36 11.8 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 18.1 configs/s
[3802s] Generation 14 complete: ok=38 min=2.2776 mid=2.8739 max=28.3498 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3802s] Generation 15 starting: 34 neighbors, 1 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 9.8 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 11.4 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 12.9 configs/s
[3818s] Generation 15 complete: ok=36 min=2.2791 mid=2.8864 max=28.3347 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3818s] Generation 16 starting: 35 neighbors, 1 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 9.2 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 11.5 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 8.3 configs/s
[3833s] Generation 16 complete: ok=37 min=2.2766 mid=2.8704 max=28.3699 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3833s] Generation 17 starting: 33 neighbors, 1 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 9.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 11.4 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 1917.3 configs/s
[3848s] Generation 17 complete: ok=35 min=2.2848 mid=2.8862 max=28.0245 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3848s] Generation 18 starting: 34 neighbors, 1 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 9.2 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 11.5 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 9.5 configs/s
[3864s] Generation 18 complete: ok=36 min=2.2812 mid=2.8870 max=28.2588 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3864s] Generation 19 starting: 31 neighbors, 1 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31/31 9.8 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31/31 11.5 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 1759.3 configs/s
[3878s] Generation 19 complete: ok=33 min=2.2919 mid=2.9473 max=28.4370 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3878s] Generation 20 starting: 32 neighbors, 1 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32/32 11.0 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32/32 11.3 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 1886.9 configs/s
[3892s] Generation 20 complete: ok=34 min=2.2884 mid=2.9467 max=28.9875 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3892s] Autotuning complete in 3892.8s after searching 2024 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 7:
(M, K, N)
------------------
(3840, 3968, 4096)
 80%|████████  | 8/10 [7:29:57<1:55:28, 3464.12s/it]WARNING:tritonbench.utils.triton_op:Running input ID 8:
(M, K, N)
------------------
(3968, 4096, 4224)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_302", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4", "best_time": 2.572115898132324, "best_triton_pos": 0}
AUTOTUNE mm(3968x4096, 4096x4224)
strides: [4096, 1], [4224, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_302 2.5721 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_315 2.7469 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_316 2.7486 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_318 2.8148 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_322 2.9954 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_314 3.2816 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_319 3.4525 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_304 3.5986 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_299 3.6948 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_303 3.7564 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 2.6736 seconds and 0.9772 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3951.57ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 32.08ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3968, 4096),
              'stride': (4096, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4096, 4224),
              'stride': (4224, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 3.63ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[62s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
[71s] Timeout after 60s compiling Config(block_sizes=[1024, 2, 512], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', ''], loop_orders=[[0, 1]], num_stages=5, num_warps=32, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[True, False], range_num_stages=[3, 4], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 1.1 configs/s
[2853s] Initial random population of 100, 5 starting points: error=7 timeout=2 ok=91 min=3.7587 mid=295.1454 max=68969.7188 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2853s] Generation 1 starting: 201 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201/201 7.4 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201/201 3.4 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61/61 - configs/s
[2932s] Generation 1 complete: error=18 ok=188 min=3.2030 mid=8.2615 max=690.0235 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[2932s] Generation 2 starting: 175 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 8.6 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 3.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68/68 4.3 configs/s
[3010s] Generation 2 complete: error=6 ok=174 min=2.8737 mid=5.2180 max=968.4458 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3010s] Generation 3 starting: 171 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 8.2 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 10.7 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70/70 3.1 configs/s
[3072s] Generation 3 complete: error=16 ok=160 min=2.8181 mid=4.3587 max=41.9407 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3072s] Generation 4 starting: 182 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 8.1 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 9.6 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 2.1 configs/s
[3152s] Generation 4 complete: error=7 ok=180 min=2.6692 mid=3.8318 max=39.5699 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3152s] Generation 5 starting: 173 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 7.9 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 8.2 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 2.4 configs/s
[3230s] Generation 5 complete: error=5 ok=173 min=2.6592 mid=3.4681 max=51.7731 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, None])
[3230s] Generation 6 starting: 156 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156/156 7.7 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156/156 8.2 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 2.9 configs/s
[3299s] Generation 6 complete: error=7 ok=153 min=2.6640 mid=3.7078 max=51.4906 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3299s] Generation 7 starting: 150 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150/150 7.5 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150/150 8.4 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 2.9 configs/s
[3365s] Generation 7 complete: error=7 ok=147 min=2.6507 mid=3.5020 max=50.7008 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[None, False])
[3365s] Generation 8 starting: 149 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 7.6 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149/149 8.8 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 2.7 configs/s
[3432s] Generation 8 complete: error=6 ok=147 min=2.6450 mid=3.2990 max=53.1490 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, False])
[3432s] Generation 9 starting: 155 neighbors, 4 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155/155 7.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155/155 8.3 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 2.5 configs/s
[3502s] Generation 9 complete: error=7 ok=152 min=2.6196 mid=3.4019 max=55.7207 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, False])
[3502s] Generation 10 starting: 163 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163/163 7.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163/163 8.3 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 2.5 configs/s
[3576s] Generation 10 complete: error=8 ok=159 min=2.5830 mid=3.4056 max=55.6746 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[3576s] Generation 11 starting: 120 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 7.6 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 8.1 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.4 configs/s
[3631s] Generation 11 complete: error=6 ok=117 min=2.5787 mid=3.5968 max=55.6138 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[3631s] Generation 12 starting: 119 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 7.5 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 8.1 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.4 configs/s
[3685s] Generation 12 complete: error=6 ok=116 min=2.5777 mid=3.5885 max=55.4528 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3685s] Generation 13 starting: 118 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 7.4 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 8.4 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.6 configs/s
[3739s] Generation 13 complete: error=6 ok=115 min=2.5763 mid=3.5898 max=55.5111 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3739s] Generation 14 starting: 120 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 7.5 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 8.3 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.6 configs/s
[3794s] Generation 14 complete: error=6 ok=117 min=2.5805 mid=3.5920 max=55.3348 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3794s] Generation 15 starting: 117 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 7.4 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 8.2 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.4 configs/s
[3848s] Generation 15 complete: error=6 ok=114 min=2.5767 mid=3.5850 max=55.5008 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3848s] Generation 16 starting: 116 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 7.2 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 8.1 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.2 configs/s
[3901s] Generation 16 complete: error=6 ok=113 min=2.5728 mid=3.5904 max=55.4506 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3901s] Generation 17 starting: 120 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 7.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 8.2 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.9 configs/s
[3957s] Generation 17 complete: error=6 ok=117 min=2.5767 mid=3.5908 max=55.6103 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3957s] Generation 18 starting: 115 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 7.2 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 8.3 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.6 configs/s
[4010s] Generation 18 complete: error=6 ok=112 min=2.5758 mid=3.5826 max=55.5084 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[4010s] Generation 19 starting: 115 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 7.2 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 8.2 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 3.8 configs/s
[4063s] Generation 19 complete: error=6 ok=112 min=2.5759 mid=3.5922 max=55.3667 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[4063s] Generation 20 starting: 121 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 7.3 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 8.1 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 4.0 configs/s
[4119s] Generation 20 complete: error=6 ok=118 min=2.5767 mid=3.5858 max=55.3513 best=Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[4119s] Autotuning complete in 4119.7s after searching 2954 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], num_stages=4, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 8:
(M, K, N)
------------------
(3968, 4096, 4224)
 90%|█████████ | 9/10 [8:38:44<1:01:11, 3671.18s/it]WARNING:tritonbench.utils.triton_op:Running input ID 9:
(M, K, N)
------------------
(4096, 4224, 4352)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_350", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 2.629836082458496, "best_triton_pos": 0}
AUTOTUNE mm(4096x4224, 4224x4352)
strides: [4224, 1], [4352, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_350 2.6298 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_347 2.6379 ms 99.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_351 2.6680 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_357 2.6848 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_338 2.6942 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_354 2.7477 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_349 2.8709 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
  triton_mm_352 2.9062 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_358 2.9113 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_348 2.9170 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 3.1574 seconds and 1.0843 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 4810.22ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 31.58ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4096, 4224),
              'stride': (4224, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4224, 4352),
              'stride': (4352, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 4.26ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 669079826
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[63s] Timeout after 60s compiling Config(block_sizes=[1, 32, 4096], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], num_stages=2, num_warps=32, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[0, 0], range_warp_specializes=[True, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.9 configs/s
[3050s] Initial random population of 100, 5 starting points: error=6 timeout=1 ok=93 min=2.9732 mid=339.7414 max=66939.2031 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3050s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 7.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 2.1 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 11.4 configs/s
[3140s] Generation 1 complete: error=7 ok=200 min=2.7743 mid=11.3193 max=498.6652 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3140s] Generation 2 starting: 170 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 7.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 9.7 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 4.4 configs/s
[3198s] Generation 2 complete: error=5 ok=170 min=2.7796 mid=5.2170 max=55.9554 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3198s] Generation 3 starting: 174 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 7.2 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 10.3 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 2.7 configs/s
[3269s] Generation 3 complete: error=9 ok=170 min=2.7432 mid=4.1065 max=50.2299 best=Config(block_sizes=[128, 256, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=16, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3269s] Generation 4 starting: 169 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 7.0 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 9.5 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 2.8 configs/s
[3340s] Generation 4 complete: error=4 ok=170 min=2.6666 mid=3.6617 max=54.3033 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3340s] Generation 5 starting: 176 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 5.7 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 7.9 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 2.3 configs/s
[3430s] Generation 5 complete: error=1 ok=180 min=2.6337 mid=3.1939 max=56.5969 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3430s] Generation 6 starting: 172 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172/172 6.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172/172 8.2 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76/76 2.3 configs/s
[3513s] Generation 6 complete: error=1 ok=176 min=2.6595 mid=2.9313 max=60.4960 best=Config(block_sizes=[128, 128, 16], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[3513s] Generation 7 starting: 147 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147/147 6.7 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147/147 8.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 2.8 configs/s
[3582s] Generation 7 complete: error=1 ok=150 min=2.5744 mid=2.8834 max=53.5708 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3582s] Generation 8 starting: 143 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 6.9 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143/143 8.6 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 2.7 configs/s
[3650s] Generation 8 complete: error=1 ok=146 min=2.5533 mid=2.8601 max=53.9135 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3650s] Generation 9 starting: 107 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 6.6 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 8.4 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 4.0 configs/s
[3701s] Generation 9 complete: error=1 ok=110 min=2.5480 mid=2.9751 max=53.8480 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3701s] Generation 10 starting: 109 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 7.0 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 8.2 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 3.9 configs/s
[3753s] Generation 10 complete: error=1 ok=112 min=2.5533 mid=2.8815 max=53.8375 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3753s] Generation 11 starting: 106 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 6.6 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106/106 8.1 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 4.2 configs/s
[3805s] Generation 11 complete: error=1 ok=109 min=2.5616 mid=2.8852 max=53.6503 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3805s] Generation 12 starting: 107 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 6.5 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 8.1 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 3.9 configs/s
[3857s] Generation 12 complete: error=1 ok=110 min=2.5535 mid=2.9628 max=53.6076 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3857s] Generation 13 starting: 107 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 6.5 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 8.1 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 4.1 configs/s
[3909s] Generation 13 complete: error=1 ok=110 min=2.5608 mid=2.9626 max=53.6365 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3909s] Generation 14 starting: 72 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 6.3 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 7.6 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 6.0 configs/s
[3944s] Generation 14 complete: error=1 ok=74 min=2.5653 mid=2.9998 max=53.7844 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3944s] Generation 15 starting: 74 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 7.1 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 7.5 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 4.9 configs/s
[3980s] Generation 15 complete: error=1 ok=76 min=2.5707 mid=2.9404 max=53.6750 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[3980s] Generation 16 starting: 71 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 7.3 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 7.4 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 6.8 configs/s
[4016s] Generation 16 complete: error=1 ok=73 min=2.5598 mid=2.9414 max=53.6640 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[4016s] Generation 17 starting: 74 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 6.3 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 7.5 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 5.3 configs/s
[4053s] Generation 17 complete: error=1 ok=76 min=2.5642 mid=2.9452 max=53.8012 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[4053s] Generation 18 starting: 72 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 6.1 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 7.4 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 7.3 configs/s
[4089s] Generation 18 complete: error=1 ok=74 min=2.5656 mid=2.9973 max=53.9364 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[4089s] Generation 19 starting: 73 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73/73 6.0 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73/73 7.5 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 6.4 configs/s
[4125s] Generation 19 complete: error=1 ok=75 min=2.5570 mid=2.9434 max=53.6502 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[4125s] Generation 20 starting: 72 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 6.3 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 7.4 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 5.1 configs/s
[4161s] Generation 20 complete: error=1 ok=74 min=2.5521 mid=2.9431 max=53.8327 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None])
[4161s] Autotuning complete in 4161.6s after searching 2496 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 9:
(M, K, N)
------------------
(4096, 4224, 4352)
100%|██████████| 10/10 [9:48:13<00:00, 3824.94s/it] 100%|██████████| 10/10 [9:48:13<00:00, 3529.35s/it]
INFO:tritonbench.utils.run_utils:[tritonbench] Output result csv to /tmp/tmpk5z3u_j2.csv
         (M, K, N)    torch_matmul-latency    torch_matmul-tflops    triton_matmul-latency    triton_matmul-accuracy    triton_matmul-speedup    triton_matmul-tflops    kernelllm_matmul-latency    kernelllm_matmul-accuracy    kernelllm_matmul-speedup    kernelllm_matmul-tflops    mako_matmul-latency    mako_matmul-accuracy    mako_matmul-speedup    mako_matmul-tflops    torch_compile_max_matmul-latency    torch_compile_max_matmul-accuracy    torch_compile_max_matmul-speedup    torch_compile_max_matmul-tflops    torch_compile_default_matmul-latency    torch_compile_default_matmul-accuracy    torch_compile_default_matmul-speedup    torch_compile_default_matmul-tflops    helion_matmul_tritonbench-latency    helion_matmul_tritonbench-accuracy    helion_matmul_tritonbench-speedup    helion_matmul_tritonbench-tflops
------------------  ----------------------  ---------------------  -----------------------  ------------------------  -----------------------  ----------------------  --------------------------  ---------------------------  --------------------------  -------------------------  ---------------------  ----------------------  ---------------------  --------------------  ----------------------------------  -----------------------------------  ----------------------------------  ---------------------------------  --------------------------------------  ---------------------------------------  --------------------------------------  -------------------------------------  -----------------------------------  ------------------------------------  -----------------------------------  ----------------------------------
(2944, 3072, 3200)       2.920320 (±2.72%)                19.8202        1.589941 (±3.49%)                         1                  1.83675                 36.4047           3.116203 (±3.41%)                            1                    0.93714                     18.5743      1.719063 (±3.68%)                       1                1.69879               33.6703                   1.391699 (±5.39%)                                    1                             2.09838                            41.5905                       2.883879 (±3.31%)                                        1                                1.01264                                 20.0707                    0.984134 (±4.04%)                                     1                              2.9674                              58.8145
(3072, 3200, 3328)       3.006803 (±1.90%)                21.761         1.742265 (±4.65%)                         1                  1.7258                  37.5552           3.287447 (±1.58%)                            1                    0.914632                    19.9033      1.521262 (±1.74%)                       1                1.97652               43.0111                   1.509502 (±4.09%)                                    1                             1.99192                            43.3462                       3.244127 (±4.34%)                                        1                                0.926845                                20.1691                    1.336140 (±3.73%)                                     1                              2.25037                             48.9703
(3200, 3328, 3456)       3.346407 (±2.04%)                21.9967        1.958708 (±4.10%)                         1                  1.70848                 37.5809           3.649892 (±1.20%)                            1                    0.916851                    20.1677      1.685504 (±1.54%)                       1                1.9854                43.6724                   1.688824 (±3.75%)                                    1                             1.9815                             43.5866                       3.623652 (±2.92%)                                        1                                0.92349                                 20.3138                   1.434981 (±13.43%)                                     1                              2.33202                             51.2969
(3328, 3456, 3584)       3.757533 (±1.20%)                21.9408        2.152871 (±4.32%)                         1                  1.74536                 38.2946           4.059897 (±0.80%)                            1                    0.925524                    20.3067      1.844186 (±1.15%)                       1                2.0375                44.7044                   1.811385 (±3.53%)                                    1                             2.0744                             45.5139                       4.004817 (±2.86%)                                        1                                0.938253                                20.586                     1.615423 (±4.21%)                                     1                              2.32604                             51.0351
(3456, 3584, 3712)       4.122498 (±1.99%)                22.3059        2.401874 (±3.46%)                         1                  1.71637                 38.2851           4.465943 (±1.48%)                            1                    0.923097                    20.5905      2.390754 (±2.08%)                       1                1.72435               38.4631                   2.012348 (±3.05%)                                    1                             2.0486                             45.6958                       4.447703 (±6.85%)                                        1                                0.926883                                20.6749                    1.709104 (±2.16%)                                     1                              2.41208                             53.8036
(3584, 3712, 3840)       4.562624 (±1.20%)                22.3935        2.609157 (±3.16%)                         1                  1.7487                  39.1595           4.930469 (±1.42%)                            1                    0.925393                    20.7228      2.174711 (±1.36%)                       1                2.09804               46.9824                   2.192150 (±4.62%)                                    1                             2.08135                            46.6087                       4.860668 (±8.53%)                                        1                                0.938682                                21.0204                    1.865826 (±3.37%)                                     1                              2.44536                             54.7603
(3712, 3840, 3968)       4.990030 (±6.33%)                22.6693        2.868640 (±3.09%)                         1                  1.73951                 39.4335           5.442636 (±1.28%)                            1                    0.916841                    20.7841      2.403794 (±1.18%)                       1                2.0759                47.0591                   2.421994 (±3.67%)                                    1                             2.0603                             46.7055                       5.399636 (±7.57%)                                        1                                0.924142                                20.9496                    2.031588 (±2.84%)                                     1                              2.45622                             55.6808
(3840, 3968, 4096)       6.153406 (±5.47%)                20.2851        3.094684 (±3.46%)                         1                  1.98838                 40.3345           6.592892 (±4.98%)                            1                    0.933339                    18.9329      3.225925 (±6.46%)                       1                1.90749               38.6935                   2.617636 (±5.06%)                                    1                             2.35075                            47.6852                       6.191486 (±4.82%)                                        1                                0.99385                                 20.1603                    2.308872 (±2.83%)                                     1                              2.66511                             54.0621
(3968, 4096, 4224)       6.045284 (±5.09%)                22.7127        3.847693 (±1.88%)                         1                  1.57115                 35.685            6.531491 (±4.63%)                            1                    0.92556                     21.022       4.873548 (±9.47%)                       1                1.24043               28.1735                   3.131604 (±2.64%)                                    1                             1.93041                            43.8449                       6.529291 (±6.41%)                                        1                                0.925871                                21.029                     2.668317 (±4.33%)                                     1                              2.26558                             51.4574
(4096, 4224, 4352)       6.518570 (±4.93%)                23.102         3.671571 (±4.27%)                         1                  1.77542                 41.0158           7.015537 (±4.45%)                            1                    0.929162                    21.4655      3.135883 (±1.52%)                       1                2.0787                48.0223                   3.078282 (±2.95%)                                    1                             2.1176                             48.9209                       6.933336 (±5.69%)                                        1                                0.940178                                21.72                      2.595676 (±2.77%)                                     1                              2.51132                             58.0166
           average       4.542347478866577                21.8987        2.593740391731262                         1                  1.75559                 38.3749           4.909240674972534                            1                    0.924754                    20.247       2.497463011741638                       1                1.88231               41.2452                   2.185542416572571                                    1                             2.07352                            45.3498                       4.811859464645385                                        1                                0.945083                                20.6694                    1.855006104707718                                     1                              2.46315                             53.7898INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency

INFO:tritonbench.utils.run_utils:[tritonbench] Running helion benchmark: /home/nvme/fedora/miniconda3/envs/lkesem-tritonenv/bin/python benchmarks/run.py --device=cuda --precision bf16 --kernel-config ../custom_kernel_config.yaml --kernel bf16_gemm_gelu --output benchmark_bf16_gemm_gelu.json
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
/opt/amdgpu/share/libdrm/amdgpu.ids: No such file or directory
Loading custom kernel configuration from: ../custom_kernel_config.yaml
Loaded 3 kernel mapping(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Loaded metric mappings for 3 kernel(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Using num_inputs=20 for bf16_gemm_gelu
Running bf16_gemm_gelu benchmark with Helion implementation...

WARNING:tritonbench.utils.triton_op:First-k mode: Selected 5 sequential inputs starting from index 0 (total available: 5)
WARNING:tritonbench.utils.triton_op:Input IDs to run: [0, 1, 2, 3, 4]
  0%|          | 0/5 [00:00<?, ?it/s]WARNING:tritonbench.utils.triton_op:Running input ID 0:
(M, K, N)
------------
(32, 64, 16)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 12, "num_triton_choices": 12, "best_kernel": "triton_mm_3", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2", "best_time": 0.007000000216066837, "best_triton_pos": 0}
AUTOTUNE mm(32x64, 64x16)
strides: [64, 1], [16, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_3 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_4 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_7 0.0070 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_1 0.0070 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_8 0.0071 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_5 0.0072 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_9 0.0072 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_11 0.0072 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_10 0.0072 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_0 0.0073 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=16, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=8, num_stages=2, num_warps=1
SingleProcess AUTOTUNE benchmarking takes 0.0416 seconds and 0.0729 seconds precompiling for 12 choices
INFO:tritonbench.utils.triton_op:Took 1098.05ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 148.56ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (32, 64),
              'stride': (64, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (64, 16),
              'stride': (16, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (16,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 0.39ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 704392404
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:55:33: note: - use: %98 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x32xbf16, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x32xf32, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<64> : tensor<1x32xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x32xi64, #blocked>
    %cst_1 = arith.constant dense<32> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<64> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x32xbf16, #blocked>
    %c512_i32 = arith.constant 512 : i32
    %c4_i32 = arith.constant 4 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<16> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x2xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x2xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x2xf32, #blocked2>
    %cst_9 = arith.constant dense<16> : tensor<32x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x2xf32, #blocked2>
    %c2_i32 = arith.constant 2 : i32
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c32_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 2 : i32, start = 0 : i32} : tensor<2xi32, #blocked3>
    %6 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x32x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<32x2x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<2x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x2x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c4_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c2_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<2xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<2xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x32xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x32xi64, #blocked1> -> tensor<8x32xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x32xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x32xi1, #blocked1> -> tensor<8x32xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<2xi32, #blocked3> -> tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x2xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x2xi32, #blocked5> -> tensor<1x2xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x2xi32, #blocked2> -> tensor<32x2xi32, #blocked2>
      %c64_i32_11 = arith.constant 64 : i32
      %cst_12 = arith.constant dense<0> : tensor<32xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<32xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<32xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<32xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x32xi64, #blocked5> -> tensor<1x32xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x32xi64, #blocked> -> tensor<8x32xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x32xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x32x!tt.ptr<bf16>, #blocked>, tensor<8x32xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x32xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x32xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x32xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x32xi1, #blocked> -> tensor<8x32xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x32xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x32x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<32x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<32x1xi32, #blocked1> -> tensor<32x2xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<32x2xi32, #blocked1> -> tensor<32x2xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<32x2xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<32x2x!tt.ptr<bf16>, #blocked2>, tensor<32x2xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<32x2x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x32xbf16, #blocked> -> tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<32x2xbf16, #blocked2> -> tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x2xf32, #blocked2> -> tensor<8x2xf32, #blocked2>
      %73 = tt.dot %70, %71, %72 : tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x2xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c32_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<32xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<32xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<32xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<32xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x32xi64, #blocked5> -> tensor<1x32xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x32xi64, #blocked> -> tensor<8x32xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x32xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x32x!tt.ptr<bf16>, #blocked>, tensor<8x32xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x32xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x32xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x32xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x32xi1, #blocked> -> tensor<8x32xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x32xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x32x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<32x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<32x1xi32, #blocked1> -> tensor<32x2xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<32x2xi32, #blocked1> -> tensor<32x2xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<32x2xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<32x2x!tt.ptr<bf16>, #blocked2>, tensor<32x2xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<32x2x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x32xbf16, #blocked> -> tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<32x2xbf16, #blocked2> -> tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x2xf32, #blocked2> -> tensor<8x2xf32, #blocked2>
      %105 = tt.dot %102, %103, %104 : tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x2xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<2x!tt.ptr<bf16>, #blocked3>, tensor<2xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_last : tensor<2x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<2xbf16, #blocked3> -> tensor<2xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<2xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x2xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x2xbf16, #blocked5> -> tensor<1x2xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x2xbf16, #blocked2> to tensor<1x2xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x2xf32, #blocked2> -> tensor<8x2xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x2xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x2xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x2xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x2xf32, #blocked2>) -> tensor<8x2xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x2xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x2xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x2xf32, #blocked2> to tensor<8x2xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<2xi32, #blocked3> -> tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x2xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x2xi32, #blocked5> -> tensor<1x2xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x2xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x2xi32, #blocked1> -> tensor<8x2xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x2xi32, #blocked2> -> tensor<8x2xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x2xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x2x!tt.ptr<bf16>, #blocked2>, tensor<8x2xi32, #blocked2>
      tt.store %131, %119 : tensor<8x2x!tt.ptr<bf16>, #blocked2>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=7 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 2, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_blocked', range_flattens=[True, True], range_multi_buffers=[None, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:52:85: error: 'tt.load' op operation destroyed but still has uses
            a_tile = tl.load(x + (indices_0[:, None] * 64 + indices_2[None, :] * 1), None)
                                                                                    ^
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:52:85: note: - use: %66 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<32x16xbf16, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>>) -> tensor<32x16xf32, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<16> : tensor<1x8xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x8xi64, #blocked>
    %cst_1 = arith.constant dense<32> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<16> : tensor<32x1xi64, #blocked1>
    %c2_i32 = arith.constant 2 : i32
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<1.000000e+00> : tensor<32x8xf32, #blocked>
    %cst_5 = arith.constant dense<0.707106769> : tensor<32x8xf32, #blocked>
    %cst_6 = arith.constant dense<5.000000e-01> : tensor<32x8xf32, #blocked>
    %cst_7 = arith.constant dense<16> : tensor<16x1xi32, #blocked1>
    %cst_8 = arith.constant dense<64> : tensor<32x1xi32, #blocked1>
    %cst_9 = arith.constant dense<0.000000e+00> : tensor<32x8xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked2>
    %2 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked2>
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %4 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked3>
    %5 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked>
    %6 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked2>
    %7 = arith.extsi %1 : tensor<32xi32, #blocked2> to tensor<32xi64, #blocked2>
    %8 = arith.extsi %2 : tensor<8xi32, #blocked2> to tensor<8xi64, #blocked2>
    %9 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x8x!tt.ptr<bf16>, #blocked>
    scf.for %arg4 = %0 to %c2_i32 step %c16_i32  : i32 {
      %10 = arith.divsi %arg4, %c16_i32 : i32
      %11 = arith.muli %10, %c8_i32 : i32
      %12 = arith.subi %c1_i32, %11 : i32
      %13 = arith.minsi %12, %c8_i32 : i32
      %14 = arith.remsi %arg4, %c16_i32 : i32
      %15 = arith.remsi %14, %13 : i32
      %16 = arith.addi %11, %15 : i32
      %17 = arith.divsi %14, %13 : i32
      %18 = arith.muli %16, %c32_i32 : i32
      %19 = tt.splat %18 : i32 -> tensor<32xi32, #blocked2>
      %20 = arith.addi %19, %1 : tensor<32xi32, #blocked2>
      %21 = arith.muli %17, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked2>
      %23 = arith.addi %22, %2 : tensor<8xi32, #blocked2>
      %24 = ttg.convert_layout %20 : tensor<32xi32, #blocked2> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %25 = tt.expand_dims %24 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %26 = ttg.convert_layout %25 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %27 = arith.muli %26, %cst_8 : tensor<32x1xi32, #blocked1>
      %28 = tt.broadcast %27 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %29 = ttg.convert_layout %28 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked3>
      %30 = ttg.convert_layout %23 : tensor<8xi32, #blocked2> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %31 = tt.expand_dims %30 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %32 = ttg.convert_layout %31 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked>
      %33 = tt.broadcast %32 : tensor<1x8xi32, #blocked> -> tensor<16x8xi32, #blocked>
      %c32_i32_10 = arith.constant 32 : i32
      %34 = scf.for %arg5 = %c0_i32 to %c64_i32 step %c32_i32_10 iter_args(%arg6 = %cst_9) -> (tensor<32x8xf32, #blocked>)  : i32 {
        %77 = tt.splat %arg5 : i32 -> tensor<16xi32, #blocked2>
        %78 = arith.addi %77, %3 : tensor<16xi32, #blocked2>
        %79 = ttg.convert_layout %78 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %81 = ttg.convert_layout %80 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %82 = tt.broadcast %81 : tensor<1x16xi32, #blocked3> -> tensor<32x16xi32, #blocked3>
        %83 = arith.addi %29, %82 : tensor<32x16xi32, #blocked3>
        %84 = tt.addptr %4, %83 : tensor<32x16x!tt.ptr<bf16>, #blocked3>, tensor<32x16xi32, #blocked3>
        %85 = tt.load %84 : tensor<32x16x!tt.ptr<bf16>, #blocked3>
        %86 = ttg.convert_layout %78 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %87 = tt.expand_dims %86 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %88 = ttg.convert_layout %87 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
        %89 = arith.muli %88, %cst_7 : tensor<16x1xi32, #blocked1>
        %90 = tt.broadcast %89 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
        %91 = ttg.convert_layout %90 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked>
        %92 = arith.addi %91, %33 : tensor<16x8xi32, #blocked>
        %93 = tt.addptr %5, %92 : tensor<16x8x!tt.ptr<bf16>, #blocked>, tensor<16x8xi32, #blocked>
        %94 = tt.load %93 : tensor<16x8x!tt.ptr<bf16>, #blocked>
        %95 = ttg.convert_layout %85 : tensor<32x16xbf16, #blocked3> -> tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %96 = ttg.convert_layout %94 : tensor<16x8xbf16, #blocked> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %97 = ttg.convert_layout %arg6 : tensor<32x8xf32, #blocked> -> tensor<32x8xf32, #blocked>
        %98 = tt.dot %95, %96, %97 : tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<32x8xf32, #blocked>
        %c1_i32_11 = arith.constant 1 : i32
        %99 = arith.muli %c16_i32, %c1_i32_11 : i32
        %100 = arith.addi %arg5, %99 : i32
        %101 = tt.splat %100 : i32 -> tensor<16xi32, #blocked2>
        %102 = arith.addi %101, %3 : tensor<16xi32, #blocked2>
        %103 = ttg.convert_layout %102 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %104 = tt.expand_dims %103 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %105 = ttg.convert_layout %104 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %106 = tt.broadcast %105 : tensor<1x16xi32, #blocked3> -> tensor<32x16xi32, #blocked3>
        %107 = arith.addi %29, %106 : tensor<32x16xi32, #blocked3>
        %108 = tt.addptr %4, %107 : tensor<32x16x!tt.ptr<bf16>, #blocked3>, tensor<32x16xi32, #blocked3>
        %109 = tt.load %108 : tensor<32x16x!tt.ptr<bf16>, #blocked3>
        %110 = ttg.convert_layout %102 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %111 = tt.expand_dims %110 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %112 = ttg.convert_layout %111 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
        %113 = arith.muli %112, %cst_7 : tensor<16x1xi32, #blocked1>
        %114 = tt.broadcast %113 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
        %115 = ttg.convert_layout %114 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked>
        %116 = arith.addi %115, %33 : tensor<16x8xi32, #blocked>
        %117 = tt.addptr %5, %116 : tensor<16x8x!tt.ptr<bf16>, #blocked>, tensor<16x8xi32, #blocked>
        %118 = tt.load %117 : tensor<16x8x!tt.ptr<bf16>, #blocked>
        %119 = ttg.convert_layout %109 : tensor<32x16xbf16, #blocked3> -> tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %120 = ttg.convert_layout %118 : tensor<16x8xbf16, #blocked> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %121 = ttg.convert_layout %98 : tensor<32x8xf32, #blocked> -> tensor<32x8xf32, #blocked>
        %122 = tt.dot %119, %120, %121 : tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<32x8xf32, #blocked>
        scf.yield %122 : tensor<32x8xf32, #blocked>
      } {tt.disallow_acc_multi_buffer}
      %35 = tt.addptr %6, %23 : tensor<8x!tt.ptr<bf16>, #blocked2>, tensor<8xi32, #blocked2>
      %36 = tt.load %35 evictionPolicy = evict_last : tensor<8x!tt.ptr<bf16>, #blocked2>
      %37 = ttg.convert_layout %36 : tensor<8xbf16, #blocked2> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %38 = tt.expand_dims %37 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %39 = ttg.convert_layout %38 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked>
      %40 = arith.extf %39 : tensor<1x8xbf16, #blocked> to tensor<1x8xf32, #blocked>
      %41 = tt.broadcast %40 : tensor<1x8xf32, #blocked> -> tensor<32x8xf32, #blocked>
      %42 = arith.addf %34, %41 : tensor<32x8xf32, #blocked>
      %43 = arith.mulf %42, %cst_6 : tensor<32x8xf32, #blocked>
      %44 = arith.mulf %42, %cst_5 : tensor<32x8xf32, #blocked>
      %45 = tt.extern_elementwise %44 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x8xf32, #blocked>) -> tensor<32x8xf32, #blocked>
      %46 = arith.addf %45, %cst_4 : tensor<32x8xf32, #blocked>
      %47 = arith.mulf %43, %46 : tensor<32x8xf32, #blocked>
      %48 = arith.truncf %47 : tensor<32x8xf32, #blocked> to tensor<32x8xbf16, #blocked>
      %49 = arith.extsi %18 : i32 to i64
      %50 = arith.extsi %21 : i32 to i64
      %51 = tt.splat %49 : i64 -> tensor<32xi64, #blocked2>
      %52 = arith.addi %51, %7 : tensor<32xi64, #blocked2>
      %53 = ttg.convert_layout %52 : tensor<32xi64, #blocked2> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %54 = tt.expand_dims %53 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %55 = ttg.convert_layout %54 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %56 = tt.splat %50 : i64 -> tensor<8xi64, #blocked2>
      %57 = arith.addi %56, %8 : tensor<8xi64, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<8xi64, #blocked2> -> tensor<8xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %59 = tt.expand_dims %58 {axis = 0 : i32} : tensor<8xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi64, #blocked5>
      %60 = ttg.convert_layout %59 : tensor<1x8xi64, #blocked5> -> tensor<1x8xi64, #blocked>
      %61 = arith.muli %55, %cst_3 : tensor<32x1xi64, #blocked1>
      %62 = tt.broadcast %61 : tensor<32x1xi64, #blocked1> -> tensor<32x8xi64, #blocked1>
      %63 = ttg.convert_layout %62 : tensor<32x8xi64, #blocked1> -> tensor<32x8xi64, #blocked>
      %64 = tt.broadcast %60 : tensor<1x8xi64, #blocked> -> tensor<32x8xi64, #blocked>
      %65 = arith.addi %63, %64 : tensor<32x8xi64, #blocked>
      %66 = tt.addptr %9, %65 : tensor<32x8x!tt.ptr<bf16>, #blocked>, tensor<32x8xi64, #blocked>
      %67 = arith.cmpi sge, %55, %cst_2 : tensor<32x1xi64, #blocked1>
      %68 = arith.cmpi slt, %55, %cst_1 : tensor<32x1xi64, #blocked1>
      %69 = arith.andi %67, %68 : tensor<32x1xi1, #blocked1>
      %70 = tt.broadcast %69 : tensor<32x1xi1, #blocked1> -> tensor<32x8xi1, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<32x8xi1, #blocked1> -> tensor<32x8xi1, #blocked>
      %72 = arith.cmpi sge, %60, %cst_0 : tensor<1x8xi64, #blocked>
      %73 = arith.cmpi slt, %60, %cst : tensor<1x8xi64, #blocked>
      %74 = arith.andi %72, %73 : tensor<1x8xi1, #blocked>
      %75 = tt.broadcast %74 : tensor<1x8xi1, #blocked> -> tensor<32x8xi1, #blocked>
      %76 = arith.andi %71, %75 : tensor<32x8xi1, #blocked>
      tt.store %66, %48, %76 : tensor<32x8x!tt.ptr<bf16>, #blocked>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=1 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 8, 16], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 42.8 configs/s
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:55:33: note: - use: %98 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x32xbf16, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x32xf32, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [16, 2], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<64> : tensor<1x32xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x32xi64, #blocked>
    %cst_1 = arith.constant dense<32> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<64> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x32xbf16, #blocked>
    %c512_i32 = arith.constant 512 : i32
    %c4_i32 = arith.constant 4 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<16> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x2xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x2xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x2xf32, #blocked2>
    %cst_9 = arith.constant dense<16> : tensor<32x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x2xf32, #blocked2>
    %c2_i32 = arith.constant 2 : i32
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c32_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 2 : i32, start = 0 : i32} : tensor<2xi32, #blocked3>
    %6 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x32x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<32x2x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<2x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x2x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c4_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c2_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<2xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<2xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x32xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x32xi64, #blocked1> -> tensor<8x32xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x32xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x32xi1, #blocked1> -> tensor<8x32xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<2xi32, #blocked3> -> tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x2xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x2xi32, #blocked5> -> tensor<1x2xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x2xi32, #blocked2> -> tensor<32x2xi32, #blocked2>
      %c64_i32_11 = arith.constant 64 : i32
      %cst_12 = arith.constant dense<0> : tensor<32xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<32xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<32xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<32xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x32xi64, #blocked5> -> tensor<1x32xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x32xi64, #blocked> -> tensor<8x32xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x32xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x32x!tt.ptr<bf16>, #blocked>, tensor<8x32xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x32xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x32xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x32xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x32xi1, #blocked> -> tensor<8x32xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x32xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x32x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<32x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<32x1xi32, #blocked1> -> tensor<32x2xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<32x2xi32, #blocked1> -> tensor<32x2xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<32x2xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<32x2x!tt.ptr<bf16>, #blocked2>, tensor<32x2xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<32x2x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x32xbf16, #blocked> -> tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<32x2xbf16, #blocked2> -> tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x2xf32, #blocked2> -> tensor<8x2xf32, #blocked2>
      %73 = tt.dot %70, %71, %72 : tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x2xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c32_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<32xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<32xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<32xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<32xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<32xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x32xi64, #blocked5> -> tensor<1x32xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x32xi64, #blocked> -> tensor<8x32xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x32xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x32x!tt.ptr<bf16>, #blocked>, tensor<8x32xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x32xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x32xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x32xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x32xi1, #blocked> -> tensor<8x32xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x32xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x32x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<32x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<32x1xi32, #blocked1> -> tensor<32x2xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<32x2xi32, #blocked1> -> tensor<32x2xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<32x2xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<32x2x!tt.ptr<bf16>, #blocked2>, tensor<32x2xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<32x2x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x32xbf16, #blocked> -> tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<32x2xbf16, #blocked2> -> tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x2xf32, #blocked2> -> tensor<8x2xf32, #blocked2>
      %105 = tt.dot %102, %103, %104 : tensor<8x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<32x2xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x2xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<2x!tt.ptr<bf16>, #blocked3>, tensor<2xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_last : tensor<2x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<2xbf16, #blocked3> -> tensor<2xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<2xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x2xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x2xbf16, #blocked5> -> tensor<1x2xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x2xbf16, #blocked2> to tensor<1x2xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x2xf32, #blocked2> -> tensor<8x2xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x2xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x2xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x2xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x2xf32, #blocked2>) -> tensor<8x2xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x2xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x2xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x2xf32, #blocked2> to tensor<8x2xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<2xi32, #blocked3> -> tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<2xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x2xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x2xi32, #blocked5> -> tensor<1x2xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x2xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x2xi32, #blocked1> -> tensor<8x2xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x2xi32, #blocked2> -> tensor<8x2xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x2xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x2x!tt.ptr<bf16>, #blocked2>, tensor<8x2xi32, #blocked2>
      tt.store %131, %119 : tensor<8x2x!tt.ptr<bf16>, #blocked2>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=7 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/uv/cuvzvzionyhms7elmjhuh7nsbnj6nfwrm2gd4hzcwsdjcy5uvhjl.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[4s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 2, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=4, pid_type='persistent_blocked', range_flattens=[True, True], range_multi_buffers=[None, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:52:85: error: 'tt.load' op operation destroyed but still has uses
            a_tile = tl.load(x + (indices_0[:, None] * 64 + indices_2[None, :] * 1), None)
                                                                                    ^
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:52:85: note: - use: %66 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<32x16xbf16, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>>) -> tensor<32x16xf32, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [8], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [8, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [8, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 8], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 8 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<16> : tensor<1x8xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x8xi64, #blocked>
    %cst_1 = arith.constant dense<32> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<16> : tensor<32x1xi64, #blocked1>
    %c2_i32 = arith.constant 2 : i32
    %c1_i32 = arith.constant 1 : i32
    %c64_i32 = arith.constant 64 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst_4 = arith.constant dense<1.000000e+00> : tensor<32x8xf32, #blocked>
    %cst_5 = arith.constant dense<0.707106769> : tensor<32x8xf32, #blocked>
    %cst_6 = arith.constant dense<5.000000e-01> : tensor<32x8xf32, #blocked>
    %cst_7 = arith.constant dense<16> : tensor<16x1xi32, #blocked1>
    %cst_8 = arith.constant dense<64> : tensor<32x1xi32, #blocked1>
    %cst_9 = arith.constant dense<0.000000e+00> : tensor<32x8xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %0 = tt.get_program_id x : i32
    %1 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked2>
    %2 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked2>
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %4 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked3>
    %5 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked>
    %6 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked2>
    %7 = arith.extsi %1 : tensor<32xi32, #blocked2> to tensor<32xi64, #blocked2>
    %8 = arith.extsi %2 : tensor<8xi32, #blocked2> to tensor<8xi64, #blocked2>
    %9 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x8x!tt.ptr<bf16>, #blocked>
    scf.for %arg4 = %0 to %c2_i32 step %c16_i32  : i32 {
      %10 = arith.divsi %arg4, %c16_i32 : i32
      %11 = arith.muli %10, %c8_i32 : i32
      %12 = arith.subi %c1_i32, %11 : i32
      %13 = arith.minsi %12, %c8_i32 : i32
      %14 = arith.remsi %arg4, %c16_i32 : i32
      %15 = arith.remsi %14, %13 : i32
      %16 = arith.addi %11, %15 : i32
      %17 = arith.divsi %14, %13 : i32
      %18 = arith.muli %16, %c32_i32 : i32
      %19 = tt.splat %18 : i32 -> tensor<32xi32, #blocked2>
      %20 = arith.addi %19, %1 : tensor<32xi32, #blocked2>
      %21 = arith.muli %17, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked2>
      %23 = arith.addi %22, %2 : tensor<8xi32, #blocked2>
      %24 = ttg.convert_layout %20 : tensor<32xi32, #blocked2> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %25 = tt.expand_dims %24 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %26 = ttg.convert_layout %25 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %27 = arith.muli %26, %cst_8 : tensor<32x1xi32, #blocked1>
      %28 = tt.broadcast %27 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %29 = ttg.convert_layout %28 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked3>
      %30 = ttg.convert_layout %23 : tensor<8xi32, #blocked2> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %31 = tt.expand_dims %30 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %32 = ttg.convert_layout %31 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked>
      %33 = tt.broadcast %32 : tensor<1x8xi32, #blocked> -> tensor<16x8xi32, #blocked>
      %c32_i32_10 = arith.constant 32 : i32
      %34 = scf.for %arg5 = %c0_i32 to %c64_i32 step %c32_i32_10 iter_args(%arg6 = %cst_9) -> (tensor<32x8xf32, #blocked>)  : i32 {
        %77 = tt.splat %arg5 : i32 -> tensor<16xi32, #blocked2>
        %78 = arith.addi %77, %3 : tensor<16xi32, #blocked2>
        %79 = ttg.convert_layout %78 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %81 = ttg.convert_layout %80 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %82 = tt.broadcast %81 : tensor<1x16xi32, #blocked3> -> tensor<32x16xi32, #blocked3>
        %83 = arith.addi %29, %82 : tensor<32x16xi32, #blocked3>
        %84 = tt.addptr %4, %83 : tensor<32x16x!tt.ptr<bf16>, #blocked3>, tensor<32x16xi32, #blocked3>
        %85 = tt.load %84 : tensor<32x16x!tt.ptr<bf16>, #blocked3>
        %86 = ttg.convert_layout %78 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %87 = tt.expand_dims %86 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %88 = ttg.convert_layout %87 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
        %89 = arith.muli %88, %cst_7 : tensor<16x1xi32, #blocked1>
        %90 = tt.broadcast %89 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
        %91 = ttg.convert_layout %90 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked>
        %92 = arith.addi %91, %33 : tensor<16x8xi32, #blocked>
        %93 = tt.addptr %5, %92 : tensor<16x8x!tt.ptr<bf16>, #blocked>, tensor<16x8xi32, #blocked>
        %94 = tt.load %93 : tensor<16x8x!tt.ptr<bf16>, #blocked>
        %95 = ttg.convert_layout %85 : tensor<32x16xbf16, #blocked3> -> tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %96 = ttg.convert_layout %94 : tensor<16x8xbf16, #blocked> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %97 = ttg.convert_layout %arg6 : tensor<32x8xf32, #blocked> -> tensor<32x8xf32, #blocked>
        %98 = tt.dot %95, %96, %97 : tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<32x8xf32, #blocked>
        %c1_i32_11 = arith.constant 1 : i32
        %99 = arith.muli %c16_i32, %c1_i32_11 : i32
        %100 = arith.addi %arg5, %99 : i32
        %101 = tt.splat %100 : i32 -> tensor<16xi32, #blocked2>
        %102 = arith.addi %101, %3 : tensor<16xi32, #blocked2>
        %103 = ttg.convert_layout %102 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %104 = tt.expand_dims %103 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %105 = ttg.convert_layout %104 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %106 = tt.broadcast %105 : tensor<1x16xi32, #blocked3> -> tensor<32x16xi32, #blocked3>
        %107 = arith.addi %29, %106 : tensor<32x16xi32, #blocked3>
        %108 = tt.addptr %4, %107 : tensor<32x16x!tt.ptr<bf16>, #blocked3>, tensor<32x16xi32, #blocked3>
        %109 = tt.load %108 : tensor<32x16x!tt.ptr<bf16>, #blocked3>
        %110 = ttg.convert_layout %102 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %111 = tt.expand_dims %110 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %112 = ttg.convert_layout %111 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
        %113 = arith.muli %112, %cst_7 : tensor<16x1xi32, #blocked1>
        %114 = tt.broadcast %113 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
        %115 = ttg.convert_layout %114 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked>
        %116 = arith.addi %115, %33 : tensor<16x8xi32, #blocked>
        %117 = tt.addptr %5, %116 : tensor<16x8x!tt.ptr<bf16>, #blocked>, tensor<16x8xi32, #blocked>
        %118 = tt.load %117 : tensor<16x8x!tt.ptr<bf16>, #blocked>
        %119 = ttg.convert_layout %109 : tensor<32x16xbf16, #blocked3> -> tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %120 = ttg.convert_layout %118 : tensor<16x8xbf16, #blocked> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %121 = ttg.convert_layout %98 : tensor<32x8xf32, #blocked> -> tensor<32x8xf32, #blocked>
        %122 = tt.dot %119, %120, %121 : tensor<32x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<32x8xf32, #blocked>
        scf.yield %122 : tensor<32x8xf32, #blocked>
      } {tt.disallow_acc_multi_buffer}
      %35 = tt.addptr %6, %23 : tensor<8x!tt.ptr<bf16>, #blocked2>, tensor<8xi32, #blocked2>
      %36 = tt.load %35 evictionPolicy = evict_last : tensor<8x!tt.ptr<bf16>, #blocked2>
      %37 = ttg.convert_layout %36 : tensor<8xbf16, #blocked2> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %38 = tt.expand_dims %37 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %39 = ttg.convert_layout %38 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked>
      %40 = arith.extf %39 : tensor<1x8xbf16, #blocked> to tensor<1x8xf32, #blocked>
      %41 = tt.broadcast %40 : tensor<1x8xf32, #blocked> -> tensor<32x8xf32, #blocked>
      %42 = arith.addf %34, %41 : tensor<32x8xf32, #blocked>
      %43 = arith.mulf %42, %cst_6 : tensor<32x8xf32, #blocked>
      %44 = arith.mulf %42, %cst_5 : tensor<32x8xf32, #blocked>
      %45 = tt.extern_elementwise %44 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x8xf32, #blocked>) -> tensor<32x8xf32, #blocked>
      %46 = arith.addf %45, %cst_4 : tensor<32x8xf32, #blocked>
      %47 = arith.mulf %43, %46 : tensor<32x8xf32, #blocked>
      %48 = arith.truncf %47 : tensor<32x8xf32, #blocked> to tensor<32x8xbf16, #blocked>
      %49 = arith.extsi %18 : i32 to i64
      %50 = arith.extsi %21 : i32 to i64
      %51 = tt.splat %49 : i64 -> tensor<32xi64, #blocked2>
      %52 = arith.addi %51, %7 : tensor<32xi64, #blocked2>
      %53 = ttg.convert_layout %52 : tensor<32xi64, #blocked2> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %54 = tt.expand_dims %53 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %55 = ttg.convert_layout %54 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %56 = tt.splat %50 : i64 -> tensor<8xi64, #blocked2>
      %57 = arith.addi %56, %8 : tensor<8xi64, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<8xi64, #blocked2> -> tensor<8xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %59 = tt.expand_dims %58 {axis = 0 : i32} : tensor<8xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi64, #blocked5>
      %60 = ttg.convert_layout %59 : tensor<1x8xi64, #blocked5> -> tensor<1x8xi64, #blocked>
      %61 = arith.muli %55, %cst_3 : tensor<32x1xi64, #blocked1>
      %62 = tt.broadcast %61 : tensor<32x1xi64, #blocked1> -> tensor<32x8xi64, #blocked1>
      %63 = ttg.convert_layout %62 : tensor<32x8xi64, #blocked1> -> tensor<32x8xi64, #blocked>
      %64 = tt.broadcast %60 : tensor<1x8xi64, #blocked> -> tensor<32x8xi64, #blocked>
      %65 = arith.addi %63, %64 : tensor<32x8xi64, #blocked>
      %66 = tt.addptr %9, %65 : tensor<32x8x!tt.ptr<bf16>, #blocked>, tensor<32x8xi64, #blocked>
      %67 = arith.cmpi sge, %55, %cst_2 : tensor<32x1xi64, #blocked1>
      %68 = arith.cmpi slt, %55, %cst_1 : tensor<32x1xi64, #blocked1>
      %69 = arith.andi %67, %68 : tensor<32x1xi1, #blocked1>
      %70 = tt.broadcast %69 : tensor<32x1xi1, #blocked1> -> tensor<32x8xi1, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<32x8xi1, #blocked1> -> tensor<32x8xi1, #blocked>
      %72 = arith.cmpi sge, %60, %cst_0 : tensor<1x8xi64, #blocked>
      %73 = arith.cmpi slt, %60, %cst : tensor<1x8xi64, #blocked>
      %74 = arith.andi %72, %73 : tensor<1x8xi1, #blocked>
      %75 = tt.broadcast %74 : tensor<1x8xi1, #blocked> -> tensor<32x8xi1, #blocked>
      %76 = arith.andi %71, %75 : tensor<32x8xi1, #blocked>
      tt.store %66, %48, %76 : tensor<32x8x!tt.ptr<bf16>, #blocked>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=1 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/iu/ciuxmfm256rb4ypfnxf4sn4qg7ce62wfxwhonca5yvqaccxwys4z.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[5s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 8, 16], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 17.6 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 17.9 configs/s
[65s] Initial random population of 100, 5 starting points: error=2 ok=98 min=0.0070 mid=0.0085 max=0.0468 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[65s] Generation 1 starting: 213 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 45.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 17.7 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.4 configs/s
[271s] Generation 1 complete: ok=218 min=0.0068 mid=0.0073 max=0.0128 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[271s] Generation 2 starting: 206 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 42.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 18.3 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.5 configs/s
[470s] Generation 2 complete: ok=211 min=0.0068 mid=0.0072 max=0.0129 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[470s] Generation 3 starting: 203 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203/203 38.6 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203/203 18.3 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.7 configs/s
[667s] Generation 3 complete: ok=208 min=0.0070 mid=0.0074 max=0.0131 best=Config(block_sizes=[4, 4, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', 'last', 'first'], loop_orders=[[1, 0]], num_stages=6, num_warps=1, pid_type='xyz', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[667s] Generation 4 starting: 202 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 41.6 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 18.1 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.7 configs/s
[862s] Generation 4 complete: ok=207 min=0.0068 mid=0.0072 max=0.0129 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[862s] Generation 5 starting: 197 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 43.0 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 18.3 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.8 configs/s
[1053s] Generation 5 complete: ok=202 min=0.0068 mid=0.0073 max=0.0130 best=Config(block_sizes=[2, 16, 32], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1053s] Generation 6 starting: 195 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 42.1 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 18.1 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.9 configs/s
[1242s] Generation 6 complete: ok=200 min=0.0068 mid=0.0072 max=0.0129 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=2, pid_type='persistent_blocked', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[1, 1], range_warp_specializes=[False, None])
[1242s] Generation 7 starting: 194 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 36.9 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 18.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.8 configs/s
[1430s] Generation 7 complete: ok=199 min=0.0067 mid=0.0071 max=0.0128 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=2, pid_type='persistent_blocked', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[1, 1], range_warp_specializes=[False, None])
[1430s] Generation 8 starting: 151 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 42.2 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 18.2 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.6 configs/s
[1577s] Generation 8 complete: ok=155 min=0.0067 mid=0.0071 max=0.0129 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1577s] Generation 9 starting: 156 neighbors, 4 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156/156 39.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 156/156 18.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.3 configs/s
[1727s] Generation 9 complete: ok=160 min=0.0067 mid=0.0071 max=0.0129 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=1, pid_type='persistent_blocked', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[1, 1], range_warp_specializes=[False, None])
[1727s] Generation 10 starting: 151 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 40.7 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 18.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.7 configs/s
[1872s] Generation 10 complete: ok=155 min=0.0069 mid=0.0072 max=0.0129 best=Config(block_sizes=[16, 4, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], num_stages=7, num_warps=1, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[True, False], range_num_stages=[4, 4], range_unroll_factors=[4, 0], range_warp_specializes=[False, True])
[1872s] Generation 11 starting: 153 neighbors, 4 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153/153 40.2 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 153/153 18.0 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.5 configs/s
[2021s] Generation 11 complete: ok=157 min=0.0069 mid=0.0073 max=0.0122 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2021s] Generation 12 starting: 117 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 41.6 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 18.3 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.8 configs/s
[2135s] Generation 12 complete: ok=120 min=0.0069 mid=0.0072 max=0.0123 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2135s] Generation 13 starting: 71 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 43.2 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.3 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.9 configs/s
[2204s] Generation 13 complete: ok=73 min=0.0068 mid=0.0071 max=0.0082 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2204s] Generation 14 starting: 71 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 45.3 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.3 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.9 configs/s
[2274s] Generation 14 complete: ok=73 min=0.0067 mid=0.0070 max=0.0081 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2274s] Generation 15 starting: 71 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 39.1 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 18.3 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.9 configs/s
[2343s] Generation 15 complete: ok=73 min=0.0069 mid=0.0072 max=0.0082 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2343s] Generation 16 starting: 67 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67/67 44.8 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67/67 18.3 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.7 configs/s
[2409s] Generation 16 complete: ok=69 min=0.0067 mid=0.0070 max=0.0081 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2409s] Generation 17 starting: 30 neighbors, 1 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30/30 55.8 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 30/30 18.4 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 37.4 configs/s
[2438s] Generation 17 complete: ok=31 min=0.0070 mid=0.0070 max=0.0072 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2438s] Generation 18 starting: 33 neighbors, 1 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 52.7 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33/33 17.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 33.8 configs/s
[2471s] Generation 18 complete: ok=34 min=0.0070 mid=0.0072 max=0.0075 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2471s] Generation 19 starting: 32 neighbors, 1 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32/32 40.6 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32/32 18.3 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 35.4 configs/s
[2503s] Generation 19 complete: ok=33 min=0.0069 mid=0.0072 max=0.0075 best=Config(block_sizes=[2, 16, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2503s] Generation 20 starting: 34 neighbors, 1 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 49.8 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34/34 18.3 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 33.1 configs/s
[2536s] Generation 20 complete: ok=35 min=0.0068 mid=0.0071 max=0.0073 best=Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[2536s] Autotuning complete in 2536.4s after searching 2647 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[2, 16, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, True]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 0:
(M, K, N)
------------
(32, 64, 16)
 20%|██        | 1/5 [42:23<2:49:35, 2543.83s/it]WARNING:tritonbench.utils.triton_op:Running input ID 1:
(M, K, N)
--------------
(128, 256, 64)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 30, "num_triton_choices": 30, "best_kernel": "triton_mm_13", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=256, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2", "best_time": 0.0075599998235702515, "best_triton_pos": 0}
AUTOTUNE mm(128x256, 256x64)
strides: [256, 1], [64, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_13 0.0076 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=256, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_15 0.0076 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_12 0.0079 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=256, BLOCK_M=16, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=8, num_stages=2, num_warps=1
  triton_mm_17 0.0079 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_21 0.0082 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_20 0.0084 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_29 0.0086 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_19 0.0087 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_24 0.0087 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_16 0.0087 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4005 seconds and 0.3646 seconds precompiling for 30 choices
INFO:tritonbench.utils.triton_op:Took 1385.40ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 130.01ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (128, 256),
              'stride': (256, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (256, 64),
              'stride': (64, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (64,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 2.59ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 704392404
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:48:86: error: 'tt.load' op operation destroyed but still has uses
            a_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                     ^
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:48:86: note: - use: %64 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<4x16xbf16, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<4x16xf32, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c256_i32 = arith.constant 256 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<4x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<4x8xf32, #blocked1>
    %cst_1 = arith.constant dense<0.707106769> : tensor<4x8xf32, #blocked1>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<4x8xf32, #blocked1>
    %cst_3 = arith.constant dense<64> : tensor<16x1xi32, #blocked>
    %cst_4 = arith.constant dense<256> : tensor<4x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<4x8xf32, #blocked1>
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c16_i32 : i32
    %2 = arith.addi %1, %c16_i32 : i32
    %3 = arith.minsi %2, %c256_i32 : i32
    %4 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #blocked2>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked2>
    %6 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<4x16x!tt.ptr<bf16>, #blocked3>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked1>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<4x8x!tt.ptr<bf16>, #blocked1>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c128_i32 : i32
      %12 = arith.muli %11, %c16_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c16_i32 : i32
      %15 = arith.remsi %arg4, %c128_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c4_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<4xi32, #blocked2>
      %21 = arith.addi %20, %4 : tensor<4xi32, #blocked2>
      %22 = arith.muli %18, %c8_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<8xi32, #blocked2>
      %24 = arith.addi %23, %5 : tensor<8xi32, #blocked2>
      %25 = ttg.convert_layout %21 : tensor<4xi32, #blocked2> -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %26 = tt.expand_dims %25 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<4x1xi32, #blocked4>
      %27 = ttg.convert_layout %26 : tensor<4x1xi32, #blocked4> -> tensor<4x1xi32, #blocked>
      %28 = arith.muli %27, %cst_4 : tensor<4x1xi32, #blocked>
      %29 = tt.broadcast %28 : tensor<4x1xi32, #blocked> -> tensor<4x16xi32, #blocked>
      %30 = ttg.convert_layout %29 : tensor<4x16xi32, #blocked> -> tensor<4x16xi32, #blocked3>
      %31 = ttg.convert_layout %24 : tensor<8xi32, #blocked2> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %32 = tt.expand_dims %31 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %33 = ttg.convert_layout %32 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked1>
      %34 = tt.broadcast %33 : tensor<1x8xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %c32_i32_6 = arith.constant 32 : i32
      %35 = scf.for %arg5 = %c0_i32 to %c256_i32 step %c32_i32_6 iter_args(%arg6 = %cst_5) -> (tensor<4x8xf32, #blocked1>)  : i32 {
        %62 = tt.splat %arg5 : i32 -> tensor<16xi32, #blocked2>
        %63 = arith.addi %62, %6 : tensor<16xi32, #blocked2>
        %64 = ttg.convert_layout %63 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %65 = tt.expand_dims %64 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %66 = ttg.convert_layout %65 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %67 = tt.broadcast %66 : tensor<1x16xi32, #blocked3> -> tensor<4x16xi32, #blocked3>
        %68 = arith.addi %30, %67 : tensor<4x16xi32, #blocked3>
        %69 = tt.addptr %7, %68 : tensor<4x16x!tt.ptr<bf16>, #blocked3>, tensor<4x16xi32, #blocked3>
        %70 = tt.load %69 evictionPolicy = evict_last : tensor<4x16x!tt.ptr<bf16>, #blocked3>
        %71 = ttg.convert_layout %63 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %72 = tt.expand_dims %71 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %73 = ttg.convert_layout %72 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked>
        %74 = arith.muli %73, %cst_3 : tensor<16x1xi32, #blocked>
        %75 = tt.broadcast %74 : tensor<16x1xi32, #blocked> -> tensor<16x8xi32, #blocked>
        %76 = ttg.convert_layout %75 : tensor<16x8xi32, #blocked> -> tensor<16x8xi32, #blocked1>
        %77 = arith.addi %76, %34 : tensor<16x8xi32, #blocked1>
        %78 = tt.addptr %8, %77 : tensor<16x8x!tt.ptr<bf16>, #blocked1>, tensor<16x8xi32, #blocked1>
        %79 = tt.load %78 : tensor<16x8x!tt.ptr<bf16>, #blocked1>
        %80 = ttg.convert_layout %70 : tensor<4x16xbf16, #blocked3> -> tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
        %81 = ttg.convert_layout %79 : tensor<16x8xbf16, #blocked1> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
        %82 = ttg.convert_layout %arg6 : tensor<4x8xf32, #blocked1> -> tensor<4x8xf32, #blocked1>
        %83 = tt.dot %80, %81, %82 : tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<4x8xf32, #blocked1>
        %c1_i32_7 = arith.constant 1 : i32
        %84 = arith.muli %c16_i32, %c1_i32_7 : i32
        %85 = arith.addi %arg5, %84 : i32
        %86 = tt.splat %85 : i32 -> tensor<16xi32, #blocked2>
        %87 = arith.addi %86, %6 : tensor<16xi32, #blocked2>
        %88 = ttg.convert_layout %87 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %89 = tt.expand_dims %88 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %90 = ttg.convert_layout %89 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %91 = tt.broadcast %90 : tensor<1x16xi32, #blocked3> -> tensor<4x16xi32, #blocked3>
        %92 = arith.addi %30, %91 : tensor<4x16xi32, #blocked3>
        %93 = tt.addptr %7, %92 : tensor<4x16x!tt.ptr<bf16>, #blocked3>, tensor<4x16xi32, #blocked3>
        %94 = tt.load %93 evictionPolicy = evict_last : tensor<4x16x!tt.ptr<bf16>, #blocked3>
        %95 = ttg.convert_layout %87 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %96 = tt.expand_dims %95 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %97 = ttg.convert_layout %96 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked>
        %98 = arith.muli %97, %cst_3 : tensor<16x1xi32, #blocked>
        %99 = tt.broadcast %98 : tensor<16x1xi32, #blocked> -> tensor<16x8xi32, #blocked>
        %100 = ttg.convert_layout %99 : tensor<16x8xi32, #blocked> -> tensor<16x8xi32, #blocked1>
        %101 = arith.addi %100, %34 : tensor<16x8xi32, #blocked1>
        %102 = tt.addptr %8, %101 : tensor<16x8x!tt.ptr<bf16>, #blocked1>, tensor<16x8xi32, #blocked1>
        %103 = tt.load %102 : tensor<16x8x!tt.ptr<bf16>, #blocked1>
        %104 = ttg.convert_layout %94 : tensor<4x16xbf16, #blocked3> -> tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
        %105 = ttg.convert_layout %103 : tensor<16x8xbf16, #blocked1> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
        %106 = ttg.convert_layout %83 : tensor<4x8xf32, #blocked1> -> tensor<4x8xf32, #blocked1>
        %107 = tt.dot %104, %105, %106 : tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<4x8xf32, #blocked1>
        scf.yield %107 : tensor<4x8xf32, #blocked1>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %36 = tt.addptr %9, %24 : tensor<8x!tt.ptr<bf16>, #blocked2>, tensor<8xi32, #blocked2>
      %37 = tt.load %36 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked2>
      %38 = ttg.convert_layout %37 : tensor<8xbf16, #blocked2> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %39 = tt.expand_dims %38 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %40 = ttg.convert_layout %39 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked1>
      %41 = arith.extf %40 : tensor<1x8xbf16, #blocked1> to tensor<1x8xf32, #blocked1>
      %42 = tt.broadcast %41 : tensor<1x8xf32, #blocked1> -> tensor<4x8xf32, #blocked1>
      %43 = arith.addf %35, %42 : tensor<4x8xf32, #blocked1>
      %44 = arith.mulf %43, %cst_2 : tensor<4x8xf32, #blocked1>
      %45 = arith.mulf %43, %cst_1 : tensor<4x8xf32, #blocked1>
      %46 = tt.extern_elementwise %45 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<4x8xf32, #blocked1>) -> tensor<4x8xf32, #blocked1>
      %47 = arith.addf %46, %cst_0 : tensor<4x8xf32, #blocked1>
      %48 = arith.mulf %44, %47 : tensor<4x8xf32, #blocked1>
      %49 = arith.truncf %48 : tensor<4x8xf32, #blocked1> to tensor<4x8xbf16, #blocked1>
      %50 = ttg.convert_layout %21 : tensor<4xi32, #blocked2> -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %51 = tt.expand_dims %50 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<4x1xi32, #blocked4>
      %52 = ttg.convert_layout %51 : tensor<4x1xi32, #blocked4> -> tensor<4x1xi32, #blocked>
      %53 = arith.muli %52, %cst : tensor<4x1xi32, #blocked>
      %54 = ttg.convert_layout %24 : tensor<8xi32, #blocked2> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %55 = tt.expand_dims %54 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %56 = ttg.convert_layout %55 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked1>
      %57 = tt.broadcast %53 : tensor<4x1xi32, #blocked> -> tensor<4x8xi32, #blocked>
      %58 = ttg.convert_layout %57 : tensor<4x8xi32, #blocked> -> tensor<4x8xi32, #blocked1>
      %59 = tt.broadcast %56 : tensor<1x8xi32, #blocked1> -> tensor<4x8xi32, #blocked1>
      %60 = arith.addi %58, %59 : tensor<4x8xi32, #blocked1>
      %61 = tt.addptr %10, %60 : tensor<4x8x!tt.ptr<bf16>, #blocked1>, tensor<4x8xi32, #blocked1>
      tt.store %61, %49 : tensor<4x8x!tt.ptr<bf16>, #blocked1>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=7 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:13:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[4, 8, 16], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], num_stages=7, num_warps=1, pid_type='persistent_blocked', range_flattens=[True, False], range_multi_buffers=[False, False], range_num_stages=[4, 1], range_unroll_factors=[0, 2], range_warp_specializes=[None, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:48:86: error: 'tt.load' op operation destroyed but still has uses
            a_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                     ^
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:48:86: note: - use: %61 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x32xbf16, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x32xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c32_i32 = arith.constant 32 : i32
    %c256_i32 = arith.constant 256 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<16x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<16x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<16x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<16x1xf32, #blocked>
    %cst_3 = arith.constant dense<64> : tensor<32x1xi32, #blocked>
    %cst_4 = arith.constant dense<256> : tensor<16x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<16x1xf32, #blocked>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c32_i32 : i32
    %2 = arith.addi %1, %c32_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked1>
    %5 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked1>
    %6 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    %7 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<32x1x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x1x!tt.ptr<bf16>, #blocked>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %9 = arith.divsi %arg4, %c128_i32 : i32
      %10 = arith.muli %9, %c16_i32 : i32
      %11 = arith.subi %c64_i32, %10 : i32
      %12 = arith.minsi %11, %c16_i32 : i32
      %13 = arith.remsi %arg4, %c128_i32 : i32
      %14 = arith.remsi %13, %12 : i32
      %15 = arith.addi %10, %14 : i32
      %16 = arith.divsi %13, %12 : i32
      %17 = arith.muli %16, %c16_i32 : i32
      %18 = tt.splat %17 : i32 -> tensor<16xi32, #blocked1>
      %19 = arith.addi %18, %4 : tensor<16xi32, #blocked1>
      %20 = ttg.convert_layout %19 : tensor<16xi32, #blocked1> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %21 = tt.expand_dims %20 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
      %22 = ttg.convert_layout %21 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked>
      %23 = arith.muli %22, %cst_4 : tensor<16x1xi32, #blocked>
      %24 = tt.broadcast %23 : tensor<16x1xi32, #blocked> -> tensor<16x32xi32, #blocked>
      %25 = ttg.convert_layout %24 : tensor<16x32xi32, #blocked> -> tensor<16x32xi32, #blocked2>
      %26 = tt.splat %15 : i32 -> tensor<32x1xi32, #blocked>
      %c64_i32_6 = arith.constant 64 : i32
      %27 = scf.for %arg5 = %c0_i32 to %c256_i32 step %c64_i32_6 iter_args(%arg6 = %cst_5) -> (tensor<16x1xf32, #blocked>)  : i32 {
        %50 = tt.splat %arg5 : i32 -> tensor<32xi32, #blocked1>
        %51 = arith.addi %50, %5 : tensor<32xi32, #blocked1>
        %52 = ttg.convert_layout %51 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
        %53 = tt.expand_dims %52 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x32xi32, #blocked4>
        %54 = ttg.convert_layout %53 : tensor<1x32xi32, #blocked4> -> tensor<1x32xi32, #blocked2>
        %55 = tt.broadcast %54 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
        %56 = arith.addi %25, %55 : tensor<16x32xi32, #blocked2>
        %57 = tt.addptr %6, %56 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
        %58 = tt.load %57 evictionPolicy = evict_last : tensor<16x32x!tt.ptr<bf16>, #blocked2>
        %59 = ttg.convert_layout %51 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<32x1xi32, #blocked3>
        %61 = ttg.convert_layout %60 : tensor<32x1xi32, #blocked3> -> tensor<32x1xi32, #blocked>
        %62 = arith.muli %61, %cst_3 : tensor<32x1xi32, #blocked>
        %63 = arith.addi %62, %26 : tensor<32x1xi32, #blocked>
        %64 = tt.addptr %7, %63 : tensor<32x1x!tt.ptr<bf16>, #blocked>, tensor<32x1xi32, #blocked>
        %65 = tt.load %64 : tensor<32x1x!tt.ptr<bf16>, #blocked>
        %66 = ttg.convert_layout %58 : tensor<16x32xbf16, #blocked2> -> tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %67 = ttg.convert_layout %65 : tensor<32x1xbf16, #blocked> -> tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %68 = ttg.convert_layout %arg6 : tensor<16x1xf32, #blocked> -> tensor<16x1xf32, #blocked>
        %69 = tt.dot %66, %67, %68 : tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<16x1xf32, #blocked>
        %c1_i32_7 = arith.constant 1 : i32
        %70 = arith.muli %c32_i32, %c1_i32_7 : i32
        %71 = arith.addi %arg5, %70 : i32
        %72 = tt.splat %71 : i32 -> tensor<32xi32, #blocked1>
        %73 = arith.addi %72, %5 : tensor<32xi32, #blocked1>
        %74 = ttg.convert_layout %73 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
        %75 = tt.expand_dims %74 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x32xi32, #blocked4>
        %76 = ttg.convert_layout %75 : tensor<1x32xi32, #blocked4> -> tensor<1x32xi32, #blocked2>
        %77 = tt.broadcast %76 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
        %78 = arith.addi %25, %77 : tensor<16x32xi32, #blocked2>
        %79 = tt.addptr %6, %78 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
        %80 = tt.load %79 evictionPolicy = evict_last : tensor<16x32x!tt.ptr<bf16>, #blocked2>
        %81 = ttg.convert_layout %73 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
        %82 = tt.expand_dims %81 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<32x1xi32, #blocked3>
        %83 = ttg.convert_layout %82 : tensor<32x1xi32, #blocked3> -> tensor<32x1xi32, #blocked>
        %84 = arith.muli %83, %cst_3 : tensor<32x1xi32, #blocked>
        %85 = arith.addi %84, %26 : tensor<32x1xi32, #blocked>
        %86 = tt.addptr %7, %85 : tensor<32x1x!tt.ptr<bf16>, #blocked>, tensor<32x1xi32, #blocked>
        %87 = tt.load %86 : tensor<32x1x!tt.ptr<bf16>, #blocked>
        %88 = ttg.convert_layout %80 : tensor<16x32xbf16, #blocked2> -> tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %89 = ttg.convert_layout %87 : tensor<32x1xbf16, #blocked> -> tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %90 = ttg.convert_layout %69 : tensor<16x1xf32, #blocked> -> tensor<16x1xf32, #blocked>
        %91 = tt.dot %88, %89, %90 : tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<16x1xf32, #blocked>
        scf.yield %91 : tensor<16x1xf32, #blocked>
      } {tt.num_stages = 1 : i32}
      %28 = tt.addptr %arg2, %15 : !tt.ptr<bf16>, i32
      %29 = tt.splat %28 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
      %30 = tt.load %29 : tensor<1x!tt.ptr<bf16>, #blocked1>
      %31 = ttg.convert_layout %30 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %32 = tt.expand_dims %31 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
      %33 = ttg.convert_layout %32 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
      %34 = arith.extf %33 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
      %35 = tt.broadcast %34 : tensor<1x1xf32, #blocked> -> tensor<16x1xf32, #blocked>
      %36 = arith.addf %27, %35 : tensor<16x1xf32, #blocked>
      %37 = arith.mulf %36, %cst_2 : tensor<16x1xf32, #blocked>
      %38 = arith.mulf %36, %cst_1 : tensor<16x1xf32, #blocked>
      %39 = tt.extern_elementwise %38 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x1xf32, #blocked>) -> tensor<16x1xf32, #blocked>
      %40 = arith.addf %39, %cst_0 : tensor<16x1xf32, #blocked>
      %41 = arith.mulf %37, %40 : tensor<16x1xf32, #blocked>
      %42 = arith.truncf %41 : tensor<16x1xf32, #blocked> to tensor<16x1xbf16, #blocked>
      %43 = ttg.convert_layout %19 : tensor<16xi32, #blocked1> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
      %45 = ttg.convert_layout %44 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked>
      %46 = arith.muli %45, %cst : tensor<16x1xi32, #blocked>
      %47 = tt.splat %15 : i32 -> tensor<16x1xi32, #blocked>
      %48 = arith.addi %46, %47 : tensor<16x1xi32, #blocked>
      %49 = tt.addptr %8, %48 : tensor<16x1x!tt.ptr<bf16>, #blocked>, tensor<16x1xi32, #blocked>
      tt.store %49, %42 : tensor<16x1x!tt.ptr<bf16>, #blocked>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=3 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:13:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 1, 32], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['last', '', ''], loop_orders=[[1, 0]], num_stages=3, num_warps=4, pid_type='persistent_blocked', range_flattens=[True, False], range_multi_buffers=[None, True], range_num_stages=[4, 3], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 35.9 configs/s
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:48:86: error: 'tt.load' op operation destroyed but still has uses
            a_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                     ^
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:48:86: note: - use: %64 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<4x16xbf16, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<4x16xf32, #ttg.blocked<{sizePerThread = [1, 2], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c256_i32 = arith.constant 256 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<4x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<4x8xf32, #blocked1>
    %cst_1 = arith.constant dense<0.707106769> : tensor<4x8xf32, #blocked1>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<4x8xf32, #blocked1>
    %cst_3 = arith.constant dense<64> : tensor<16x1xi32, #blocked>
    %cst_4 = arith.constant dense<256> : tensor<4x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<4x8xf32, #blocked1>
    %c8_i32 = arith.constant 8 : i32
    %c4_i32 = arith.constant 4 : i32
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c16_i32 : i32
    %2 = arith.addi %1, %c16_i32 : i32
    %3 = arith.minsi %2, %c256_i32 : i32
    %4 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #blocked2>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked2>
    %6 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<4x16x!tt.ptr<bf16>, #blocked3>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked1>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<4x8x!tt.ptr<bf16>, #blocked1>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c128_i32 : i32
      %12 = arith.muli %11, %c16_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c16_i32 : i32
      %15 = arith.remsi %arg4, %c128_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c4_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<4xi32, #blocked2>
      %21 = arith.addi %20, %4 : tensor<4xi32, #blocked2>
      %22 = arith.muli %18, %c8_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<8xi32, #blocked2>
      %24 = arith.addi %23, %5 : tensor<8xi32, #blocked2>
      %25 = ttg.convert_layout %21 : tensor<4xi32, #blocked2> -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %26 = tt.expand_dims %25 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<4x1xi32, #blocked4>
      %27 = ttg.convert_layout %26 : tensor<4x1xi32, #blocked4> -> tensor<4x1xi32, #blocked>
      %28 = arith.muli %27, %cst_4 : tensor<4x1xi32, #blocked>
      %29 = tt.broadcast %28 : tensor<4x1xi32, #blocked> -> tensor<4x16xi32, #blocked>
      %30 = ttg.convert_layout %29 : tensor<4x16xi32, #blocked> -> tensor<4x16xi32, #blocked3>
      %31 = ttg.convert_layout %24 : tensor<8xi32, #blocked2> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %32 = tt.expand_dims %31 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %33 = ttg.convert_layout %32 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked1>
      %34 = tt.broadcast %33 : tensor<1x8xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %c32_i32_6 = arith.constant 32 : i32
      %35 = scf.for %arg5 = %c0_i32 to %c256_i32 step %c32_i32_6 iter_args(%arg6 = %cst_5) -> (tensor<4x8xf32, #blocked1>)  : i32 {
        %62 = tt.splat %arg5 : i32 -> tensor<16xi32, #blocked2>
        %63 = arith.addi %62, %6 : tensor<16xi32, #blocked2>
        %64 = ttg.convert_layout %63 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %65 = tt.expand_dims %64 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %66 = ttg.convert_layout %65 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %67 = tt.broadcast %66 : tensor<1x16xi32, #blocked3> -> tensor<4x16xi32, #blocked3>
        %68 = arith.addi %30, %67 : tensor<4x16xi32, #blocked3>
        %69 = tt.addptr %7, %68 : tensor<4x16x!tt.ptr<bf16>, #blocked3>, tensor<4x16xi32, #blocked3>
        %70 = tt.load %69 evictionPolicy = evict_last : tensor<4x16x!tt.ptr<bf16>, #blocked3>
        %71 = ttg.convert_layout %63 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %72 = tt.expand_dims %71 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %73 = ttg.convert_layout %72 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked>
        %74 = arith.muli %73, %cst_3 : tensor<16x1xi32, #blocked>
        %75 = tt.broadcast %74 : tensor<16x1xi32, #blocked> -> tensor<16x8xi32, #blocked>
        %76 = ttg.convert_layout %75 : tensor<16x8xi32, #blocked> -> tensor<16x8xi32, #blocked1>
        %77 = arith.addi %76, %34 : tensor<16x8xi32, #blocked1>
        %78 = tt.addptr %8, %77 : tensor<16x8x!tt.ptr<bf16>, #blocked1>, tensor<16x8xi32, #blocked1>
        %79 = tt.load %78 : tensor<16x8x!tt.ptr<bf16>, #blocked1>
        %80 = ttg.convert_layout %70 : tensor<4x16xbf16, #blocked3> -> tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
        %81 = ttg.convert_layout %79 : tensor<16x8xbf16, #blocked1> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
        %82 = ttg.convert_layout %arg6 : tensor<4x8xf32, #blocked1> -> tensor<4x8xf32, #blocked1>
        %83 = tt.dot %80, %81, %82 : tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<4x8xf32, #blocked1>
        %c1_i32_7 = arith.constant 1 : i32
        %84 = arith.muli %c16_i32, %c1_i32_7 : i32
        %85 = arith.addi %arg5, %84 : i32
        %86 = tt.splat %85 : i32 -> tensor<16xi32, #blocked2>
        %87 = arith.addi %86, %6 : tensor<16xi32, #blocked2>
        %88 = ttg.convert_layout %87 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
        %89 = tt.expand_dims %88 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
        %90 = ttg.convert_layout %89 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked3>
        %91 = tt.broadcast %90 : tensor<1x16xi32, #blocked3> -> tensor<4x16xi32, #blocked3>
        %92 = arith.addi %30, %91 : tensor<4x16xi32, #blocked3>
        %93 = tt.addptr %7, %92 : tensor<4x16x!tt.ptr<bf16>, #blocked3>, tensor<4x16xi32, #blocked3>
        %94 = tt.load %93 evictionPolicy = evict_last : tensor<4x16x!tt.ptr<bf16>, #blocked3>
        %95 = ttg.convert_layout %87 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
        %96 = tt.expand_dims %95 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
        %97 = ttg.convert_layout %96 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked>
        %98 = arith.muli %97, %cst_3 : tensor<16x1xi32, #blocked>
        %99 = tt.broadcast %98 : tensor<16x1xi32, #blocked> -> tensor<16x8xi32, #blocked>
        %100 = ttg.convert_layout %99 : tensor<16x8xi32, #blocked> -> tensor<16x8xi32, #blocked1>
        %101 = arith.addi %100, %34 : tensor<16x8xi32, #blocked1>
        %102 = tt.addptr %8, %101 : tensor<16x8x!tt.ptr<bf16>, #blocked1>, tensor<16x8xi32, #blocked1>
        %103 = tt.load %102 : tensor<16x8x!tt.ptr<bf16>, #blocked1>
        %104 = ttg.convert_layout %94 : tensor<4x16xbf16, #blocked3> -> tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
        %105 = ttg.convert_layout %103 : tensor<16x8xbf16, #blocked1> -> tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
        %106 = ttg.convert_layout %83 : tensor<4x8xf32, #blocked1> -> tensor<4x8xf32, #blocked1>
        %107 = tt.dot %104, %105, %106 : tensor<4x16xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<16x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<4x8xf32, #blocked1>
        scf.yield %107 : tensor<4x8xf32, #blocked1>
      } {tt.disallow_acc_multi_buffer, tt.num_stages = 1 : i32}
      %36 = tt.addptr %9, %24 : tensor<8x!tt.ptr<bf16>, #blocked2>, tensor<8xi32, #blocked2>
      %37 = tt.load %36 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked2>
      %38 = ttg.convert_layout %37 : tensor<8xbf16, #blocked2> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %39 = tt.expand_dims %38 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %40 = ttg.convert_layout %39 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked1>
      %41 = arith.extf %40 : tensor<1x8xbf16, #blocked1> to tensor<1x8xf32, #blocked1>
      %42 = tt.broadcast %41 : tensor<1x8xf32, #blocked1> -> tensor<4x8xf32, #blocked1>
      %43 = arith.addf %35, %42 : tensor<4x8xf32, #blocked1>
      %44 = arith.mulf %43, %cst_2 : tensor<4x8xf32, #blocked1>
      %45 = arith.mulf %43, %cst_1 : tensor<4x8xf32, #blocked1>
      %46 = tt.extern_elementwise %45 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<4x8xf32, #blocked1>) -> tensor<4x8xf32, #blocked1>
      %47 = arith.addf %46, %cst_0 : tensor<4x8xf32, #blocked1>
      %48 = arith.mulf %44, %47 : tensor<4x8xf32, #blocked1>
      %49 = arith.truncf %48 : tensor<4x8xf32, #blocked1> to tensor<4x8xbf16, #blocked1>
      %50 = ttg.convert_layout %21 : tensor<4xi32, #blocked2> -> tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %51 = tt.expand_dims %50 {axis = 1 : i32} : tensor<4xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<4x1xi32, #blocked4>
      %52 = ttg.convert_layout %51 : tensor<4x1xi32, #blocked4> -> tensor<4x1xi32, #blocked>
      %53 = arith.muli %52, %cst : tensor<4x1xi32, #blocked>
      %54 = ttg.convert_layout %24 : tensor<8xi32, #blocked2> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %55 = tt.expand_dims %54 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %56 = ttg.convert_layout %55 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked1>
      %57 = tt.broadcast %53 : tensor<4x1xi32, #blocked> -> tensor<4x8xi32, #blocked>
      %58 = ttg.convert_layout %57 : tensor<4x8xi32, #blocked> -> tensor<4x8xi32, #blocked1>
      %59 = tt.broadcast %56 : tensor<1x8xi32, #blocked1> -> tensor<4x8xi32, #blocked1>
      %60 = arith.addi %58, %59 : tensor<4x8xi32, #blocked1>
      %61 = tt.addptr %10, %60 : tensor<4x8x!tt.ptr<bf16>, #blocked1>, tensor<4x8xi32, #blocked1>
      tt.store %61, %49 : tensor<4x8x!tt.ptr<bf16>, #blocked1>
    } {tt.disallow_acc_multi_buffer, tt.flatten, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=7 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/ap/cap3hfk3m4kiqoz6kmbro6dx2wyrvyagarambpl75b3oklsmqj2a.py:13:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[4s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[4, 8, 16], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], num_stages=7, num_warps=1, pid_type='persistent_blocked', range_flattens=[True, False], range_multi_buffers=[False, False], range_num_stages=[4, 1], range_unroll_factors=[0, 2], range_warp_specializes=[None, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:48:86: error: 'tt.load' op operation destroyed but still has uses
            a_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                     ^
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:48:86: note: - use: %61 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x32xbf16, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x32xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c128_i32 = arith.constant 128 : i32
    %c32_i32 = arith.constant 32 : i32
    %c256_i32 = arith.constant 256 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst = arith.constant dense<64> : tensor<16x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<16x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<16x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<16x1xf32, #blocked>
    %cst_3 = arith.constant dense<64> : tensor<32x1xi32, #blocked>
    %cst_4 = arith.constant dense<256> : tensor<16x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<16x1xf32, #blocked>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c32_i32 : i32
    %2 = arith.addi %1, %c32_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked1>
    %5 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked1>
    %6 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    %7 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<32x1x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x1x!tt.ptr<bf16>, #blocked>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %9 = arith.divsi %arg4, %c128_i32 : i32
      %10 = arith.muli %9, %c16_i32 : i32
      %11 = arith.subi %c64_i32, %10 : i32
      %12 = arith.minsi %11, %c16_i32 : i32
      %13 = arith.remsi %arg4, %c128_i32 : i32
      %14 = arith.remsi %13, %12 : i32
      %15 = arith.addi %10, %14 : i32
      %16 = arith.divsi %13, %12 : i32
      %17 = arith.muli %16, %c16_i32 : i32
      %18 = tt.splat %17 : i32 -> tensor<16xi32, #blocked1>
      %19 = arith.addi %18, %4 : tensor<16xi32, #blocked1>
      %20 = ttg.convert_layout %19 : tensor<16xi32, #blocked1> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %21 = tt.expand_dims %20 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
      %22 = ttg.convert_layout %21 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked>
      %23 = arith.muli %22, %cst_4 : tensor<16x1xi32, #blocked>
      %24 = tt.broadcast %23 : tensor<16x1xi32, #blocked> -> tensor<16x32xi32, #blocked>
      %25 = ttg.convert_layout %24 : tensor<16x32xi32, #blocked> -> tensor<16x32xi32, #blocked2>
      %26 = tt.splat %15 : i32 -> tensor<32x1xi32, #blocked>
      %c64_i32_6 = arith.constant 64 : i32
      %27 = scf.for %arg5 = %c0_i32 to %c256_i32 step %c64_i32_6 iter_args(%arg6 = %cst_5) -> (tensor<16x1xf32, #blocked>)  : i32 {
        %50 = tt.splat %arg5 : i32 -> tensor<32xi32, #blocked1>
        %51 = arith.addi %50, %5 : tensor<32xi32, #blocked1>
        %52 = ttg.convert_layout %51 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
        %53 = tt.expand_dims %52 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x32xi32, #blocked4>
        %54 = ttg.convert_layout %53 : tensor<1x32xi32, #blocked4> -> tensor<1x32xi32, #blocked2>
        %55 = tt.broadcast %54 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
        %56 = arith.addi %25, %55 : tensor<16x32xi32, #blocked2>
        %57 = tt.addptr %6, %56 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
        %58 = tt.load %57 evictionPolicy = evict_last : tensor<16x32x!tt.ptr<bf16>, #blocked2>
        %59 = ttg.convert_layout %51 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
        %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<32x1xi32, #blocked3>
        %61 = ttg.convert_layout %60 : tensor<32x1xi32, #blocked3> -> tensor<32x1xi32, #blocked>
        %62 = arith.muli %61, %cst_3 : tensor<32x1xi32, #blocked>
        %63 = arith.addi %62, %26 : tensor<32x1xi32, #blocked>
        %64 = tt.addptr %7, %63 : tensor<32x1x!tt.ptr<bf16>, #blocked>, tensor<32x1xi32, #blocked>
        %65 = tt.load %64 : tensor<32x1x!tt.ptr<bf16>, #blocked>
        %66 = ttg.convert_layout %58 : tensor<16x32xbf16, #blocked2> -> tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %67 = ttg.convert_layout %65 : tensor<32x1xbf16, #blocked> -> tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %68 = ttg.convert_layout %arg6 : tensor<16x1xf32, #blocked> -> tensor<16x1xf32, #blocked>
        %69 = tt.dot %66, %67, %68 : tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<16x1xf32, #blocked>
        %c1_i32_7 = arith.constant 1 : i32
        %70 = arith.muli %c32_i32, %c1_i32_7 : i32
        %71 = arith.addi %arg5, %70 : i32
        %72 = tt.splat %71 : i32 -> tensor<32xi32, #blocked1>
        %73 = arith.addi %72, %5 : tensor<32xi32, #blocked1>
        %74 = ttg.convert_layout %73 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
        %75 = tt.expand_dims %74 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x32xi32, #blocked4>
        %76 = ttg.convert_layout %75 : tensor<1x32xi32, #blocked4> -> tensor<1x32xi32, #blocked2>
        %77 = tt.broadcast %76 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
        %78 = arith.addi %25, %77 : tensor<16x32xi32, #blocked2>
        %79 = tt.addptr %6, %78 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
        %80 = tt.load %79 evictionPolicy = evict_last : tensor<16x32x!tt.ptr<bf16>, #blocked2>
        %81 = ttg.convert_layout %73 : tensor<32xi32, #blocked1> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
        %82 = tt.expand_dims %81 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<32x1xi32, #blocked3>
        %83 = ttg.convert_layout %82 : tensor<32x1xi32, #blocked3> -> tensor<32x1xi32, #blocked>
        %84 = arith.muli %83, %cst_3 : tensor<32x1xi32, #blocked>
        %85 = arith.addi %84, %26 : tensor<32x1xi32, #blocked>
        %86 = tt.addptr %7, %85 : tensor<32x1x!tt.ptr<bf16>, #blocked>, tensor<32x1xi32, #blocked>
        %87 = tt.load %86 : tensor<32x1x!tt.ptr<bf16>, #blocked>
        %88 = ttg.convert_layout %80 : tensor<16x32xbf16, #blocked2> -> tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
        %89 = ttg.convert_layout %87 : tensor<32x1xbf16, #blocked> -> tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
        %90 = ttg.convert_layout %69 : tensor<16x1xf32, #blocked> -> tensor<16x1xf32, #blocked>
        %91 = tt.dot %88, %89, %90 : tensor<16x32xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<32x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<16x1xf32, #blocked>
        scf.yield %91 : tensor<16x1xf32, #blocked>
      } {tt.num_stages = 1 : i32}
      %28 = tt.addptr %arg2, %15 : !tt.ptr<bf16>, i32
      %29 = tt.splat %28 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
      %30 = tt.load %29 : tensor<1x!tt.ptr<bf16>, #blocked1>
      %31 = ttg.convert_layout %30 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %32 = tt.expand_dims %31 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
      %33 = ttg.convert_layout %32 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
      %34 = arith.extf %33 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
      %35 = tt.broadcast %34 : tensor<1x1xf32, #blocked> -> tensor<16x1xf32, #blocked>
      %36 = arith.addf %27, %35 : tensor<16x1xf32, #blocked>
      %37 = arith.mulf %36, %cst_2 : tensor<16x1xf32, #blocked>
      %38 = arith.mulf %36, %cst_1 : tensor<16x1xf32, #blocked>
      %39 = tt.extern_elementwise %38 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x1xf32, #blocked>) -> tensor<16x1xf32, #blocked>
      %40 = arith.addf %39, %cst_0 : tensor<16x1xf32, #blocked>
      %41 = arith.mulf %37, %40 : tensor<16x1xf32, #blocked>
      %42 = arith.truncf %41 : tensor<16x1xf32, #blocked> to tensor<16x1xbf16, #blocked>
      %43 = ttg.convert_layout %19 : tensor<16xi32, #blocked1> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %44 = tt.expand_dims %43 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
      %45 = ttg.convert_layout %44 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked>
      %46 = arith.muli %45, %cst : tensor<16x1xi32, #blocked>
      %47 = tt.splat %15 : i32 -> tensor<16x1xi32, #blocked>
      %48 = arith.addi %46, %47 : tensor<16x1xi32, #blocked>
      %49 = tt.addptr %8, %48 : tensor<16x1x!tt.ptr<bf16>, #blocked>, tensor<16x1xi32, #blocked>
      tt.store %49, %42 : tensor<16x1x!tt.ptr<bf16>, #blocked>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=3 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:13:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/5x/c5xapocmoqu7kihvtrnuclhnbtjbjrje3oe4qbu7mumbzoac74pj.py:13:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[8s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 1, 32], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['last', '', ''], loop_orders=[[1, 0]], num_stages=3, num_warps=4, pid_type='persistent_blocked', range_flattens=[True, False], range_multi_buffers=[None, True], range_num_stages=[4, 3], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 18.0 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 59.1 configs/s
[26s] Initial random population of 100, 5 starting points: error=4 ok=96 min=0.0098 mid=0.0411 max=1.7215 best=Config(block_sizes=[16, 16, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], num_stages=3, num_warps=1, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[False, False], range_num_stages=[1, 3], range_unroll_factors=[1, 1], range_warp_specializes=[True, None])
[26s] Generation 1 starting: 246 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 246/246 33.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 246/246 17.2 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 5.5 configs/s
[235s] Generation 1 complete: ok=251 min=0.0083 mid=0.0106 max=0.0405 best=Config(block_sizes=[16, 16, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], num_stages=3, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, False])
[235s] Generation 2 starting: 211 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211/211 31.0 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211/211 17.6 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 6.0 configs/s
[423s] Generation 2 complete: error=1 ok=215 min=0.0080 mid=0.0096 max=0.1697 best=Config(block_sizes=[16, 16, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'first'], loop_orders=[[0, 1]], num_stages=3, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, False])
[423s] Generation 3 starting: 212 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212/212 30.8 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212/212 17.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 6.0 configs/s
[613s] Generation 3 complete: error=1 ok=216 min=0.0082 mid=0.0092 max=0.1705 best=Config(block_sizes=[32, 16, 128], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, False])
[613s] Generation 4 starting: 175 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 34.0 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 17.5 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 6.9 configs/s
[774s] Generation 4 complete: error=1 ok=178 min=0.0076 mid=0.0088 max=0.0386 best=Config(block_sizes=[32, 16, 128], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, False])
[774s] Generation 5 starting: 169 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 32.0 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 18.0 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.3 configs/s
[929s] Generation 5 complete: error=1 ok=172 min=0.0081 mid=0.0089 max=0.0385 best=Config(block_sizes=[32, 16, 128], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[929s] Generation 6 starting: 176 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 32.4 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 17.7 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.0 configs/s
[1091s] Generation 6 complete: error=1 ok=179 min=0.0079 mid=0.0084 max=0.0384 best=Config(block_sizes=[32, 16, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1091s] Generation 7 starting: 164 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 31.1 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 17.9 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.3 configs/s
[1246s] Generation 7 complete: error=1 ok=167 min=0.0076 mid=0.0082 max=0.0531 best=Config(block_sizes=[32, 16, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1246s] Generation 8 starting: 170 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 28.9 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 18.2 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.1 configs/s
[1404s] Generation 8 complete: error=1 ok=173 min=0.0080 mid=0.0084 max=0.0530 best=Config(block_sizes=[32, 16, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1404s] Generation 9 starting: 160 neighbors, 4 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160/160 31.5 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160/160 18.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.4 configs/s
[1557s] Generation 9 complete: ok=164 min=0.0078 mid=0.0081 max=0.0529 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1557s] Generation 10 starting: 145 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145/145 27.6 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 145/145 17.9 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 8.7 configs/s
[1688s] Generation 10 complete: ok=149 min=0.0075 mid=0.0080 max=0.1698 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1688s] Generation 11 starting: 120 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 30.4 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.2 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.4 configs/s
[1797s] Generation 11 complete: ok=123 min=0.0078 mid=0.0084 max=0.1704 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1797s] Generation 12 starting: 125 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 28.1 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 17.7 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.8 configs/s
[1912s] Generation 12 complete: ok=128 min=0.0076 mid=0.0081 max=0.1703 best=Config(block_sizes=[32, 16, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[16], load_eviction_policies=['last', 'first', ''], loop_orders=[[1, 0]], num_stages=6, num_warps=4, pid_type='xyz', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1912s] Generation 13 starting: 121 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 31.7 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 18.2 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.2 configs/s
[2023s] Generation 13 complete: ok=124 min=0.0078 mid=0.0082 max=0.1697 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', '', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2023s] Generation 14 starting: 123 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 25.7 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 17.6 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.0 configs/s
[2137s] Generation 14 complete: ok=126 min=0.0079 mid=0.0084 max=0.1705 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2137s] Generation 15 starting: 117 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 24.2 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 17.8 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.6 configs/s
[2246s] Generation 15 complete: ok=120 min=0.0080 mid=0.0084 max=0.1704 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[2246s] Generation 16 starting: 120 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 28.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 18.1 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.5 configs/s
[2355s] Generation 16 complete: ok=123 min=0.0078 mid=0.0083 max=0.1708 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[2355s] Generation 17 starting: 82 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82/82 19.3 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82/82 17.9 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.6 configs/s
[2430s] Generation 17 complete: ok=84 min=0.0080 mid=0.0082 max=0.1704 best=Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False])
[2430s] Autotuning complete in 2430.3s after searching 2736 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False]), static_shapes=True)

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
TritonBench accuracy check failed with Helion kernel config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, False]), static_shapes=True)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
WARNING:tritonbench.utils.triton_op:Completed input ID 1:
(M, K, N)
--------------
(128, 256, 64)
 40%|████      | 2/5 [1:22:57<2:03:57, 2479.01s/it]WARNING:tritonbench.utils.triton_op:Running input ID 2:
(M, K, N)
----------------
(512, 1024, 128)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 34, "num_triton_choices": 34, "best_kernel": "triton_mm_55", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.015281000174582005, "best_triton_pos": 0}
AUTOTUNE mm(512x1024, 1024x128)
strides: [1024, 1], [128, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_55 0.0153 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_46 0.0159 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_51 0.0163 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_49 0.0164 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_45 0.0164 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_54 0.0166 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_50 0.0176 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_47 0.0180 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_48 0.0192 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_59 0.0194 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.2677 seconds and 0.7629 seconds precompiling for 34 choices
INFO:tritonbench.utils.triton_op:Took 1917.46ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 133.11ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512, 1024),
              'stride': (1024, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024, 128),
              'stride': (128, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (128,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 4.01ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 704392404
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Accuracy check failed!
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:55:33: note: - use: %98 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<32x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<32x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x4xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x4xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x4xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x4xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c32_i32 : i32
    %2 = arith.addi %1, %c32_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #blocked3>
    %5 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %5 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x4x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<4x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x4x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c32_i32 : i32
      %14 = arith.muli %13, %c2_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c2_i32 : i32
      %17 = arith.remsi %arg4, %c32_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c4_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<4xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<4xi32, #blocked3>
      %24 = arith.muli %20, %c32_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<32xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<32xi32, #blocked3>
      %27 = arith.extsi %24 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<32xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<32xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<32x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<32x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<32x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<32x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %41 = ttg.convert_layout %23 : tensor<4xi32, #blocked3> -> tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x4xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x4xi32, #blocked5> -> tensor<1x4xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x4xi32, #blocked2> -> tensor<512x4xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<32x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<32x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x4xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x4xi32, #blocked1> -> tensor<512x4xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x4xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x4x!tt.ptr<bf16>, #blocked2>, tensor<512x4xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x4x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x4xbf16, #blocked2> -> tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<32x4xf32, #blocked2> -> tensor<32x4xf32, #blocked2>
      %73 = tt.dot %70, %71, %72 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x4xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<32x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<32x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x4xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x4xi32, #blocked1> -> tensor<512x4xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x4xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x4x!tt.ptr<bf16>, #blocked2>, tensor<512x4xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x4x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x4xbf16, #blocked2> -> tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<32x4xf32, #blocked2> -> tensor<32x4xf32, #blocked2>
      %105 = tt.dot %102, %103, %104 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x4xf32, #blocked2>
      %106 = tt.addptr %11, %23 : tensor<4x!tt.ptr<bf16>, #blocked3>, tensor<4xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_last : tensor<4x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<4xbf16, #blocked3> -> tensor<4xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<4xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x4xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x4xbf16, #blocked5> -> tensor<1x4xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x4xbf16, #blocked2> to tensor<1x4xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x4xf32, #blocked2> -> tensor<32x4xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<32x4xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<32x4xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<32x4xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x4xf32, #blocked2>) -> tensor<32x4xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<32x4xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<32x4xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<32x4xf32, #blocked2> to tensor<32x4xbf16, #blocked2>
      %120 = ttg.convert_layout %26 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<32x1xi32, #blocked1>
      %124 = ttg.convert_layout %23 : tensor<4xi32, #blocked3> -> tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x4xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x4xi32, #blocked5> -> tensor<1x4xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<32x1xi32, #blocked1> -> tensor<32x4xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<32x4xi32, #blocked1> -> tensor<32x4xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x4xi32, #blocked2> -> tensor<32x4xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<32x4xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<32x4x!tt.ptr<bf16>, #blocked2>, tensor<32x4xi32, #blocked2>
      tt.store %131, %119 : tensor<32x4x!tt.ptr<bf16>, #blocked2>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=7 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 4, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='persistent_blocked', range_flattens=[True, True], range_multi_buffers=[None, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:38:83: error: 'tt.load' op operation destroyed but still has uses
        a_tile = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                  ^
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:38:83: note: - use: %78 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x256xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<8x256xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c512_i32 = arith.constant 512 : i32
    %c256_i32 = arith.constant 256 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<128> : tensor<8x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<8x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<8x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<8x1xf32, #blocked>
    %cst_3 = arith.constant dense<128> : tensor<256x1xi32, #blocked>
    %cst_4 = arith.constant dense<1024> : tensor<8x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x1xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c128_i32 = arith.constant 128 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c512_i32 : i32
    %2 = arith.muli %1, %c8_i32 : i32
    %3 = arith.subi %c128_i32, %2 : i32
    %4 = arith.minsi %3, %c8_i32 : i32
    %5 = arith.remsi %0, %c512_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %8, %c8_i32 : i32
    %10 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked1>
    %11 = tt.splat %9 : i32 -> tensor<8xi32, #blocked1>
    %12 = arith.addi %11, %10 : tensor<8xi32, #blocked1>
    %13 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>
    %14 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %15 = tt.expand_dims %14 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %16 = ttg.convert_layout %15 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %17 = arith.muli %16, %cst_4 : tensor<8x1xi32, #blocked>
    %18 = tt.broadcast %17 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked>
    %19 = ttg.convert_layout %18 : tensor<8x256xi32, #blocked> -> tensor<8x256xi32, #blocked3>
    %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x256x!tt.ptr<bf16>, #blocked3>
    %21 = tt.splat %7 : i32 -> tensor<256x1xi32, #blocked>
    %22 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<256x1x!tt.ptr<bf16>, #blocked>
    %c512_i32_6 = arith.constant 512 : i32
    %23 = scf.for %arg4 = %c0_i32 to %c1024_i32 step %c512_i32_6 iter_args(%arg5 = %cst_5) -> (tensor<8x1xf32, #blocked>)  : i32 {
      %47 = tt.splat %arg4 : i32 -> tensor<256xi32, #blocked1>
      %48 = arith.addi %47, %13 : tensor<256xi32, #blocked1>
      %49 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %51 = ttg.convert_layout %50 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %52 = tt.broadcast %51 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %53 = arith.addi %19, %52 : tensor<8x256xi32, #blocked3>
      %54 = tt.addptr %20, %53 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %55 = tt.load %54 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %56 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %57 = tt.expand_dims %56 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %59 = arith.muli %58, %cst_3 : tensor<256x1xi32, #blocked>
      %60 = arith.addi %59, %21 : tensor<256x1xi32, #blocked>
      %61 = tt.addptr %22, %60 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %62 = tt.load %61 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %63 = ttg.convert_layout %55 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %64 = ttg.convert_layout %62 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %65 = ttg.convert_layout %arg5 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %66 = tt.dot %63, %64, %65 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      %c1_i32 = arith.constant 1 : i32
      %67 = arith.muli %c256_i32, %c1_i32 : i32
      %68 = arith.addi %arg4, %67 : i32
      %69 = tt.splat %68 : i32 -> tensor<256xi32, #blocked1>
      %70 = arith.addi %69, %13 : tensor<256xi32, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %72 = tt.expand_dims %71 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %73 = ttg.convert_layout %72 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %74 = tt.broadcast %73 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %75 = arith.addi %19, %74 : tensor<8x256xi32, #blocked3>
      %76 = tt.addptr %20, %75 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %77 = tt.load %76 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %78 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %79 = tt.expand_dims %78 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %80 = ttg.convert_layout %79 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %81 = arith.muli %80, %cst_3 : tensor<256x1xi32, #blocked>
      %82 = arith.addi %81, %21 : tensor<256x1xi32, #blocked>
      %83 = tt.addptr %22, %82 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %84 = tt.load %83 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %85 = ttg.convert_layout %77 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %86 = ttg.convert_layout %84 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %87 = ttg.convert_layout %66 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %88 = tt.dot %85, %86, %87 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      scf.yield %88 : tensor<8x1xf32, #blocked>
    } {tt.flatten}
    %24 = tt.addptr %arg2, %7 : !tt.ptr<bf16>, i32
    %25 = tt.splat %24 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
    %26 = tt.load %25 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked1>
    %27 = ttg.convert_layout %26 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %28 = tt.expand_dims %27 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %29 = ttg.convert_layout %28 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
    %30 = arith.extf %29 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
    %31 = tt.broadcast %30 : tensor<1x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
    %32 = arith.addf %23, %31 : tensor<8x1xf32, #blocked>
    %33 = arith.mulf %32, %cst_2 : tensor<8x1xf32, #blocked>
    %34 = arith.mulf %32, %cst_1 : tensor<8x1xf32, #blocked>
    %35 = tt.extern_elementwise %34 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x1xf32, #blocked>) -> tensor<8x1xf32, #blocked>
    %36 = arith.addf %35, %cst_0 : tensor<8x1xf32, #blocked>
    %37 = arith.mulf %33, %36 : tensor<8x1xf32, #blocked>
    %38 = arith.truncf %37 : tensor<8x1xf32, #blocked> to tensor<8x1xbf16, #blocked>
    %39 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %40 = tt.expand_dims %39 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %41 = ttg.convert_layout %40 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %42 = arith.muli %41, %cst : tensor<8x1xi32, #blocked>
    %43 = tt.splat %7 : i32 -> tensor<8x1xi32, #blocked>
    %44 = arith.addi %42, %43 : tensor<8x1xi32, #blocked>
    %45 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x1x!tt.ptr<bf16>, #blocked>
    %46 = tt.addptr %45, %44 : tensor<8x1x!tt.ptr<bf16>, #blocked>, tensor<8x1xi32, #blocked>
    tt.store %46, %38 : tensor<8x1x!tt.ptr<bf16>, #blocked>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:12:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 1, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['last', '', 'first'], loop_orders=[[1, 0]], num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 10.6 configs/s
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:55:33: note: - use: %98 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<32x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>) -> tensor<32x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [2, 2], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 4], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x4xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x4xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x4xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x4xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c4_i32 = arith.constant 4 : i32
    %c2_i32 = arith.constant 2 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c32_i32 : i32
    %2 = arith.addi %1, %c32_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 4 : i32, start = 0 : i32} : tensor<4xi32, #blocked3>
    %5 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %5 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x4x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<4x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x4x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c32_i32 : i32
      %14 = arith.muli %13, %c2_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c2_i32 : i32
      %17 = arith.remsi %arg4, %c32_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c4_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<4xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<4xi32, #blocked3>
      %24 = arith.muli %20, %c32_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<32xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<32xi32, #blocked3>
      %27 = arith.extsi %24 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<32xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<32xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<32x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<32x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<32x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<32x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %41 = ttg.convert_layout %23 : tensor<4xi32, #blocked3> -> tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x4xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x4xi32, #blocked5> -> tensor<1x4xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x4xi32, #blocked2> -> tensor<512x4xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<32x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<32x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x4xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x4xi32, #blocked1> -> tensor<512x4xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x4xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x4x!tt.ptr<bf16>, #blocked2>, tensor<512x4xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x4x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x4xbf16, #blocked2> -> tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<32x4xf32, #blocked2> -> tensor<32x4xf32, #blocked2>
      %73 = tt.dot %70, %71, %72 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x4xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<32x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<32x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x4xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x4xi32, #blocked1> -> tensor<512x4xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x4xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x4x!tt.ptr<bf16>, #blocked2>, tensor<512x4xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x4x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x4xbf16, #blocked2> -> tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<32x4xf32, #blocked2> -> tensor<32x4xf32, #blocked2>
      %105 = tt.dot %102, %103, %104 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x4xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x4xf32, #blocked2>
      %106 = tt.addptr %11, %23 : tensor<4x!tt.ptr<bf16>, #blocked3>, tensor<4xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_last : tensor<4x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<4xbf16, #blocked3> -> tensor<4xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<4xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x4xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x4xbf16, #blocked5> -> tensor<1x4xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x4xbf16, #blocked2> to tensor<1x4xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x4xf32, #blocked2> -> tensor<32x4xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<32x4xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<32x4xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<32x4xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x4xf32, #blocked2>) -> tensor<32x4xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<32x4xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<32x4xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<32x4xf32, #blocked2> to tensor<32x4xbf16, #blocked2>
      %120 = ttg.convert_layout %26 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<32x1xi32, #blocked1>
      %124 = ttg.convert_layout %23 : tensor<4xi32, #blocked3> -> tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<4xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x4xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x4xi32, #blocked5> -> tensor<1x4xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<32x1xi32, #blocked1> -> tensor<32x4xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<32x4xi32, #blocked1> -> tensor<32x4xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x4xi32, #blocked2> -> tensor<32x4xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<32x4xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<32x4x!tt.ptr<bf16>, #blocked2>, tensor<32x4xi32, #blocked2>
      tt.store %131, %119 : tensor<32x4x!tt.ptr<bf16>, #blocked2>
    } {tt.flatten, tt.num_stages = 4 : i32, tt.warp_specialize}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=7 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/yp/cypy6z4soqvx44g6mjpvk7mxar5sg2beissx4mrc2wz66wluljyq.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[12s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 4, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='persistent_blocked', range_flattens=[True, True], range_multi_buffers=[None, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[True, None]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:38:83: error: 'tt.load' op operation destroyed but still has uses
        a_tile = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                  ^
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:38:83: note: - use: %78 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x256xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<8x256xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c512_i32 = arith.constant 512 : i32
    %c256_i32 = arith.constant 256 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant dense<128> : tensor<8x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<8x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<8x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<8x1xf32, #blocked>
    %cst_3 = arith.constant dense<128> : tensor<256x1xi32, #blocked>
    %cst_4 = arith.constant dense<1024> : tensor<8x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x1xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c128_i32 = arith.constant 128 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c512_i32 : i32
    %2 = arith.muli %1, %c8_i32 : i32
    %3 = arith.subi %c128_i32, %2 : i32
    %4 = arith.minsi %3, %c8_i32 : i32
    %5 = arith.remsi %0, %c512_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %8, %c8_i32 : i32
    %10 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked1>
    %11 = tt.splat %9 : i32 -> tensor<8xi32, #blocked1>
    %12 = arith.addi %11, %10 : tensor<8xi32, #blocked1>
    %13 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>
    %14 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %15 = tt.expand_dims %14 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %16 = ttg.convert_layout %15 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %17 = arith.muli %16, %cst_4 : tensor<8x1xi32, #blocked>
    %18 = tt.broadcast %17 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked>
    %19 = ttg.convert_layout %18 : tensor<8x256xi32, #blocked> -> tensor<8x256xi32, #blocked3>
    %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x256x!tt.ptr<bf16>, #blocked3>
    %21 = tt.splat %7 : i32 -> tensor<256x1xi32, #blocked>
    %22 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<256x1x!tt.ptr<bf16>, #blocked>
    %c512_i32_6 = arith.constant 512 : i32
    %23 = scf.for %arg4 = %c0_i32 to %c1024_i32 step %c512_i32_6 iter_args(%arg5 = %cst_5) -> (tensor<8x1xf32, #blocked>)  : i32 {
      %47 = tt.splat %arg4 : i32 -> tensor<256xi32, #blocked1>
      %48 = arith.addi %47, %13 : tensor<256xi32, #blocked1>
      %49 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %51 = ttg.convert_layout %50 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %52 = tt.broadcast %51 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %53 = arith.addi %19, %52 : tensor<8x256xi32, #blocked3>
      %54 = tt.addptr %20, %53 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %55 = tt.load %54 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %56 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %57 = tt.expand_dims %56 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %59 = arith.muli %58, %cst_3 : tensor<256x1xi32, #blocked>
      %60 = arith.addi %59, %21 : tensor<256x1xi32, #blocked>
      %61 = tt.addptr %22, %60 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %62 = tt.load %61 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %63 = ttg.convert_layout %55 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %64 = ttg.convert_layout %62 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %65 = ttg.convert_layout %arg5 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %66 = tt.dot %63, %64, %65 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      %c1_i32 = arith.constant 1 : i32
      %67 = arith.muli %c256_i32, %c1_i32 : i32
      %68 = arith.addi %arg4, %67 : i32
      %69 = tt.splat %68 : i32 -> tensor<256xi32, #blocked1>
      %70 = arith.addi %69, %13 : tensor<256xi32, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %72 = tt.expand_dims %71 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %73 = ttg.convert_layout %72 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %74 = tt.broadcast %73 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %75 = arith.addi %19, %74 : tensor<8x256xi32, #blocked3>
      %76 = tt.addptr %20, %75 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %77 = tt.load %76 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %78 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %79 = tt.expand_dims %78 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %80 = ttg.convert_layout %79 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %81 = arith.muli %80, %cst_3 : tensor<256x1xi32, #blocked>
      %82 = arith.addi %81, %21 : tensor<256x1xi32, #blocked>
      %83 = tt.addptr %22, %82 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %84 = tt.load %83 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %85 = ttg.convert_layout %77 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %86 = ttg.convert_layout %84 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %87 = ttg.convert_layout %66 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %88 = tt.dot %85, %86, %87 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      scf.yield %88 : tensor<8x1xf32, #blocked>
    } {tt.flatten}
    %24 = tt.addptr %arg2, %7 : !tt.ptr<bf16>, i32
    %25 = tt.splat %24 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
    %26 = tt.load %25 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked1>
    %27 = ttg.convert_layout %26 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %28 = tt.expand_dims %27 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %29 = ttg.convert_layout %28 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
    %30 = arith.extf %29 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
    %31 = tt.broadcast %30 : tensor<1x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
    %32 = arith.addf %23, %31 : tensor<8x1xf32, #blocked>
    %33 = arith.mulf %32, %cst_2 : tensor<8x1xf32, #blocked>
    %34 = arith.mulf %32, %cst_1 : tensor<8x1xf32, #blocked>
    %35 = tt.extern_elementwise %34 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x1xf32, #blocked>) -> tensor<8x1xf32, #blocked>
    %36 = arith.addf %35, %cst_0 : tensor<8x1xf32, #blocked>
    %37 = arith.mulf %33, %36 : tensor<8x1xf32, #blocked>
    %38 = arith.truncf %37 : tensor<8x1xf32, #blocked> to tensor<8x1xbf16, #blocked>
    %39 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %40 = tt.expand_dims %39 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %41 = ttg.convert_layout %40 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %42 = arith.muli %41, %cst : tensor<8x1xi32, #blocked>
    %43 = tt.splat %7 : i32 -> tensor<8x1xi32, #blocked>
    %44 = arith.addi %42, %43 : tensor<8x1xi32, #blocked>
    %45 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x1x!tt.ptr<bf16>, #blocked>
    %46 = tt.addptr %45, %44 : tensor<8x1x!tt.ptr<bf16>, #blocked>, tensor<8x1xi32, #blocked>
    tt.store %46, %38 : tensor<8x1x!tt.ptr<bf16>, #blocked>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/tu/ctu3b3get2sn3wkvtgevcbb5jfjbrywfsxxckonk6vkiux2akhe5.py:12:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[18s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 1, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['last', '', 'first'], loop_orders=[[1, 0]], num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 9.2 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 197.5 configs/s
[26s] Initial random population of 100, 5 starting points: error=6 ok=94 min=0.0337 mid=0.7247 max=119.6035 best=Config(block_sizes=[32, 32, 128], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, True], range_multi_buffers=[False, None], range_num_stages=[4, 4], range_unroll_factors=[0, 1], range_warp_specializes=[False, True])
[26s] Generation 1 starting: 228 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228/228 22.0 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228/228 18.8 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 14.8 configs/s
[119s] Generation 1 complete: error=16 ok=217 min=0.0284 mid=0.0459 max=2.6227 best=Config(block_sizes=[32, 32, 128], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 4], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[119s] Generation 2 starting: 200 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 10.1 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 19.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.0 configs/s
[296s] Generation 2 complete: error=16 ok=189 min=0.0271 mid=0.0344 max=0.8476 best=Config(block_sizes=[32, 32, 128], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[296s] Generation 3 starting: 185 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 23.9 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 17.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.0 configs/s
[463s] Generation 3 complete: error=2 ok=188 min=0.0264 mid=0.0298 max=0.5090 best=Config(block_sizes=[32, 32, 128], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[463s] Generation 4 starting: 201 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201/201 18.3 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 201/201 17.2 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 6.9 configs/s
[632s] Generation 4 complete: error=1 ok=205 min=0.0226 mid=0.0276 max=0.2112 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[632s] Generation 5 starting: 197 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 22.6 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 17.9 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.3 configs/s
[792s] Generation 5 complete: error=2 ok=200 min=0.0218 mid=0.0250 max=0.5594 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[792s] Generation 6 starting: 210 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 23.3 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 18.1 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 6.9 configs/s
[961s] Generation 6 complete: error=2 ok=213 min=0.0215 mid=0.0256 max=0.5575 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[961s] Generation 7 starting: 207 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207/207 23.3 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207/207 18.3 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.2 configs/s
[1126s] Generation 7 complete: error=2 ok=210 min=0.0215 mid=0.0262 max=0.5592 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1126s] Generation 8 starting: 206 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 22.9 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 18.1 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7.1 configs/s
[1291s] Generation 8 complete: error=2 ok=209 min=0.0214 mid=0.0257 max=0.5570 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1291s] Generation 9 starting: 208 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208/208 23.5 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 208/208 18.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 6.8 configs/s
[1464s] Generation 9 complete: ok=213 min=0.0214 mid=0.0261 max=0.5573 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1464s] Generation 10 starting: 162 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 19.0 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 18.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.3 configs/s
[1591s] Generation 10 complete: error=2 ok=165 min=0.0214 mid=0.0276 max=0.7193 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1591s] Generation 11 starting: 172 neighbors, 4 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172/172 22.7 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172/172 18.0 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 8.4 configs/s
[1730s] Generation 11 complete: error=2 ok=175 min=0.0213 mid=0.0272 max=0.7161 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1730s] Generation 12 starting: 170 neighbors, 4 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 22.7 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 17.9 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 8.5 configs/s
[1867s] Generation 12 complete: ok=175 min=0.0214 mid=0.0268 max=0.7181 best=Config(block_sizes=[16, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[1, 0]], num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[1867s] Generation 13 starting: 164 neighbors, 4 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 22.8 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 18.3 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 8.8 configs/s
[2000s] Generation 13 complete: error=2 ok=166 min=0.0214 mid=0.0245 max=0.7181 best=Config(block_sizes=[16, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[1, 0]], num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2000s] Generation 14 starting: 126 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126/126 22.4 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126/126 18.3 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.5 configs/s
[2102s] Generation 14 complete: error=2 ok=128 min=0.0214 mid=0.0247 max=0.7179 best=Config(block_sizes=[16, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[1, 0]], num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, None])
[2102s] Generation 15 starting: 129 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 18.5 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 18.1 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.1 configs/s
[2209s] Generation 15 complete: error=2 ok=131 min=0.0215 mid=0.0246 max=0.7191 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2209s] Generation 16 starting: 127 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127/127 23.3 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127/127 18.3 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.4 configs/s
[2312s] Generation 16 complete: error=2 ok=128 min=0.0212 mid=0.0242 max=0.7196 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2312s] Generation 17 starting: 87 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/87 19.1 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/87 18.4 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 15.8 configs/s
[2386s] Generation 17 complete: error=2 ok=87 min=0.0214 mid=0.0229 max=0.0445 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2387s] Generation 18 starting: 85 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85/85 23.9 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85/85 18.4 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.1 configs/s
[2459s] Generation 18 complete: error=2 ok=85 min=0.0214 mid=0.0231 max=0.0445 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2459s] Generation 19 starting: 41 neighbors, 1 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41/41 24.2 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41/41 19.2 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 34.5 configs/s
[2492s] Generation 19 complete: error=2 ok=41 min=0.0214 mid=0.0257 max=0.0452 best=Config(block_sizes=[32, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2492s] Generation 20 starting: 43 neighbors, 1 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43/43 18.0 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43/43 18.5 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 30.5 configs/s
[2530s] Generation 20 complete: error=2 ok=43 min=0.0208 mid=0.0238 max=0.0405 best=Config(block_sizes=[32, 32, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', '', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 0], range_unroll_factors=[0, 4], range_warp_specializes=[None, None])
[2530s] Autotuning complete in 2530.6s after searching 3248 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[32, 32, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', '', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 0], range_unroll_factors=[0, 4], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 2:
(M, K, N)
----------------
(512, 1024, 128)
 60%|██████    | 3/5 [2:05:11<1:23:28, 2504.33s/it]WARNING:tritonbench.utils.triton_op:Running input ID 3:
(M, K, N)
-----------------
(1024, 2048, 256)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_104", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.03888100013136864, "best_triton_pos": 0}
AUTOTUNE mm(1024x2048, 2048x256)
strides: [2048, 1], [256, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_104 0.0389 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_98 0.0399 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_93 0.0400 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_87 0.0410 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_103 0.0416 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_92 0.0418 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_89 0.0432 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_88 0.0444 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_91 0.0454 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_95 0.0469 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4717 seconds and 0.9262 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2486.12ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 145.37ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024, 2048),
              'stride': (2048, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048, 256),
              'stride': (256, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (256,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 4.83ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 704392404
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:38:83: error: 'tt.load' op operation destroyed but still has uses
        a_tile = tl.load(x + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                  ^
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:38:83: note: - use: %80 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x256xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<8x256xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c256_i32 = arith.constant 256 : i32
    %c2048_i32 = arith.constant 2048 : i32
    %c0_i32 = arith.constant 0 : i32
    %c8192_i32 = arith.constant 8192 : i32
    %cst = arith.constant dense<256> : tensor<8x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<8x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<8x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<8x1xf32, #blocked>
    %cst_3 = arith.constant dense<256> : tensor<256x1xi32, #blocked>
    %cst_4 = arith.constant dense<2048> : tensor<8x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x1xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c8192_i32 : i32
    %2 = arith.muli %1, %c32_i32 : i32
    %3 = arith.subi %c128_i32, %2 : i32
    %4 = arith.minsi %3, %c32_i32 : i32
    %5 = arith.remsi %0, %c8192_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c8_i32 : i32
    %10 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked1>
    %11 = tt.splat %9 : i32 -> tensor<8xi32, #blocked1>
    %12 = arith.addi %11, %10 : tensor<8xi32, #blocked1>
    %13 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>
    %14 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %15 = tt.expand_dims %14 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %16 = ttg.convert_layout %15 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %17 = arith.muli %16, %cst_4 : tensor<8x1xi32, #blocked>
    %18 = tt.broadcast %17 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked>
    %19 = ttg.convert_layout %18 : tensor<8x256xi32, #blocked> -> tensor<8x256xi32, #blocked3>
    %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x256x!tt.ptr<bf16>, #blocked3>
    %21 = tt.splat %8 : i32 -> tensor<256x1xi32, #blocked>
    %22 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<256x1x!tt.ptr<bf16>, #blocked>
    %c512_i32 = arith.constant 512 : i32
    %23 = scf.for %arg4 = %c0_i32 to %c2048_i32 step %c512_i32 iter_args(%arg5 = %cst_5) -> (tensor<8x1xf32, #blocked>)  : i32 {
      %47 = tt.splat %arg4 : i32 -> tensor<256xi32, #blocked1>
      %48 = arith.addi %47, %13 : tensor<256xi32, #blocked1>
      %49 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %51 = ttg.convert_layout %50 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %52 = tt.broadcast %51 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %53 = arith.addi %19, %52 : tensor<8x256xi32, #blocked3>
      %54 = tt.addptr %20, %53 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %55 = tt.load %54 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %56 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %57 = tt.expand_dims %56 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %59 = arith.muli %58, %cst_3 : tensor<256x1xi32, #blocked>
      %60 = arith.addi %59, %21 : tensor<256x1xi32, #blocked>
      %61 = tt.addptr %22, %60 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %62 = tt.load %61 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %63 = ttg.convert_layout %55 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %64 = ttg.convert_layout %62 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %65 = ttg.convert_layout %arg5 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %66 = tt.dot %63, %64, %65 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      %c1_i32 = arith.constant 1 : i32
      %67 = arith.muli %c256_i32, %c1_i32 : i32
      %68 = arith.addi %arg4, %67 : i32
      %69 = tt.splat %68 : i32 -> tensor<256xi32, #blocked1>
      %70 = arith.addi %69, %13 : tensor<256xi32, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %72 = tt.expand_dims %71 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %73 = ttg.convert_layout %72 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %74 = tt.broadcast %73 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %75 = arith.addi %19, %74 : tensor<8x256xi32, #blocked3>
      %76 = tt.addptr %20, %75 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %77 = tt.load %76 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %78 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %79 = tt.expand_dims %78 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %80 = ttg.convert_layout %79 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %81 = arith.muli %80, %cst_3 : tensor<256x1xi32, #blocked>
      %82 = arith.addi %81, %21 : tensor<256x1xi32, #blocked>
      %83 = tt.addptr %22, %82 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %84 = tt.load %83 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %85 = ttg.convert_layout %77 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %86 = ttg.convert_layout %84 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %87 = ttg.convert_layout %66 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %88 = tt.dot %85, %86, %87 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      scf.yield %88 : tensor<8x1xf32, #blocked>
    } {tt.flatten}
    %24 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %25 = tt.splat %24 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
    %26 = tt.load %25 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked1>
    %27 = ttg.convert_layout %26 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %28 = tt.expand_dims %27 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %29 = ttg.convert_layout %28 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
    %30 = arith.extf %29 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
    %31 = tt.broadcast %30 : tensor<1x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
    %32 = arith.addf %23, %31 : tensor<8x1xf32, #blocked>
    %33 = arith.mulf %32, %cst_2 : tensor<8x1xf32, #blocked>
    %34 = arith.mulf %32, %cst_1 : tensor<8x1xf32, #blocked>
    %35 = tt.extern_elementwise %34 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x1xf32, #blocked>) -> tensor<8x1xf32, #blocked>
    %36 = arith.addf %35, %cst_0 : tensor<8x1xf32, #blocked>
    %37 = arith.mulf %33, %36 : tensor<8x1xf32, #blocked>
    %38 = arith.truncf %37 : tensor<8x1xf32, #blocked> to tensor<8x1xbf16, #blocked>
    %39 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %40 = tt.expand_dims %39 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %41 = ttg.convert_layout %40 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %42 = arith.muli %41, %cst : tensor<8x1xi32, #blocked>
    %43 = tt.splat %8 : i32 -> tensor<8x1xi32, #blocked>
    %44 = arith.addi %42, %43 : tensor<8x1xi32, #blocked>
    %45 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x1x!tt.ptr<bf16>, #blocked>
    %46 = tt.addptr %45, %44 : tensor<8x1x!tt.ptr<bf16>, #blocked>, tensor<8x1xi32, #blocked>
    tt.store %46, %38 : tensor<8x1x!tt.ptr<bf16>, #blocked>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:12:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 1, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:38:83: error: 'tt.load' op operation destroyed but still has uses
        a_tile = tl.load(x + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                  ^
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:38:83: note: - use: %80 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x256xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<8x256xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c256_i32 = arith.constant 256 : i32
    %c2048_i32 = arith.constant 2048 : i32
    %c0_i32 = arith.constant 0 : i32
    %c8192_i32 = arith.constant 8192 : i32
    %cst = arith.constant dense<256> : tensor<8x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<8x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<8x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<8x1xf32, #blocked>
    %cst_3 = arith.constant dense<256> : tensor<256x1xi32, #blocked>
    %cst_4 = arith.constant dense<2048> : tensor<8x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x1xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c128_i32 = arith.constant 128 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c8192_i32 : i32
    %2 = arith.muli %1, %c32_i32 : i32
    %3 = arith.subi %c128_i32, %2 : i32
    %4 = arith.minsi %3, %c32_i32 : i32
    %5 = arith.remsi %0, %c8192_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c8_i32 : i32
    %10 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked1>
    %11 = tt.splat %9 : i32 -> tensor<8xi32, #blocked1>
    %12 = arith.addi %11, %10 : tensor<8xi32, #blocked1>
    %13 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>
    %14 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %15 = tt.expand_dims %14 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %16 = ttg.convert_layout %15 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %17 = arith.muli %16, %cst_4 : tensor<8x1xi32, #blocked>
    %18 = tt.broadcast %17 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked>
    %19 = ttg.convert_layout %18 : tensor<8x256xi32, #blocked> -> tensor<8x256xi32, #blocked3>
    %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x256x!tt.ptr<bf16>, #blocked3>
    %21 = tt.splat %8 : i32 -> tensor<256x1xi32, #blocked>
    %22 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<256x1x!tt.ptr<bf16>, #blocked>
    %c512_i32 = arith.constant 512 : i32
    %23 = scf.for %arg4 = %c0_i32 to %c2048_i32 step %c512_i32 iter_args(%arg5 = %cst_5) -> (tensor<8x1xf32, #blocked>)  : i32 {
      %47 = tt.splat %arg4 : i32 -> tensor<256xi32, #blocked1>
      %48 = arith.addi %47, %13 : tensor<256xi32, #blocked1>
      %49 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %51 = ttg.convert_layout %50 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %52 = tt.broadcast %51 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %53 = arith.addi %19, %52 : tensor<8x256xi32, #blocked3>
      %54 = tt.addptr %20, %53 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %55 = tt.load %54 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %56 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %57 = tt.expand_dims %56 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %59 = arith.muli %58, %cst_3 : tensor<256x1xi32, #blocked>
      %60 = arith.addi %59, %21 : tensor<256x1xi32, #blocked>
      %61 = tt.addptr %22, %60 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %62 = tt.load %61 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %63 = ttg.convert_layout %55 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %64 = ttg.convert_layout %62 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %65 = ttg.convert_layout %arg5 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %66 = tt.dot %63, %64, %65 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      %c1_i32 = arith.constant 1 : i32
      %67 = arith.muli %c256_i32, %c1_i32 : i32
      %68 = arith.addi %arg4, %67 : i32
      %69 = tt.splat %68 : i32 -> tensor<256xi32, #blocked1>
      %70 = arith.addi %69, %13 : tensor<256xi32, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %72 = tt.expand_dims %71 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %73 = ttg.convert_layout %72 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %74 = tt.broadcast %73 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %75 = arith.addi %19, %74 : tensor<8x256xi32, #blocked3>
      %76 = tt.addptr %20, %75 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %77 = tt.load %76 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %78 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %79 = tt.expand_dims %78 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %80 = ttg.convert_layout %79 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %81 = arith.muli %80, %cst_3 : tensor<256x1xi32, #blocked>
      %82 = arith.addi %81, %21 : tensor<256x1xi32, #blocked>
      %83 = tt.addptr %22, %82 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %84 = tt.load %83 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %85 = ttg.convert_layout %77 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %86 = ttg.convert_layout %84 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %87 = ttg.convert_layout %66 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %88 = tt.dot %85, %86, %87 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      scf.yield %88 : tensor<8x1xf32, #blocked>
    } {tt.flatten}
    %24 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %25 = tt.splat %24 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
    %26 = tt.load %25 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked1>
    %27 = ttg.convert_layout %26 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %28 = tt.expand_dims %27 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %29 = ttg.convert_layout %28 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
    %30 = arith.extf %29 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
    %31 = tt.broadcast %30 : tensor<1x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
    %32 = arith.addf %23, %31 : tensor<8x1xf32, #blocked>
    %33 = arith.mulf %32, %cst_2 : tensor<8x1xf32, #blocked>
    %34 = arith.mulf %32, %cst_1 : tensor<8x1xf32, #blocked>
    %35 = tt.extern_elementwise %34 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x1xf32, #blocked>) -> tensor<8x1xf32, #blocked>
    %36 = arith.addf %35, %cst_0 : tensor<8x1xf32, #blocked>
    %37 = arith.mulf %33, %36 : tensor<8x1xf32, #blocked>
    %38 = arith.truncf %37 : tensor<8x1xf32, #blocked> to tensor<8x1xbf16, #blocked>
    %39 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %40 = tt.expand_dims %39 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %41 = ttg.convert_layout %40 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %42 = arith.muli %41, %cst : tensor<8x1xi32, #blocked>
    %43 = tt.splat %8 : i32 -> tensor<8x1xi32, #blocked>
    %44 = arith.addi %42, %43 : tensor<8x1xi32, #blocked>
    %45 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x1x!tt.ptr<bf16>, #blocked>
    %46 = tt.addptr %45, %44 : tensor<8x1x!tt.ptr<bf16>, #blocked>, tensor<8x1xi32, #blocked>
    tt.store %46, %38 : tensor<8x1x!tt.ptr<bf16>, #blocked>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/tn/ctnw7ewo3k7ho65seujfcegn3ve3ntktv7pk66qivsyddp4z2n24.py:12:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[79s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 1, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 17.4 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 395.6 configs/s
[129s] Initial random population of 100, 5 starting points: error=9 ok=91 min=0.1575 mid=3.0173 max=4604.6240 best=Config(block_sizes=[16, 64, 16], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[129s] Generation 1 starting: 228 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228/228 8.4 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228/228 18.2 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 11.9 configs/s
[254s] Generation 1 complete: error=10 ok=223 min=0.1238 mid=0.1943 max=2.1358 best=Config(block_sizes=[16, 64, 32], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[254s] Generation 2 starting: 200 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 18.6 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 18.6 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.2 configs/s
[341s] Generation 2 complete: error=11 ok=194 min=0.0906 mid=0.1425 max=0.7383 best=Config(block_sizes=[128, 32, 128], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[341s] Generation 3 starting: 194 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 12.8 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 18.5 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 20.6 configs/s
[417s] Generation 3 complete: error=10 ok=189 min=0.0669 mid=0.1072 max=4.8572 best=Config(block_sizes=[128, 64, 256], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[417s] Generation 4 starting: 204 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 204/204 15.8 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 204/204 19.8 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.3 configs/s
[540s] Generation 4 complete: error=22 ok=187 min=0.0653 mid=0.0864 max=2.2652 best=Config(block_sizes=[32, 64, 128], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[540s] Generation 5 starting: 202 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 15.3 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 20.4 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 10.9 configs/s
[658s] Generation 5 complete: error=27 ok=180 min=0.0628 mid=0.0832 max=0.3052 best=Config(block_sizes=[32, 64, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[658s] Generation 6 starting: 213 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 16.8 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 20.2 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.4 configs/s
[791s] Generation 6 complete: error=27 ok=191 min=0.0617 mid=0.0829 max=0.2267 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[791s] Generation 7 starting: 198 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 16.8 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 20.1 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.4 configs/s
[920s] Generation 7 complete: error=23 ok=180 min=0.0609 mid=0.0832 max=0.2422 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[920s] Generation 8 starting: 212 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212/212 17.5 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212/212 20.5 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.5 configs/s
[1051s] Generation 8 complete: error=26 ok=191 min=0.0607 mid=0.0830 max=0.2408 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1051s] Generation 9 starting: 213 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 17.2 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 20.5 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.2 configs/s
[1185s] Generation 9 complete: error=26 ok=192 min=0.0607 mid=0.0831 max=0.2404 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1185s] Generation 10 starting: 211 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211/211 15.6 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211/211 20.1 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 9.4 configs/s
[1319s] Generation 10 complete: error=26 ok=190 min=0.0607 mid=0.0832 max=0.2410 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1319s] Generation 11 starting: 124 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 16.8 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 20.4 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 12.7 configs/s
[1412s] Generation 11 complete: error=16 ok=111 min=0.0607 mid=0.0682 max=0.1943 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1412s] Generation 12 starting: 82 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82/82 14.1 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82/82 20.6 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 20.7 configs/s
[1471s] Generation 12 complete: error=12 ok=72 min=0.0606 mid=0.0672 max=0.1871 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1471s] Generation 13 starting: 85 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85/85 15.1 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85/85 19.9 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 19.5 configs/s
[1535s] Generation 13 complete: error=10 ok=77 min=0.0608 mid=0.0672 max=0.1858 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1535s] Generation 14 starting: 77 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 14.3 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 18.3 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 18.4 configs/s
[1599s] Generation 14 complete: error=2 ok=77 min=0.0606 mid=0.0668 max=0.1860 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1599s] Generation 15 starting: 84 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 14.1 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 17.9 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 16.7 configs/s
[1671s] Generation 15 complete: error=1 ok=85 min=0.0607 mid=0.0649 max=0.1862 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1671s] Generation 16 starting: 42 neighbors, 1 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42/42 15.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42/42 18.0 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 31.0 configs/s
[1710s] Generation 16 complete: ok=44 min=0.0607 mid=0.0645 max=0.1486 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1710s] Generation 17 starting: 41 neighbors, 1 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41/41 14.8 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41/41 17.6 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 31.8 configs/s
[1747s] Generation 17 complete: ok=43 min=0.0608 mid=0.0645 max=0.1491 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1747s] Generation 18 starting: 42 neighbors, 1 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42/42 17.9 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42/42 18.3 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 30.7 configs/s
[1785s] Generation 18 complete: ok=44 min=0.0607 mid=0.0644 max=0.1487 best=Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1785s] Autotuning complete in 1785.9s after searching 2752 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[32, 64, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', 'last'], loop_orders=[[1, 0]], num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, True]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 3:
(M, K, N)
-----------------
(1024, 2048, 256)
 80%|████████  | 4/5 [2:35:02<37:02, 2222.46s/it]  WARNING:tritonbench.utils.triton_op:Running input ID 4:
(M, K, N)
-----------------
(512, 2048, 4096)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.04ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_138", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 0.15948200225830078, "best_triton_pos": 0}
AUTOTUNE mm(512x2048, 2048x4096)
strides: [2048, 1], [4096, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_138 0.1595 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_135 0.1608 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_146 0.1630 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_142 0.1646 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_126 0.1656 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_139 0.1669 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_132 0.1730 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_145 0.1731 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_136 0.1732 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_137 0.1736 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32='False', BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=1, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.0829 seconds and 0.9235 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3433.57ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 149.87ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512, 2048),
              'stride': (2048, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048, 4096),
              'stride': (4096, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4096,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 5.00ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 704392404
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:38:83: error: 'tt.load' op operation destroyed but still has uses
        a_tile = tl.load(x + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                  ^
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:38:83: note: - use: %80 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x256xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<8x256xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c256_i32 = arith.constant 256 : i32
    %c2048_i32 = arith.constant 2048 : i32
    %c0_i32 = arith.constant 0 : i32
    %c131072_i32 = arith.constant 131072 : i32
    %cst = arith.constant dense<4096> : tensor<8x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<8x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<8x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<8x1xf32, #blocked>
    %cst_3 = arith.constant dense<4096> : tensor<256x1xi32, #blocked>
    %cst_4 = arith.constant dense<2048> : tensor<8x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x1xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c131072_i32 : i32
    %2 = arith.muli %1, %c32_i32 : i32
    %3 = arith.subi %c64_i32, %2 : i32
    %4 = arith.minsi %3, %c32_i32 : i32
    %5 = arith.remsi %0, %c131072_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c8_i32 : i32
    %10 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked1>
    %11 = tt.splat %9 : i32 -> tensor<8xi32, #blocked1>
    %12 = arith.addi %11, %10 : tensor<8xi32, #blocked1>
    %13 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>
    %14 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %15 = tt.expand_dims %14 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %16 = ttg.convert_layout %15 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %17 = arith.muli %16, %cst_4 : tensor<8x1xi32, #blocked>
    %18 = tt.broadcast %17 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked>
    %19 = ttg.convert_layout %18 : tensor<8x256xi32, #blocked> -> tensor<8x256xi32, #blocked3>
    %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x256x!tt.ptr<bf16>, #blocked3>
    %21 = tt.splat %8 : i32 -> tensor<256x1xi32, #blocked>
    %22 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<256x1x!tt.ptr<bf16>, #blocked>
    %c512_i32 = arith.constant 512 : i32
    %23 = scf.for %arg4 = %c0_i32 to %c2048_i32 step %c512_i32 iter_args(%arg5 = %cst_5) -> (tensor<8x1xf32, #blocked>)  : i32 {
      %47 = tt.splat %arg4 : i32 -> tensor<256xi32, #blocked1>
      %48 = arith.addi %47, %13 : tensor<256xi32, #blocked1>
      %49 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %51 = ttg.convert_layout %50 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %52 = tt.broadcast %51 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %53 = arith.addi %19, %52 : tensor<8x256xi32, #blocked3>
      %54 = tt.addptr %20, %53 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %55 = tt.load %54 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %56 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %57 = tt.expand_dims %56 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %59 = arith.muli %58, %cst_3 : tensor<256x1xi32, #blocked>
      %60 = arith.addi %59, %21 : tensor<256x1xi32, #blocked>
      %61 = tt.addptr %22, %60 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %62 = tt.load %61 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %63 = ttg.convert_layout %55 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %64 = ttg.convert_layout %62 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %65 = ttg.convert_layout %arg5 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %66 = tt.dot %63, %64, %65 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      %c1_i32 = arith.constant 1 : i32
      %67 = arith.muli %c256_i32, %c1_i32 : i32
      %68 = arith.addi %arg4, %67 : i32
      %69 = tt.splat %68 : i32 -> tensor<256xi32, #blocked1>
      %70 = arith.addi %69, %13 : tensor<256xi32, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %72 = tt.expand_dims %71 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %73 = ttg.convert_layout %72 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %74 = tt.broadcast %73 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %75 = arith.addi %19, %74 : tensor<8x256xi32, #blocked3>
      %76 = tt.addptr %20, %75 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %77 = tt.load %76 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %78 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %79 = tt.expand_dims %78 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %80 = ttg.convert_layout %79 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %81 = arith.muli %80, %cst_3 : tensor<256x1xi32, #blocked>
      %82 = arith.addi %81, %21 : tensor<256x1xi32, #blocked>
      %83 = tt.addptr %22, %82 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %84 = tt.load %83 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %85 = ttg.convert_layout %77 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %86 = ttg.convert_layout %84 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %87 = ttg.convert_layout %66 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %88 = tt.dot %85, %86, %87 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      scf.yield %88 : tensor<8x1xf32, #blocked>
    } {tt.flatten}
    %24 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %25 = tt.splat %24 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
    %26 = tt.load %25 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked1>
    %27 = ttg.convert_layout %26 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %28 = tt.expand_dims %27 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %29 = ttg.convert_layout %28 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
    %30 = arith.extf %29 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
    %31 = tt.broadcast %30 : tensor<1x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
    %32 = arith.addf %23, %31 : tensor<8x1xf32, #blocked>
    %33 = arith.mulf %32, %cst_2 : tensor<8x1xf32, #blocked>
    %34 = arith.mulf %32, %cst_1 : tensor<8x1xf32, #blocked>
    %35 = tt.extern_elementwise %34 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x1xf32, #blocked>) -> tensor<8x1xf32, #blocked>
    %36 = arith.addf %35, %cst_0 : tensor<8x1xf32, #blocked>
    %37 = arith.mulf %33, %36 : tensor<8x1xf32, #blocked>
    %38 = arith.truncf %37 : tensor<8x1xf32, #blocked> to tensor<8x1xbf16, #blocked>
    %39 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %40 = tt.expand_dims %39 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %41 = ttg.convert_layout %40 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %42 = arith.muli %41, %cst : tensor<8x1xi32, #blocked>
    %43 = tt.splat %8 : i32 -> tensor<8x1xi32, #blocked>
    %44 = arith.addi %42, %43 : tensor<8x1xi32, #blocked>
    %45 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x1x!tt.ptr<bf16>, #blocked>
    %46 = tt.addptr %45, %44 : tensor<8x1x!tt.ptr<bf16>, #blocked>, tensor<8x1xi32, #blocked>
    tt.store %46, %38 : tensor<8x1x!tt.ptr<bf16>, #blocked>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:12:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 1, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
[61s] Timeout after 60s compiling Config(block_sizes=[64, 4, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[4, 0], range_warp_specializes=[False, None])
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:38:83: error: 'tt.load' op operation destroyed but still has uses
        a_tile = tl.load(x + (indices_0[:, None] * 2048 + indices_2[None, :] * 1), None, eviction_policy='evict_last')
                                                                                  ^
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:38:83: note: - use: %80 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x256xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<8x256xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx1200", "ttg.threads-per-warp" = 32 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %c256_i32 = arith.constant 256 : i32
    %c2048_i32 = arith.constant 2048 : i32
    %c0_i32 = arith.constant 0 : i32
    %c131072_i32 = arith.constant 131072 : i32
    %cst = arith.constant dense<4096> : tensor<8x1xi32, #blocked>
    %cst_0 = arith.constant dense<1.000000e+00> : tensor<8x1xf32, #blocked>
    %cst_1 = arith.constant dense<0.707106769> : tensor<8x1xf32, #blocked>
    %cst_2 = arith.constant dense<5.000000e-01> : tensor<8x1xf32, #blocked>
    %cst_3 = arith.constant dense<4096> : tensor<256x1xi32, #blocked>
    %cst_4 = arith.constant dense<2048> : tensor<8x1xi32, #blocked>
    %cst_5 = arith.constant dense<0.000000e+00> : tensor<8x1xf32, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c131072_i32 : i32
    %2 = arith.muli %1, %c32_i32 : i32
    %3 = arith.subi %c64_i32, %2 : i32
    %4 = arith.minsi %3, %c32_i32 : i32
    %5 = arith.remsi %0, %c131072_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c8_i32 : i32
    %10 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked1>
    %11 = tt.splat %9 : i32 -> tensor<8xi32, #blocked1>
    %12 = arith.addi %11, %10 : tensor<8xi32, #blocked1>
    %13 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #blocked1>
    %14 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %15 = tt.expand_dims %14 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %16 = ttg.convert_layout %15 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %17 = arith.muli %16, %cst_4 : tensor<8x1xi32, #blocked>
    %18 = tt.broadcast %17 : tensor<8x1xi32, #blocked> -> tensor<8x256xi32, #blocked>
    %19 = ttg.convert_layout %18 : tensor<8x256xi32, #blocked> -> tensor<8x256xi32, #blocked3>
    %20 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x256x!tt.ptr<bf16>, #blocked3>
    %21 = tt.splat %8 : i32 -> tensor<256x1xi32, #blocked>
    %22 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<256x1x!tt.ptr<bf16>, #blocked>
    %c512_i32 = arith.constant 512 : i32
    %23 = scf.for %arg4 = %c0_i32 to %c2048_i32 step %c512_i32 iter_args(%arg5 = %cst_5) -> (tensor<8x1xf32, #blocked>)  : i32 {
      %47 = tt.splat %arg4 : i32 -> tensor<256xi32, #blocked1>
      %48 = arith.addi %47, %13 : tensor<256xi32, #blocked1>
      %49 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %51 = ttg.convert_layout %50 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %52 = tt.broadcast %51 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %53 = arith.addi %19, %52 : tensor<8x256xi32, #blocked3>
      %54 = tt.addptr %20, %53 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %55 = tt.load %54 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %56 = ttg.convert_layout %48 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %57 = tt.expand_dims %56 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %58 = ttg.convert_layout %57 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %59 = arith.muli %58, %cst_3 : tensor<256x1xi32, #blocked>
      %60 = arith.addi %59, %21 : tensor<256x1xi32, #blocked>
      %61 = tt.addptr %22, %60 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %62 = tt.load %61 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %63 = ttg.convert_layout %55 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %64 = ttg.convert_layout %62 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %65 = ttg.convert_layout %arg5 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %66 = tt.dot %63, %64, %65 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      %c1_i32 = arith.constant 1 : i32
      %67 = arith.muli %c256_i32, %c1_i32 : i32
      %68 = arith.addi %arg4, %67 : i32
      %69 = tt.splat %68 : i32 -> tensor<256xi32, #blocked1>
      %70 = arith.addi %69, %13 : tensor<256xi32, #blocked1>
      %71 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %72 = tt.expand_dims %71 {axis = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x256xi32, #blocked4>
      %73 = ttg.convert_layout %72 : tensor<1x256xi32, #blocked4> -> tensor<1x256xi32, #blocked3>
      %74 = tt.broadcast %73 : tensor<1x256xi32, #blocked3> -> tensor<8x256xi32, #blocked3>
      %75 = arith.addi %19, %74 : tensor<8x256xi32, #blocked3>
      %76 = tt.addptr %20, %75 : tensor<8x256x!tt.ptr<bf16>, #blocked3>, tensor<8x256xi32, #blocked3>
      %77 = tt.load %76 evictionPolicy = evict_last : tensor<8x256x!tt.ptr<bf16>, #blocked3>
      %78 = ttg.convert_layout %70 : tensor<256xi32, #blocked1> -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
      %79 = tt.expand_dims %78 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<256x1xi32, #blocked2>
      %80 = ttg.convert_layout %79 : tensor<256x1xi32, #blocked2> -> tensor<256x1xi32, #blocked>
      %81 = arith.muli %80, %cst_3 : tensor<256x1xi32, #blocked>
      %82 = arith.addi %81, %21 : tensor<256x1xi32, #blocked>
      %83 = tt.addptr %22, %82 : tensor<256x1x!tt.ptr<bf16>, #blocked>, tensor<256x1xi32, #blocked>
      %84 = tt.load %83 : tensor<256x1x!tt.ptr<bf16>, #blocked>
      %85 = ttg.convert_layout %77 : tensor<8x256xbf16, #blocked3> -> tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>>
      %86 = ttg.convert_layout %84 : tensor<256x1xbf16, #blocked> -> tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>>
      %87 = ttg.convert_layout %66 : tensor<8x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
      %88 = tt.dot %85, %86, %87 : tensor<8x256xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked}>> * tensor<256x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked}>> -> tensor<8x1xf32, #blocked>
      scf.yield %88 : tensor<8x1xf32, #blocked>
    } {tt.flatten}
    %24 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %25 = tt.splat %24 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked1>
    %26 = tt.load %25 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked1>
    %27 = ttg.convert_layout %26 : tensor<1xbf16, #blocked1> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %28 = tt.expand_dims %27 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %29 = ttg.convert_layout %28 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked>
    %30 = arith.extf %29 : tensor<1x1xbf16, #blocked> to tensor<1x1xf32, #blocked>
    %31 = tt.broadcast %30 : tensor<1x1xf32, #blocked> -> tensor<8x1xf32, #blocked>
    %32 = arith.addf %23, %31 : tensor<8x1xf32, #blocked>
    %33 = arith.mulf %32, %cst_2 : tensor<8x1xf32, #blocked>
    %34 = arith.mulf %32, %cst_1 : tensor<8x1xf32, #blocked>
    %35 = tt.extern_elementwise %34 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x1xf32, #blocked>) -> tensor<8x1xf32, #blocked>
    %36 = arith.addf %35, %cst_0 : tensor<8x1xf32, #blocked>
    %37 = arith.mulf %33, %36 : tensor<8x1xf32, #blocked>
    %38 = arith.truncf %37 : tensor<8x1xf32, #blocked> to tensor<8x1xbf16, #blocked>
    %39 = ttg.convert_layout %12 : tensor<8xi32, #blocked1> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>>
    %40 = tt.expand_dims %39 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked2}>> -> tensor<8x1xi32, #blocked2>
    %41 = ttg.convert_layout %40 : tensor<8x1xi32, #blocked2> -> tensor<8x1xi32, #blocked>
    %42 = arith.muli %41, %cst : tensor<8x1xi32, #blocked>
    %43 = tt.splat %8 : i32 -> tensor<8x1xi32, #blocked>
    %44 = arith.addi %42, %43 : tensor<8x1xi32, #blocked>
    %45 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x1x!tt.ptr<bf16>, #blocked>
    %46 = tt.addptr %45, %44 : tensor<8x1x!tt.ptr<bf16>, #blocked>, tensor<8x1xi32, #blocked>
    tt.store %46, %38 : tensor<8x1x!tt.ptr<bf16>, #blocked>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1200 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:12:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_fedora/fb/cfbkrss3os3y3z5mk6mqevvzkxdcr3d273ofbzktzk23ci27jfpt.py:12:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[159s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 1, 256], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[None, False]), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100/100 7.4 configs/s
[554s] Initial random population of 100, 5 starting points: error=8 timeout=1 ok=91 min=0.3721 mid=21.5611 max=39260.6602 best=Config(block_sizes=[32, 64, 16], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', '', ''], loop_orders=[[1, 0]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[554s] Generation 1 starting: 215 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215/215 7.0 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215/215 14.5 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662/662 25.2 configs/s
[626s] Generation 1 complete: error=9 ok=211 min=0.3008 mid=1.0633 max=19.1216 best=Config(block_sizes=[32, 128, 16], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[626s] Generation 2 starting: 185 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 12.8 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 17.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 712/712 13.4 configs/s
[707s] Generation 2 complete: error=9 ok=181 min=0.2765 mid=0.5301 max=7.1249 best=Config(block_sizes=[32, 128, 16], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[707s] Generation 3 starting: 186 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186/186 12.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 186/186 16.8 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 834/834 6.9 configs/s
[856s] Generation 3 complete: ok=191 min=0.2401 mid=0.3274 max=3.2229 best=Config(block_sizes=[256, 64, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[0, 1]], num_stages=1, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[856s] Generation 4 starting: 195 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 12.7 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 17.1 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 912/912 6.6 configs/s
[1024s] Generation 4 complete: error=2 ok=198 min=0.2199 mid=0.2976 max=3.0990 best=Config(block_sizes=[256, 64, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', 'last', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1024s] Generation 5 starting: 203 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203/203 12.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203/203 17.8 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916/916 6.6 configs/s
[1196s] Generation 5 complete: error=10 ok=198 min=0.2196 mid=0.2799 max=3.1560 best=Config(block_sizes=[256, 64, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1196s] Generation 6 starting: 206 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 - configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 17.0 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916/916 6.3 configs/s
[1419s] Generation 6 complete: error=10 ok=201 min=0.2196 mid=0.2802 max=3.1454 best=Config(block_sizes=[256, 64, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1419s] Generation 7 starting: 198 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 - configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 18.0 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916/916 6.7 configs/s
[1633s] Generation 7 complete: error=11 ok=192 min=0.2197 mid=0.2812 max=3.1610 best=Config(block_sizes=[256, 64, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1633s] Generation 8 starting: 203 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203/203 - configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 203/203 18.0 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916/916 6.1 configs/s
[1846s] Generation 8 complete: error=11 ok=197 min=0.2192 mid=0.2714 max=2.4240 best=Config(block_sizes=[256, 64, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'last', 'last'], loop_orders=[[0, 1]], num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[None, True])
[1846s] Generation 9 starting: 209 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209/209 - configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209/209 18.2 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916/916 6.5 configs/s
[2070s] Generation 9 complete: error=12 ok=202 min=0.2186 mid=0.2666 max=2.7875 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', '', ''], loop_orders=[[0, 1]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2070s] Generation 10 starting: 215 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215/215 - configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215/215 17.8 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 916/916 6.1 configs/s
[2305s] Generation 10 complete: error=11 ok=209 min=0.2180 mid=0.2664 max=3.0544 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'last', ''], loop_orders=[[0, 1]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2305s] Generation 11 starting: 171 neighbors, 4 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 12.3 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 18.7 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 923/923 7.4 configs/s
[2458s] Generation 11 complete: error=11 ok=164 min=0.2186 mid=0.2456 max=2.9421 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'last', ''], loop_orders=[[0, 1]], num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[None, True])
[2458s] Generation 12 starting: 163 neighbors, 4 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163/163 10.4 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163/163 18.5 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 7.9 configs/s
[2603s] Generation 12 complete: error=12 ok=155 min=0.2160 mid=0.2392 max=2.9037 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'last', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2603s] Generation 13 starting: 129 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 12.5 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 19.3 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 10.1 configs/s
[2714s] Generation 13 complete: error=11 ok=121 min=0.2162 mid=0.2457 max=2.9368 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'last', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2714s] Generation 14 starting: 128 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 12.0 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 18.9 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 9.8 configs/s
[2827s] Generation 14 complete: error=11 ok=120 min=0.2160 mid=0.2458 max=2.1122 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2827s] Generation 15 starting: 128 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 9.9 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128/128 19.1 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 9.8 configs/s
[2944s] Generation 15 complete: error=11 ok=120 min=0.2160 mid=0.2458 max=2.1120 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[2944s] Generation 16 starting: 125 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 12.0 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 19.1 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 10.2 configs/s
[3055s] Generation 16 complete: error=11 ok=117 min=0.2159 mid=0.2466 max=2.1173 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'first', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3055s] Generation 17 starting: 129 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 9.4 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 18.9 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 9.9 configs/s
[3172s] Generation 17 complete: error=11 ok=121 min=0.2157 mid=0.2401 max=2.1092 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3172s] Generation 18 starting: 125 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 9.5 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 19.2 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 10.1 configs/s
[3285s] Generation 18 complete: error=11 ok=117 min=0.2160 mid=0.2467 max=2.1059 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], num_stages=8, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3285s] Generation 19 starting: 123 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 11.4 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 19.3 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 10.2 configs/s
[3395s] Generation 19 complete: error=13 ok=113 min=0.2160 mid=0.2459 max=2.1112 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], num_stages=8, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3395s] Generation 20 starting: 84 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 8.6 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 19.8 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 930/930 15.0 configs/s
[3473s] Generation 20 complete: error=11 ok=75 min=0.2156 mid=0.2293 max=2.0990 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'first', ''], loop_orders=[[1, 0]], num_stages=8, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None])
[3473s] Autotuning complete in 3473.4s after searching 3419 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[64, 128, 64], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'first', ''], loop_orders=[[1, 0]], num_stages=8, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[None, None]), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 4:
(M, K, N)
-----------------
(512, 2048, 4096)
100%|██████████| 5/5 [3:33:01<00:00, 2675.54s/it]100%|██████████| 5/5 [3:33:01<00:00, 2556.23s/it]
INFO:tritonbench.utils.run_utils:[tritonbench] Output result csv to /tmp/tmpytr5mmvz.csv
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
        (M, K, N)    torch_gemm_gelu-latency    torch_gemm_gelu-tflops    mako_gemm_gelu-latency    mako_gemm_gelu-accuracy    mako_gemm_gelu-speedup    mako_gemm_gelu-tflops    kernelllm_gemm_gelu-latency    kernelllm_gemm_gelu-accuracy    kernelllm_gemm_gelu-speedup    kernelllm_gemm_gelu-tflops    torch_compile_max_gemm_gelu-latency    torch_compile_max_gemm_gelu-accuracy    torch_compile_max_gemm_gelu-speedup    torch_compile_max_gemm_gelu-tflops    torch_compile_default_gemm_gelu-latency    torch_compile_default_gemm_gelu-accuracy    torch_compile_default_gemm_gelu-speedup    torch_compile_default_gemm_gelu-tflops    triton_gemm_gelu_kernel-latency    triton_gemm_gelu_kernel-accuracy    triton_gemm_gelu_kernel-speedup    triton_gemm_gelu_kernel-tflops    helion_gemm_gelu_tritonbench-latency    helion_gemm_gelu_tritonbench-accuracy    helion_gemm_gelu_tritonbench-speedup    helion_gemm_gelu_tritonbench-tflops
-----------------  -------------------------  ------------------------  ------------------------  -------------------------  ------------------------  -----------------------  -----------------------------  ------------------------------  -----------------------------  ----------------------------  -------------------------------------  --------------------------------------  -------------------------------------  ------------------------------------  -----------------------------------------  ------------------------------------------  -----------------------------------------  ----------------------------------------  ---------------------------------  ----------------------------------  ---------------------------------  --------------------------------  --------------------------------------  ---------------------------------------  --------------------------------------  -------------------------------------
     (32, 64, 16)         0.015640 (±12.53%)                0.00435396        0.015520 (±13.14%)                          1                  1.00773                0.00438763             0.010921 (±12.81%)                               1                        1.4321                     0.00623533                     0.007720 (±15.03%)                                       1                               2.02591                             0.00882073                         0.011080 (±13.00%)                                           1                                    1.41155                                0.00614585                  0.008200 (±3.90%)                                   1                           1.90732                         0.00830439                      0.006800 (±13.53%)                                      1                                   2.3                                  0.0100141
   (128, 256, 64)         0.015520 (±22.17%)                0.272891          0.015561 (±20.81%)                          1                  0.997365               0.272172               0.012520 (±53.35%)                               1                        1.23962                    0.33828                        0.008721 (±20.17%)                                       1                               1.77961                             0.48564                            0.012040 (±56.81%)                                           1                                    1.28904                                0.351766                    0.009680 (±4.55%)                                   1                           1.60331                         0.437527                        0.007920 (±21.72%)                                      0                                   1.9596                               0.534756
 (512, 1024, 128)          0.027000 (±6.67%)                4.98316            0.027120 (±4.72%)                          1                  0.995575               4.96111                0.034321 (±10.02%)                               1                        0.78669                    3.92021                         0.028400 (±5.92%)                                       1                               0.950704                            4.73751                            0.023640 (±12.86%)                                           1                                    1.14213                                5.69143                     0.030081 (±8.91%)                                   1                           0.897577                        4.47277                         0.020720 (±19.50%)                                      1                                   1.30309                              6.4935
(1024, 2048, 256)          0.092081 (±1.78%)               11.6751             0.092761 (±3.58%)                          1                  0.992669              11.5895                 0.080641 (±20.24%)                               1                        1.14186                   13.3313                         0.143362 (±12.03%)                                       1                               0.642297                            7.49887                            0.076761 (±18.76%)                                           1                                    1.19958                               14.0052                      0.108602 (±4.42%)                                   1                           0.847876                        9.89901                         0.061041 (±16.58%)                                      1                                   1.50851                             17.612
(512, 2048, 4096)          0.425286 (±8.82%)               20.2227             0.424886 (±3.72%)                          1                  1.00094               20.2417                  0.400685 (±2.74%)                               1                        1.0614                    21.4643                          0.220843 (±4.89%)                                       1                               1.92574                            38.9436                              0.418606 (±4.54%)                                           1                                    1.01596                               20.5454                      0.276884 (±3.21%)                                   1                           1.53597                        31.0615                          0.218083 (±14.09%)                                      1                                   1.95011                             39.4365
          average        0.11510539967566728                7.43163          0.11516959816217423                          1                  0.998857               7.41377                0.1078176025301218                               1                        1.13233                    7.81207                        0.0818092005327344                                       1                               1.46485                            10.3349                            0.10842540245503188                                           1                                    1.21165                                8.11998                   0.08668939843773842                                   1                           1.35841                         9.17581                        0.06291279895231128                                      0.8                                 1.80426                             12.8173
INFO:__main__:Benchmark run completed
INFO:__main__:Post-processing 3 output file(s)...
INFO:__main__:Processing output for benchmark 'bf16_layernorm': benchmark_bf16_layernorm.json
INFO:__main__:Processing results from: /home/nvme/fedora/lkesem/KernelGeneration/helion/benchmark_bf16_layernorm.json
INFO:__main__:Successfully moved output to: /home/nvme/fedora/lkesem/KernelGeneration/results/AMD_Radeon_Graphics_gfx1200_20251216_081312/benchmark_bf16_layernorm_AMD_Radeon_Graphics_gfx1200.json
INFO:__main__:Processing output for benchmark 'bf16_matmul': benchmark_bf16_matmul.json
INFO:__main__:Processing results from: /home/nvme/fedora/lkesem/KernelGeneration/helion/benchmark_bf16_matmul.json
INFO:__main__:Successfully moved output to: /home/nvme/fedora/lkesem/KernelGeneration/results/AMD_Radeon_Graphics_gfx1200_20251216_081312/benchmark_bf16_matmul_AMD_Radeon_Graphics_gfx1200.json
INFO:__main__:Processing output for benchmark 'bf16_gemm_gelu': benchmark_bf16_gemm_gelu.json
INFO:__main__:Processing results from: /home/nvme/fedora/lkesem/KernelGeneration/helion/benchmark_bf16_gemm_gelu.json
INFO:__main__:Successfully moved output to: /home/nvme/fedora/lkesem/KernelGeneration/results/AMD_Radeon_Graphics_gfx1200_20251216_081312/benchmark_bf16_gemm_gelu_AMD_Radeon_Graphics_gfx1200.json
INFO:__main__:==================================================
INFO:__main__:POST-PROCESSING SUMMARY
INFO:__main__:==================================================
INFO:__main__:Total: 3 | Success: 3 | Failed: 0
INFO:__main__:Successful:
INFO:__main__:  ✓ bf16_layernorm -> /home/nvme/fedora/lkesem/KernelGeneration/results/AMD_Radeon_Graphics_gfx1200_20251216_081312/benchmark_bf16_layernorm_AMD_Radeon_Graphics_gfx1200.json
INFO:__main__:  ✓ bf16_matmul -> /home/nvme/fedora/lkesem/KernelGeneration/results/AMD_Radeon_Graphics_gfx1200_20251216_081312/benchmark_bf16_matmul_AMD_Radeon_Graphics_gfx1200.json
INFO:__main__:  ✓ bf16_gemm_gelu -> /home/nvme/fedora/lkesem/KernelGeneration/results/AMD_Radeon_Graphics_gfx1200_20251216_081312/benchmark_bf16_gemm_gelu_AMD_Radeon_Graphics_gfx1200.json
