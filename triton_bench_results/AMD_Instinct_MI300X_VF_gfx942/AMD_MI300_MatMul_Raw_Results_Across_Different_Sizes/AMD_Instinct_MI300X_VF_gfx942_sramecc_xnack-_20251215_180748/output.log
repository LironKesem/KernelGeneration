INFO:__main__:Using config from TRITONBENCH_RUN_CONFIG: /home/hotaisle/KernelGeneration/benchmark_helion_runner.yaml
INFO:__main__:Expecting 3 output file(s):
INFO:__main__:  - bf16_layernorm: benchmark_bf16_layernorm.json
INFO:__main__:  - bf16_matmul: benchmark_bf16_matmul.json
INFO:__main__:  - bf16_gemm_gelu: benchmark_bf16_gemm_gelu.json
INFO:__main__:Starting benchmark run...
INFO:tritonbench.utils.run_utils:[tritonbench] Running helion benchmark: /home/hotaisle/myenv/bin/python benchmarks/run.py --device=cuda --precision bf16 --kernel-config ../custom_kernel_config.yaml --kernel bf16_layernorm --output benchmark_bf16_layernorm.json
Loading custom kernel configuration from: ../custom_kernel_config.yaml
Loaded 3 kernel mapping(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Loaded metric mappings for 3 kernel(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Using num_inputs=20 for bf16_layernorm
Running bf16_layernorm benchmark with Helion implementation...

WARNING:tritonbench.utils.triton_op:First-k mode: Selected 3 sequential inputs starting from index 0 (total available: 3)
WARNING:tritonbench.utils.triton_op:Input IDs to run: [0, 1, 2]
Using HIP autotune config
  0%|          | 0/3 [00:00<?, ?it/s]WARNING:tritonbench.utils.triton_op:Running input ID 0:
(M, D)
----------
(512, 512)
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for torch_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_layernorm
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for kernelllm_layernorm
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_layernorm
INFO:tritonbench.utils.triton_op:Took 40.56ms to get benchmark function for torch_compile_max_layernorm
INFO:tritonbench.utils.triton_op:Took 1.26ms to get benchmark function for torch_compile_default_layernorm
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512, 512),
              'stride': (512, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512,),
              'stride': (1,)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512,),
              'stride': (1,)},
            1e-05),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 0.59ms to get benchmark function for helion_layer_norm_tritonbench
[0s] Autotune random seed: 590524584
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 8.2 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 19.1 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 430.5 configs/s
[22s] Initial random population of 100, 5 starting points: error=21 ok=79 min=0.0059 mid=0.0093 max=0.2523 best=Config(block_sizes=[2], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['first', '', 'last', ''], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[1], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[22s] Generation 1 starting: 166 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 166/166 59.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 166/166 15.9 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 103.2         
                                                                  configs/s     
[51s] Generation 1 complete: ok=171 min=0.0061 mid=0.0063 max=0.0109 best=Config(block_sizes=[4], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', 'first', 'last', 'last'], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_interleaved', range_flattens=[True], range_multi_buffers=[False], range_num_stages=[4], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[128], waves_per_eu=1)
[51s] Generation 2 starting: 157 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 157/157 64.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 157/157 15.6 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 108.6         
                                                                  configs/s     
[78s] Generation 2 complete: ok=162 min=0.0062 mid=0.0065 max=0.0102 best=Config(block_sizes=[8], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', 'first', 'last', 'last'], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_interleaved', range_flattens=[True], range_multi_buffers=[False], range_num_stages=[4], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[256], waves_per_eu=1)
[78s] Generation 3 starting: 151 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 151/151 50.9 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 151/151 15.9 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 114.8         
                                                                  configs/s     
[105s] Generation 3 complete: ok=156 min=0.0062 mid=0.0063 max=0.0126 best=Config(block_sizes=[2], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['first', 'first', 'last', ''], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[1], range_unroll_factors=[1], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[105s] Generation 4 starting: 122 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 51.0 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 122/122 15.9 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 143.3         
                                                                  configs/s     
[126s] Generation 4 complete: ok=126 min=0.0062 mid=0.0063 max=0.0125 best=Config(block_sizes=[2], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', 'first', 'first', 'first'], matrix_instr_nonkdim=0, num_stages=7, num_warps=1, pid_type='persistent_interleaved', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[3], range_unroll_factors=[2], range_warp_specializes=[], reduction_loops=[256], waves_per_eu=1)
[126s] Generation 5 starting: 59 neighbors, 2 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59/59 92.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 59/59 15.8 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 293.1         
                                                                  configs/s     
[136s] Generation 5 complete: ok=62 min=0.0061 mid=0.0063 max=0.0125 best=Config(block_sizes=[4], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[3], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[256], waves_per_eu=1)
[136s] Generation 6 starting: 60 neighbors, 2 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60/60 33.4 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 60/60 15.8 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 291.5         
                                                                  configs/s     
[147s] Generation 6 complete: ok=62 min=0.0062 mid=0.0064 max=0.0126 best=Config(block_sizes=[2], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['first', 'first', 'last', ''], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[0], range_unroll_factors=[2], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[147s] Generation 7 starting: 31 neighbors, 1 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31/31 42.0 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 31/31 16.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 540.9         
                                                                  configs/s     
[153s] Generation 7 complete: ok=33 min=0.0062 mid=0.0064 max=0.0084 best=Config(block_sizes=[2], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[3], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[128], waves_per_eu=1)
[153s] Autotuning complete in 153.2s after searching 846 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[2], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', 'last', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[3], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[128], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 0:
(M, D)
----------
(512, 512)
 33%|███▎      | 1/3 [02:38<05:17, 158.69s/it]WARNING:tritonbench.utils.triton_op:Running input ID 1:
(M, D)
----------
(32, 1024)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_layernorm
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_layernorm
INFO:tritonbench.utils.triton_op:Took 2.40ms to get benchmark function for torch_compile_max_layernorm
INFO:tritonbench.utils.triton_op:Took 1.24ms to get benchmark function for torch_compile_default_layernorm
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (32, 1024),
              'stride': (1024, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024,),
              'stride': (1,)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024,),
              'stride': (1,)},
            1e-05),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 1.98ms to get benchmark function for helion_layer_norm_tritonbench
[0s] Autotune random seed: 590524584
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━ 100/100 34.5 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 19.6 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 507.5 configs/s
[13s] Initial random population of 100, 5 starting points: error=17 ok=83 min=0.0059 mid=0.0103 max=0.0417 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[13s] Generation 1 starting: 141 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 141/141 50.6 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 141/141 16.0 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 122.4         
                                                                  configs/s     
[38s] Generation 1 complete: error=1 ok=145 min=0.0060 mid=0.0062 max=0.0109 best=Config(block_sizes=[2], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['last', 'first', '', 'last'], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[256], waves_per_eu=1)
[38s] Generation 2 starting: 107 neighbors, 4 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 107/107 48.3 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 107/107 16.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 159.0         
                                                                  configs/s     
[57s] Generation 2 complete: ok=111 min=0.0060 mid=0.0062 max=0.0107 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[None], range_num_stages=[4], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[57s] Generation 3 starting: 102 neighbors, 4 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 102/102 49.6 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 102/102 15.4 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 169.3         
                                                                  configs/s     
[75s] Generation 3 complete: ok=106 min=0.0059 mid=0.0062 max=0.0090 best=Config(block_sizes=[2], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['last', 'first', '', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=2, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[75s] Generation 4 starting: 98 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98/98 47.2 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 98/98 15.9 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 176.5         
                                                                  configs/s     
[93s] Generation 4 complete: ok=102 min=0.0059 mid=0.0062 max=0.0093 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', 'last', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='persistent_interleaved', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[4], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[93s] Generation 5 starting: 79 neighbors, 3 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 48.8 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 79/79 15.8 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 217.8         
                                                                  configs/s     
[107s] Generation 5 complete: ok=82 min=0.0059 mid=0.0061 max=0.0114 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'pointer'], load_eviction_policies=['first', 'last', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='persistent_interleaved', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[4], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[107s] Generation 6 starting: 52 neighbors, 2 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52/52 52.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 52/52 15.9 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 326.5         
                                                                  configs/s     
[117s] Generation 6 complete: ok=55 min=0.0059 mid=0.0062 max=0.0108 best=Config(block_sizes=[4], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', 'first', '', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[117s] Generation 7 starting: 50 neighbors, 2 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50/50 51.7 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 50/50 15.9 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 348.3         
                                                                  configs/s     
[125s] Generation 7 complete: ok=52 min=0.0059 mid=0.0061 max=0.0098 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[False], range_num_stages=[1], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[125s] Generation 8 starting: 50 neighbors, 2 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50/50 51.4 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 50/50 15.8 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 345.0         
                                                                  configs/s     
[134s] Generation 8 complete: ok=52 min=0.0060 mid=0.0062 max=0.0099 best=Config(block_sizes=[4], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['last', 'first', '', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[134s] Generation 9 starting: 52 neighbors, 2 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52/52 53.0 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 52/52 16.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 333.7         
                                                                  configs/s     
[143s] Generation 9 complete: ok=54 min=0.0059 mid=0.0062 max=0.0097 best=Config(block_sizes=[8], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['last', 'first', '', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[143s] Generation 10 starting: 51 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 51/51 49.7 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 51/51 15.9 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 344.4         
                                                                  configs/s     
[152s] Generation 10 complete: ok=53 min=0.0059 mid=0.0062 max=0.0096 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[True], range_num_stages=[0], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[153s] Generation 11 starting: 51 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 51/51 48.7 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 51/51 15.9 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 343.3         
                                                                  configs/s     
[162s] Generation 11 complete: ok=53 min=0.0059 mid=0.0062 max=0.0114 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[True], range_num_stages=[0], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[162s] Generation 12 starting: 51 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 51/51 59.5 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 51/51 16.0 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 344.0         
                                                                  configs/s     
[171s] Generation 12 complete: ok=53 min=0.0059 mid=0.0062 max=0.0102 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[171s] Generation 13 starting: 51 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 51/51 58.7 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 51/51 16.0 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 340.9         
                                                                  configs/s     
[180s] Generation 13 complete: ok=53 min=0.0059 mid=0.0062 max=0.0098 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[True], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[180s] Generation 14 starting: 45 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 45/45 55.7 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 45/45 16.2 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 388.2         
                                                                  configs/s     
[188s] Generation 14 complete: ok=47 min=0.0060 mid=0.0062 max=0.0109 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[True], range_multi_buffers=[None], range_num_stages=[1], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[188s] Generation 15 starting: 47 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 47/47 55.5 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 47/47 16.3 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 372.9         
                                                                  configs/s     
[196s] Generation 15 complete: ok=49 min=0.0061 mid=0.0062 max=0.0107 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[True], range_multi_buffers=[None], range_num_stages=[2], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[196s] Generation 16 starting: 50 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 50/50 52.3 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 50/50 16.3 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 356.4         
                                                                  configs/s     
[204s] Generation 16 complete: error=1 ok=51 min=0.0060 mid=0.0062 max=0.0103 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[None], range_num_stages=[2], range_unroll_factors=[3], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[204s] Generation 17 starting: 49 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 49/49 56.6 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 49/49 16.3 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 369.9         
                                                                  configs/s     
[213s] Generation 17 complete: error=1 ok=50 min=0.0060 mid=0.0062 max=0.0116 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[None], range_num_stages=[2], range_unroll_factors=[4], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[213s] Generation 18 starting: 48 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 48/48 48.6 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 48/48 16.3 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 369.5         
                                                                  configs/s     
[221s] Generation 18 complete: error=1 ok=49 min=0.0059 mid=0.0062 max=0.0107 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[None], range_num_stages=[3], range_unroll_factors=[4], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[221s] Generation 19 starting: 49 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 49/49 77.4 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 49/49 16.4 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 363.6         
                                                                  configs/s     
[229s] Generation 19 complete: error=1 ok=50 min=0.0059 mid=0.0062 max=0.0115 best=Config(block_sizes=[16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['first', '', '', 'last'], matrix_instr_nonkdim=0, num_stages=3, num_warps=16, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[229s] Generation 20 starting: 50 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 50/50 45.6 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 50/50 16.4 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 357.3         
                                                                  configs/s     
[238s] Generation 20 complete: error=1 ok=51 min=0.0059 mid=0.0063 max=0.0105 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[True], range_num_stages=[3], range_unroll_factors=[4], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
[238s] Autotuning complete in 238.7s after searching 1373 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], load_eviction_policies=['last', '', 'last', 'first'], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='persistent_blocked', range_flattens=[None], range_multi_buffers=[True], range_num_stages=[3], range_unroll_factors=[4], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 1:
(M, D)
----------
(32, 1024)
 67%|██████▋   | 2/3 [06:41<03:28, 208.39s/it]WARNING:tritonbench.utils.triton_op:Running input ID 2:
(M, D)
------------
(2048, 2048)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for triton_layernorm
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for kernelllm_layernorm
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_layernorm
INFO:tritonbench.utils.triton_op:Took 2.58ms to get benchmark function for torch_compile_max_layernorm
INFO:tritonbench.utils.triton_op:Took 1.25ms to get benchmark function for torch_compile_default_layernorm
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048, 2048),
              'stride': (2048, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048,),
              'stride': (1,)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048,),
              'stride': (1,)},
            1e-05),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 5.04ms to get benchmark function for helion_layer_norm_tritonbench
[0s] Autotune random seed: 590524584
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[63s] Timeout after 60s compiling Config(block_sizes=[256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['first', 'last', 'first', 'first'], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_blocked', range_flattens=[False], range_multi_buffers=[False], range_num_stages=[0], range_unroll_factors=[2], range_warp_specializes=[], reduction_loops=[512], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 19.3 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 2548.1 configs/s
[68s] Initial random population of 100, 5 starting points: error=15 timeout=1 ok=84 min=0.0096 mid=0.0484 max=17.3245 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[68s] Generation 1 starting: 136 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 136/136 45.6 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 136/136 15.9 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 138.7         
                                                                  configs/s     
[92s] Generation 1 complete: error=1 ok=140 min=0.0095 mid=0.0120 max=0.0320 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[92s] Generation 2 starting: 120 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 38.9 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 120/120 16.4 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 149.4         
                                                                  configs/s     
[114s] Generation 2 complete: ok=125 min=0.0096 mid=0.0113 max=0.0165 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[114s] Generation 3 starting: 119 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 52.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 119/119 16.6 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 152.6         
                                                                  configs/s     
[134s] Generation 3 complete: ok=124 min=0.0095 mid=0.0110 max=0.0166 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[134s] Generation 4 starting: 121 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 62.3 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 121/121 16.6 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 148.1         
                                                                  configs/s     
[155s] Generation 4 complete: ok=126 min=0.0095 mid=0.0112 max=0.0166 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[155s] Generation 5 starting: 121 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 50.9 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 121/121 16.6 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 147.8         
                                                                  configs/s     
[175s] Generation 5 complete: ok=126 min=0.0097 mid=0.0114 max=0.0166 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[175s] Generation 6 starting: 120 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 54.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 120/120 16.6 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 150.3         
                                                                  configs/s     
[196s] Generation 6 complete: ok=125 min=0.0098 mid=0.0115 max=0.0166 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[196s] Generation 7 starting: 119 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 51.0 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 119/119 16.6 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 150.6         
                                                                  configs/s     
[216s] Generation 7 complete: ok=124 min=0.0095 mid=0.0112 max=0.0166 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[216s] Generation 8 starting: 115 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 54.9 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 115/115 16.6 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 156.8         
                                                                  configs/s     
[236s] Generation 8 complete: ok=120 min=0.0098 mid=0.0115 max=0.0166 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[236s] Generation 9 starting: 117 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 62.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 117/117 16.6 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 152.7         
                                                                  configs/s     
[256s] Generation 9 complete: ok=122 min=0.0098 mid=0.0115 max=0.0167 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[256s] Generation 10 starting: 116 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 53.4 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 116/116 16.6 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 154.7         
                                                                  configs/s     
[276s] Generation 10 complete: ok=121 min=0.0095 mid=0.0112 max=0.0165 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[276s] Generation 11 starting: 115 neighbors, 5 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 62.8 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 115/115 16.6 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 155.6         
                                                                  configs/s     
[295s] Generation 11 complete: ok=120 min=0.0097 mid=0.0114 max=0.0168 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[295s] Generation 12 starting: 116 neighbors, 5 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 52.1 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 116/116 16.6 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 155.7         
                                                                  configs/s     
[315s] Generation 12 complete: ok=121 min=0.0096 mid=0.0113 max=0.0174 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[315s] Generation 13 starting: 113 neighbors, 5 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 50.7 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 113/113 16.5 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 158.3         
                                                                  configs/s     
[335s] Generation 13 complete: ok=118 min=0.0097 mid=0.0114 max=0.0166 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[335s] Generation 14 starting: 115 neighbors, 5 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 54.9 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 115/115 16.6 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 157.7         
                                                                  configs/s     
[354s] Generation 14 complete: ok=120 min=0.0093 mid=0.0111 max=0.0166 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[354s] Generation 15 starting: 112 neighbors, 5 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 54.2 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 112/112 16.5 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 161.2         
                                                                  configs/s     
[374s] Generation 15 complete: ok=117 min=0.0099 mid=0.0116 max=0.0166 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[374s] Generation 16 starting: 116 neighbors, 5 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 63.2 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 116/116 16.6 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 154.1         
                                                                  configs/s     
[393s] Generation 16 complete: ok=121 min=0.0098 mid=0.0115 max=0.0166 best=Config(block_sizes=[1], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[393s] Generation 17 starting: 113 neighbors, 5 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 113/113 54.3 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 113/113 16.5 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 158.2         
                                                                  configs/s     
[412s] Generation 17 complete: ok=118 min=0.0097 mid=0.0115 max=0.0166 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[412s] Generation 18 starting: 110 neighbors, 5 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 110/110 63.1 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 110/110 16.6 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 163.1         
                                                                  configs/s     
[431s] Generation 18 complete: ok=115 min=0.0096 mid=0.0114 max=0.0167 best=Config(block_sizes=[1], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[431s] Generation 19 starting: 114 neighbors, 5 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 114/114 41.2 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 114/114 16.5 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 156.9         
                                                                  configs/s     
[452s] Generation 19 complete: ok=119 min=0.0097 mid=0.0115 max=0.0165 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[452s] Generation 20 starting: 109 neighbors, 5 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 109/109 58.5 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 109/109 16.5 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 165.3         
                                                                  configs/s     
[470s] Generation 20 complete: ok=114 min=0.0095 mid=0.0110 max=0.0170 best=Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1)
[470s] Autotuning complete in 470.6s after searching 2436 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[1], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], load_eviction_policies=['', '', '', ''], matrix_instr_nonkdim=0, num_stages=1, num_warps=4, pid_type='flat', range_flattens=[None], range_multi_buffers=[None], range_num_stages=[0], range_unroll_factors=[0], range_warp_specializes=[], reduction_loops=[None], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 2:
(M, D)
------------
(2048, 2048)
100%|██████████| 3/3 [14:38<00:00, 330.98s/it]100%|██████████| 3/3 [14:38<00:00, 292.91s/it]
INFO:tritonbench.utils.run_utils:[tritonbench] Output result csv to /tmp/tmpy7yac0ks.csv
INFO:__main__:ignoring torch_layernorm-latency
INFO:__main__:ignoring torch_layernorm-best_config
INFO:__main__:ignoring torch_layernorm-gbps
INFO:__main__:ignoring triton_layernorm-best_config
INFO:__main__:ignoring kernelllm_layernorm-best_config
INFO:__main__:ignoring mako_layernorm-best_config
INFO:__main__:ignoring torch_compile_max_layernorm-best_config
INFO:__main__:ignoring torch_compile_default_layernorm-best_config
INFO:__main__:ignoring helion_layer_norm_tritonbench-best_config
INFO:__main__:ignoring torch_layernorm-latency
INFO:__main__:ignoring torch_layernorm-best_config
INFO:__main__:ignoring torch_layernorm-gbps
INFO:__main__:ignoring triton_layernorm-best_config
INFO:__main__:ignoring kernelllm_layernorm-best_config
INFO:__main__:ignoring mako_layernorm-best_config
INFO:__main__:ignoring torch_compile_max_layernorm-best_config
INFO:__main__:ignoring torch_compile_default_layernorm-best_config
INFO:__main__:ignoring helion_layer_norm_tritonbench-best_config
INFO:__main__:ignoring torch_layernorm-latency
INFO:__main__:ignoring torch_layernorm-best_config
INFO:__main__:ignoring torch_layernorm-gbps
INFO:__main__:ignoring triton_layernorm-best_config
INFO:__main__:ignoring kernelllm_layernorm-best_config
INFO:__main__:ignoring mako_layernorm-best_config
INFO:__main__:ignoring torch_compile_max_layernorm-best_config
INFO:__main__:ignoring torch_compile_default_layernorm-best_config
INFO:__main__:ignoring helion_layer_norm_tritonbench-best_config
      (M, D)    torch_layernorm-latency    torch_layernorm-best_config    torch_layernorm-gbps    triton_layernorm-latency    triton_layernorm-accuracy                                         triton_layernorm-best_config    triton_layernorm-gbps    kernelllm_layernorm-latency    kernelllm_layernorm-accuracy    kernelllm_layernorm-best_config    kernelllm_layernorm-gbps    mako_layernorm-latency    mako_layernorm-accuracy    mako_layernorm-best_config    mako_layernorm-gbps    torch_compile_max_layernorm-latency    torch_compile_max_layernorm-accuracy    torch_compile_max_layernorm-best_config    torch_compile_max_layernorm-gbps    torch_compile_default_layernorm-latency    torch_compile_default_layernorm-accuracy    torch_compile_default_layernorm-best_config    torch_compile_default_layernorm-gbps    helion_layer_norm_tritonbench-latency    helion_layer_norm_tritonbench-accuracy    helion_layer_norm_tritonbench-best_config    helion_layer_norm_tritonbench-gbps
------------  -------------------------  -----------------------------  ----------------------  --------------------------  ---------------------------  -------------------------------------------------------------------  -----------------------  -----------------------------  ------------------------------  ---------------------------------  --------------------------  ------------------------  -------------------------  ----------------------------  ---------------------  -------------------------------------  --------------------------------------  -----------------------------------------  ----------------------------------  -----------------------------------------  ------------------------------------------  ---------------------------------------------  --------------------------------------  ---------------------------------------  ----------------------------------------  -------------------------------------------  ------------------------------------
  (512, 512)          0.006615 (±9.10%)                                               158.515           0.044743 (±13.80%)                            1  {'BLOCK_SIZE': 256, 'num_warps': 4, 'num_ctas': 1, 'num_stages': 2}                  23.4355             0.050636 (±17.18%)                               1                                                        20.7081        0.013471 (±43.15%)                          1                                             77.8395                      0.178209 (±11.81%)                                       1                                                                       5.88397                           0.089325 (±9.78%)                                           1                                                                                11.7389                       0.047429 (±12.59%)                                         1                                                                            22.1083
  (32, 1024)         0.006375 (±17.60%)                                                20.5603          0.046307 (±12.12%)                            1  {'BLOCK_SIZE': 512, 'num_warps': 8, 'num_ctas': 1, 'num_stages': 2}                   2.8305             0.053804 (±13.11%)                               1                                                         2.4361        0.016718 (±28.54%)                          1                                              7.84017                    0.433634 (±832.94%)                                       1                                                                       0.302264                          0.091531 (±9.02%)                                           1                                                                                 1.432                        0.046788 (±15.00%)                                         1                                                                             2.8014
(2048, 2048)          0.014553 (±8.27%)                                              1152.84            0.045464 (±14.11%)                            1  {'BLOCK_SIZE': 256, 'num_warps': 4, 'num_ctas': 1, 'num_stages': 2}                 369.022              0.053002 (±15.05%)                               1                                                       316.539         0.017079 (±27.70%)                          1                                            982.33                      0.345273 (±1087.81%)                                       1                                                                      48.5912                            0.089766 (±9.47%)                                           1                                                                               186.899                        0.041335 (±15.23%)                                         1                                                                           405.884
     average        0.00918099998186032                                               443.97          0.045504668106635414                            1                                                                                      131.763            0.052480666587750115                               1                                                       113.228       0.015755999833345413                          1                                            356.003                       0.3190386692682902                                       1                                                                      18.2591                          0.09020733584960301                                           1                                                                                66.6901                      0.04518400008479754                                         1                                                                           143.598
INFO:tritonbench.utils.run_utils:[tritonbench] Running helion benchmark: /home/hotaisle/myenv/bin/python benchmarks/run.py --device=cuda --precision bf16 --kernel-config ../custom_kernel_config.yaml --kernel bf16_matmul --output benchmark_bf16_matmul.json
Loading custom kernel configuration from: ../custom_kernel_config.yaml
Loaded 3 kernel mapping(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Loaded metric mappings for 3 kernel(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Using num_inputs=20 for bf16_matmul
Running bf16_matmul benchmark with Helion implementation...

WARNING:tritonbench.utils.triton_op:First-k mode: Selected 13 sequential inputs starting from index 0 (total available: 13)
WARNING:tritonbench.utils.triton_op:Input IDs to run: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
Using HIP autotune config
Using HIP autotune config
Using HIP autotune config
  0%|          | 0/13 [00:00<?, ?it/s]WARNING:tritonbench.utils.triton_op:Running input ID 0:
(M, K, N)
------------------
(2560, 2688, 2816)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_34", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.0759740024805069, "best_triton_pos": 0}
AUTOTUNE mm(2560x2688, 2688x2816)
strides: [2688, 1], [2816, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_34 0.0760 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_30 0.0791 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_26 0.0820 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_23 0.0876 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_14 0.0949 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_32 0.0972 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_29 0.0987 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_35 0.1023 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_28 0.1036 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_27 0.1042 ms 72.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.7505 seconds and 1.4238 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3549.89ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 59.41ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2560, 2688),
              'stride': (2688, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2688, 2816),
              'stride': (2816, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 0.53ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[65s] Timeout after 60s compiling Config(block_sizes=[1, 128, 1024], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[2, 2], range_warp_specializes=[], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━ 100/100 - configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.9 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 523/523 - configs/s
[91s] Initial random population of 100, 5 starting points: error=30 timeout=1 ok=69 min=0.3637 mid=5.9227 max=1273.4728 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[91s] Generation 1 starting: 209 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 209/209 9.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 209/209 16.8 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 417.0         
                                                                  configs/s     
[133s] Generation 1 complete: error=5 ok=209 min=0.1780 mid=0.4694 max=12.4738 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[133s] Generation 2 starting: 179 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 179/179 9.2 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 179/179 18.2 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 102.6         
                                                                  configs/s     
[176s] Generation 2 complete: error=12 ok=172 min=0.1335 mid=0.2102 max=22.4517 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[176s] Generation 3 starting: 174 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 8.5 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 174/174 19.5 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 33.8 configs/s
[239s] Generation 3 complete: error=15 ok=164 min=0.1218 mid=0.1563 max=2.5281 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 2], range_warp_specializes=[], waves_per_eu=1)
[239s] Generation 4 starting: 185 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 15.6 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 185/185 18.3 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 34.2 configs/s
[296s] Generation 4 complete: error=5 ok=185 min=0.1181 mid=0.1446 max=4.0181 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[296s] Generation 5 starting: 164 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 30.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 164/164 19.4 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 44.1 configs/s
[336s] Generation 5 complete: error=10 ok=158 min=0.1019 mid=0.1415 max=3.1633 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 2], range_unroll_factors=[3, 1], range_warp_specializes=[], waves_per_eu=1)
[336s] Generation 6 starting: 161 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 16.1 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 20.3 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 47.5 configs/s
[380s] Generation 6 complete: error=16 ok=149 min=0.1043 mid=0.1259 max=0.3757 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[4, 2], range_unroll_factors=[3, 1], range_warp_specializes=[], waves_per_eu=1)
[380s] Generation 7 starting: 156 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 156/156 21.9 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 156/156 20.5 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 48.4 configs/s
[419s] Generation 7 complete: error=16 ok=144 min=0.1042 mid=0.1260 max=0.3776 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[4, 2], range_unroll_factors=[3, 1], range_warp_specializes=[], waves_per_eu=1)
[419s] Generation 8 starting: 122 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 23.7 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 122/122 20.0 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 60.0 configs/s
[451s] Generation 8 complete: error=9 ok=117 min=0.0975 mid=0.1239 max=0.3799 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[451s] Generation 9 starting: 114 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 114/114 23.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 114/114 20.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 75.9 configs/s
[477s] Generation 9 complete: error=9 ok=108 min=0.0970 mid=0.1166 max=0.3795 best=Config(block_sizes=[128, 256, 64], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[478s] Generation 10 starting: 121 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 23.2 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 20.0 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 75.1 configs/s
[506s] Generation 10 complete: error=9 ok=115 min=0.1033 mid=0.1204 max=0.3753 best=Config(block_sizes=[128, 256, 64], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[506s] Generation 11 starting: 112 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 112/112 21.7 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 112/112 20.5 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 77.3 configs/s
[532s] Generation 11 complete: error=9 ok=106 min=0.1013 mid=0.1172 max=0.3727 best=Config(block_sizes=[128, 256, 64], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=16, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[532s] Generation 12 starting: 123 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 28.8 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 123/123 20.3 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 69.5 configs/s
[560s] Generation 12 complete: error=9 ok=117 min=0.0993 mid=0.1176 max=0.3707 best=Config(block_sizes=[128, 256, 64], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=16, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[560s] Generation 13 starting: 123 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 27.3 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 123/123 20.3 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 69.4 configs/s
[589s] Generation 13 complete: error=9 ok=117 min=0.0939 mid=0.1182 max=0.3718 best=Config(block_sizes=[128, 256, 64], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[589s] Generation 14 starting: 122 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 26.0 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 20.0 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 69.6 configs/s
[617s] Generation 14 complete: error=8 ok=117 min=0.0949 mid=0.1166 max=0.3706 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[617s] Generation 15 starting: 122 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 23.9 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 20.0 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 70.5 configs/s
[646s] Generation 15 complete: error=8 ok=117 min=0.0939 mid=0.1164 max=0.3756 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[646s] Generation 16 starting: 123 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 26.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 123/123 20.4 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 70.1 configs/s
[674s] Generation 16 complete: error=9 ok=117 min=0.0939 mid=0.1171 max=0.3749 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[674s] Generation 17 starting: 119 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 18.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 119/119 19.9 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 71.2 configs/s
[704s] Generation 17 complete: error=9 ok=113 min=0.0921 mid=0.1196 max=0.3672 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[704s] Generation 18 starting: 114 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 114/114 20.0 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 114/114 20.0 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 73.5 configs/s
[732s] Generation 18 complete: error=9 ok=108 min=0.0920 mid=0.1172 max=0.4106 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[732s] Generation 19 starting: 121 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 19.7 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 18.9 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 74.9 configs/s
[762s] Generation 19 complete: error=9 ok=115 min=0.0941 mid=0.1209 max=0.4308 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[762s] Generation 20 starting: 120 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 19.5 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 120/120 19.9 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 75.8 configs/s
[791s] Generation 20 complete: error=9 ok=114 min=0.0941 mid=0.1217 max=0.4273 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[791s] Autotuning complete in 791.0s after searching 2883 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 0:
(M, K, N)
------------------
(2560, 2688, 2816)
  8%|▊         | 1/13 [13:23<2:40:43, 803.60s/it]WARNING:tritonbench.utils.triton_op:Running input ID 1:
(M, K, N)
------------------
(2688, 2816, 2944)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_66", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.08807799965143204, "best_triton_pos": 0}
AUTOTUNE mm(2688x2816, 2816x2944)
strides: [2816, 1], [2944, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_66 0.0881 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_70 0.0930 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_62 0.0949 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_59 0.0975 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_64 0.1083 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_63 0.1138 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_65 0.1167 ms 75.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_68 0.1221 ms 72.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_71 0.1230 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_57 0.1255 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.9356 seconds and 1.1051 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2681.30ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 68.34ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2688, 2816),
              'stride': (2816, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2816, 2944),
              'stride': (2944, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 5.55ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[65s] Timeout after 60s compiling Config(block_sizes=[1, 128, 1024], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[2, 2], range_warp_specializes=[], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━ 100/100 - configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.6 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 347/347 - configs/s
[94s] Initial random population of 100, 5 starting points: error=30 timeout=1 ok=69 min=0.5845 mid=6.0700 max=1416.8440 best=Config(block_sizes=[16, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 2], range_warp_specializes=[], waves_per_eu=1)
[94s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 24.1 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 15.9 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 855/855 362.6 configs/s
[122s] Generation 1 complete: error=5 ok=202 min=0.2331 mid=0.6326 max=13.3104 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[122s] Generation 2 starting: 182 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 20.2 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 182/182 18.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 375.0         
                                                                  configs/s     
[147s] Generation 2 complete: error=12 ok=175 min=0.1369 mid=0.2676 max=26.7162 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[147s] Generation 3 starting: 178 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 178/178 13.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 178/178 19.7 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 30.1 configs/s
[207s] Generation 3 complete: error=15 ok=168 min=0.1327 mid=0.1860 max=1.1371 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[207s] Generation 4 starting: 183 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 13.4 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 183/183 18.4 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 31.4 configs/s
[267s] Generation 4 complete: error=5 ok=183 min=0.1329 mid=0.1609 max=4.2049 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[267s] Generation 5 starting: 200 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 13.3 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 200/200 19.2 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 31.2 configs/s
[330s] Generation 5 complete: error=11 ok=194 min=0.1190 mid=0.1589 max=3.5357 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[330s] Generation 6 starting: 204 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 204/204 13.6 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 204/204 19.4 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 35.8 configs/s
[389s] Generation 6 complete: error=12 ok=197 min=0.1182 mid=0.1575 max=3.5345 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[389s] Generation 7 starting: 165 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 165/165 10.4 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 165/165 19.6 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 43.9 configs/s
[440s] Generation 7 complete: error=12 ok=157 min=0.1167 mid=0.1525 max=3.5362 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[1, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[440s] Generation 8 starting: 159 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 159/159 10.4 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 159/159 18.7 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 44.4 configs/s
[491s] Generation 8 complete: error=12 ok=151 min=0.1171 mid=0.1492 max=3.5361 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[2, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[491s] Generation 9 starting: 167 neighbors, 4 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 167/167 11.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 167/167 19.8 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 43.9 configs/s
[541s] Generation 9 complete: error=12 ok=159 min=0.1164 mid=0.1483 max=3.5361 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[541s] Generation 10 starting: 170 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 9.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 170/170 19.6 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 43.3 configs/s
[595s] Generation 10 complete: error=12 ok=162 min=0.1164 mid=0.1484 max=3.5359 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[3, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[595s] Generation 11 starting: 166 neighbors, 4 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 166/166 12.9 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 166/166 20.0 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 45.1 configs/s
[644s] Generation 11 complete: error=13 ok=157 min=0.1165 mid=0.1490 max=3.5361 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[644s] Generation 12 starting: 169 neighbors, 4 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 9.9 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 169/169 19.7 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 45.2 configs/s
[696s] Generation 12 complete: error=13 ok=160 min=0.1163 mid=0.1493 max=3.5360 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[696s] Generation 13 starting: 168 neighbors, 4 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 11.1 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 168/168 19.7 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 45.2 configs/s
[747s] Generation 13 complete: error=12 ok=160 min=0.1094 mid=0.1479 max=3.5363 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[747s] Generation 14 starting: 171 neighbors, 4 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 171/171 11.2 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 171/171 19.8 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 45.5 configs/s
[797s] Generation 14 complete: error=12 ok=163 min=0.1091 mid=0.1479 max=3.5359 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[797s] Generation 15 starting: 169 neighbors, 4 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 11.2 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 169/169 19.9 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 47.2 configs/s
[847s] Generation 15 complete: error=13 ok=160 min=0.1092 mid=0.1488 max=3.5358 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[847s] Generation 16 starting: 168 neighbors, 4 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 12.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 168/168 19.9 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 47.0 configs/s
[896s] Generation 16 complete: error=13 ok=159 min=0.1088 mid=0.1486 max=3.5366 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[896s] Generation 17 starting: 169 neighbors, 4 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 9.8 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 169/169 19.6 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 46.7 configs/s
[948s] Generation 17 complete: error=13 ok=160 min=0.1090 mid=0.1488 max=3.5358 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[948s] Generation 18 starting: 166 neighbors, 4 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 166/166 9.9 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 166/166 19.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 47.8 configs/s
[999s] Generation 18 complete: error=13 ok=157 min=0.1091 mid=0.1481 max=3.5358 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1)
[999s] Generation 19 starting: 164 neighbors, 4 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 9.8 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 164/164 19.6 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 49.0 configs/s
[1049s] Generation 19 complete: error=13 ok=155 min=0.1085 mid=0.1488 max=3.5364 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1)
[1049s] Generation 20 starting: 168 neighbors, 4 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 15.0 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 168/168 19.8 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 46.6 configs/s
[1096s] Generation 20 complete: error=11 ok=161 min=0.1093 mid=0.1488 max=3.5361 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1)
[1096s] Autotuning complete in 1096.1s after searching 3587 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 1:
(M, K, N)
------------------
(2688, 2816, 2944)
 15%|█▌        | 2/13 [31:44<2:59:22, 978.42s/it]WARNING:tritonbench.utils.triton_op:Running input ID 2:
(M, K, N)
------------------
(2816, 2944, 3072)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_102", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.08920200169086456, "best_triton_pos": 0}
AUTOTUNE mm(2816x2944, 2944x3072)
strides: [2944, 1], [3072, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_102 0.0892 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_106 0.0912 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_98 0.0979 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_95 0.1060 ms 84.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_100 0.1159 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_101 0.1177 ms 75.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_104 0.1194 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_99 0.1228 ms 72.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_107 0.1247 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_93 0.1366 ms 65.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.9949 seconds and 0.8556 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2490.11ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 69.51ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2816, 2944),
              'stride': (2944, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2944, 3072),
              'stride': (3072, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 7.35ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[66s] Timeout after 60s compiling Config(block_sizes=[1, 128, 1024], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[2, 2], range_warp_specializes=[], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━ 100/100 - configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 4.1 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 366/366 - configs/s
[104s] Initial random population of 100, 5 starting points: error=30 timeout=1 ok=69 min=0.5214 mid=7.5087 max=1628.8844 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[104s] Generation 1 starting: 210 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 14.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 210/210 16.5 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 815/815 292.5 configs/s
[138s] Generation 1 complete: error=5 ok=210 min=0.2282 mid=0.6253 max=9.2493 best=Config(block_sizes=[512, 32, 64], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[138s] Generation 2 starting: 180 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 16.1 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 180/180 18.4 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 196.9         
                                                                  configs/s     
[168s] Generation 2 complete: error=12 ok=173 min=0.1530 mid=0.2684 max=6.8252 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[168s] Generation 3 starting: 175 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 11.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 175/175 19.9 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 28.7 configs/s
[232s] Generation 3 complete: error=15 ok=165 min=0.1406 mid=0.1900 max=3.3268 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 2], range_warp_specializes=[], waves_per_eu=1)
[232s] Generation 4 starting: 187 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 187/187 7.7 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 187/187 18.5 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 29.2 configs/s
[305s] Generation 4 complete: error=6 ok=186 min=0.1374 mid=0.1706 max=4.4458 best=Config(block_sizes=[64, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[305s] Generation 5 starting: 163 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 163/163 9.2 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 163/163 19.8 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 35.5 configs/s
[364s] Generation 5 complete: error=13 ok=154 min=0.1336 mid=0.1600 max=4.4468 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[364s] Generation 6 starting: 168 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 168/168 14.1 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 168/168 20.3 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 37.3 configs/s
[416s] Generation 6 complete: error=14 ok=158 min=0.1256 mid=0.1480 max=4.4468 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[416s] Generation 7 starting: 115 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 9.3 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 115/115 20.1 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 55.7 configs/s
[455s] Generation 7 complete: error=11 ok=107 min=0.1195 mid=0.1476 max=5.0979 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[455s] Generation 8 starting: 121 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 10.1 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 121/121 20.3 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 56.1 configs/s
[495s] Generation 8 complete: error=12 ok=112 min=0.1161 mid=0.1449 max=5.0984 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[495s] Generation 9 starting: 121 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 10.2 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 121/121 20.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 57.8 configs/s
[534s] Generation 9 complete: error=11 ok=113 min=0.1163 mid=0.1478 max=5.0962 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[534s] Generation 10 starting: 83 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 83/83 15.3 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 83/83 20.5 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 86.5 configs/s
[558s] Generation 10 complete: error=10 ok=75 min=0.1143 mid=0.1450 max=5.0966 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[558s] Generation 11 starting: 83 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 83/83 15.2 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 83/83 20.6 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 87.3 configs/s
[582s] Generation 11 complete: error=10 ok=75 min=0.1138 mid=0.1445 max=5.0980 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[582s] Generation 12 starting: 85 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 85/85 14.9 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 85/85 20.5 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 86.1 configs/s
[607s] Generation 12 complete: error=10 ok=77 min=0.1132 mid=0.1454 max=6.4438 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[607s] Generation 13 starting: 83 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 83/83 14.6 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 83/83 20.6 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 87.2 configs/s
[631s] Generation 13 complete: error=10 ok=75 min=0.1132 mid=0.1453 max=6.4406 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[632s] Generation 14 starting: 35 neighbors, 1 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 35/35 12.0 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 35/35 19.0 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 232.4         
                                                                  configs/s     
[642s] Generation 14 complete: error=2 ok=35 min=0.1126 mid=0.1508 max=6.4437 best=Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[642s] Autotuning complete in 642.8s after searching 1908 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 2:
(M, K, N)
------------------
(2816, 2944, 3072)
 23%|██▎       | 3/13 [42:31<2:17:52, 827.25s/it]WARNING:tritonbench.utils.triton_op:Running input ID 3:
(M, K, N)
------------------
(2944, 3072, 3200)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_138", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.10359399765729904, "best_triton_pos": 0}
AUTOTUNE mm(2944x3072, 3072x3200)
strides: [3072, 1], [3200, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_138 0.1036 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_142 0.1086 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_134 0.1088 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_131 0.1167 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_136 0.1223 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_137 0.1305 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_135 0.1306 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_140 0.1384 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_143 0.1389 ms 74.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_129 0.1408 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.0463 seconds and 0.8215 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2505.14ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.11ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2944, 3072),
              'stride': (3072, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3072, 3200),
              'stride': (3200, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 10.88ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[66s] Timeout after 60s compiling Config(block_sizes=[1, 128, 1024], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 4], range_unroll_factors=[2, 2], range_warp_specializes=[], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━ 100/100 - configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.8 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 276/276 - configs/s
[107s] Initial random population of 100, 5 starting points: error=30 timeout=1 ok=69 min=0.7292 mid=9.2800 max=1760.5441 best=Config(block_sizes=[16, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 2], range_warp_specializes=[], waves_per_eu=1)
[107s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 16.4 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 15.7 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 682/682 600.0 configs/s
[138s] Generation 1 complete: error=5 ok=202 min=0.2815 mid=0.8464 max=17.8478 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[138s] Generation 2 starting: 182 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 15.1 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 182/182 17.6 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 218.5         
                                                                  configs/s     
[170s] Generation 2 complete: error=12 ok=175 min=0.1882 mid=0.3155 max=32.1549 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[170s] Generation 3 starting: 174 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 14.4 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 174/174 18.9 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 41.0 configs/s
[220s] Generation 3 complete: error=9 ok=170 min=0.1589 mid=0.2429 max=30.3044 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[220s] Generation 4 starting: 176 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 13.3 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 176/176 18.8 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 27.8 configs/s
[283s] Generation 4 complete: error=5 ok=176 min=0.1575 mid=0.1959 max=4.5894 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[283s] Generation 5 starting: 188 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 188/188 12.0 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 188/188 19.2 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 29.4 configs/s
[348s] Generation 5 complete: error=8 ok=185 min=0.1411 mid=0.1914 max=1.4270 best=Config(block_sizes=[256, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 2], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[348s] Generation 6 starting: 198 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 13.9 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 198/198 20.5 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 31.7 configs/s
[409s] Generation 6 complete: error=19 ok=184 min=0.1390 mid=0.1864 max=1.4331 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[409s] Generation 7 starting: 184 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 8.6 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 184/184 19.6 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 36.2 configs/s
[472s] Generation 7 complete: error=12 ok=177 min=0.1241 mid=0.1734 max=1.5083 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[472s] Generation 8 starting: 201 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 201/201 13.2 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 201/201 19.8 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 34.7 configs/s
[531s] Generation 8 complete: error=12 ok=194 min=0.1227 mid=0.1716 max=1.4330 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[531s] Generation 9 starting: 196 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 196/196 14.6 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 196/196 19.9 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 37.1 configs/s
[588s] Generation 9 complete: error=12 ok=189 min=0.1220 mid=0.1731 max=1.4357 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[588s] Generation 10 starting: 197 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 12.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 197/197 19.8 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 37.4 configs/s
[645s] Generation 10 complete: error=12 ok=190 min=0.1209 mid=0.1736 max=1.4295 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[645s] Generation 11 starting: 194 neighbors, 5 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 11.0 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 194/194 19.9 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 36.5 configs/s
[705s] Generation 11 complete: error=14 ok=185 min=0.1201 mid=0.1699 max=1.4300 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[705s] Generation 12 starting: 197 neighbors, 5 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 12.2 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 197/197 20.1 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 38.2 configs/s
[763s] Generation 12 complete: error=14 ok=188 min=0.1206 mid=0.1734 max=1.4326 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[763s] Generation 13 starting: 198 neighbors, 5 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 15.2 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 198/198 20.3 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 38.0 configs/s
[817s] Generation 13 complete: error=14 ok=189 min=0.1202 mid=0.1713 max=1.4360 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[817s] Generation 14 starting: 196 neighbors, 5 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 196/196 15.0 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 196/196 20.4 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 39.1 configs/s
[871s] Generation 14 complete: error=14 ok=187 min=0.1204 mid=0.1718 max=1.4318 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[871s] Generation 15 starting: 164 neighbors, 4 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 13.6 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 164/164 21.2 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 41.0 configs/s
[920s] Generation 15 complete: error=16 ok=152 min=0.1208 mid=0.1542 max=0.6065 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[920s] Generation 16 starting: 160 neighbors, 4 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 160/160 10.4 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 160/160 20.7 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 43.4 configs/s
[971s] Generation 16 complete: error=16 ok=148 min=0.1205 mid=0.1554 max=0.6053 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[971s] Generation 17 starting: 161 neighbors, 4 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 11.4 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 161/161 21.0 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 42.7 configs/s
[1021s] Generation 17 complete: error=16 ok=149 min=0.1203 mid=0.1551 max=0.6064 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1021s] Generation 18 starting: 162 neighbors, 4 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 10.4 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 162/162 20.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 43.8 configs/s
[1072s] Generation 18 complete: error=16 ok=150 min=0.1212 mid=0.1561 max=0.6062 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1072s] Generation 19 starting: 162 neighbors, 4 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 13.7 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 162/162 21.0 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 42.3 configs/s
[1120s] Generation 19 complete: error=16 ok=150 min=0.1207 mid=0.1539 max=0.6061 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1120s] Generation 20 starting: 160 neighbors, 4 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 160/160 13.7 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 160/160 21.0 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 44.0 configs/s
[1167s] Generation 20 complete: error=16 ok=148 min=0.1207 mid=0.1541 max=0.6063 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1167s] Autotuning complete in 1167.6s after searching 3751 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 3:
(M, K, N)
------------------
(2944, 3072, 3200)
 31%|███       | 4/13 [1:02:03<2:24:31, 963.44s/it]WARNING:tritonbench.utils.triton_op:Running input ID 4:
(M, K, N)
------------------
(3072, 3200, 3328)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_179", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.1429630070924759, "best_triton_pos": 0}
AUTOTUNE mm(3072x3200, 3200x3328)
strides: [3200, 1], [3328, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_179 0.1430 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_173 0.1452 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_176 0.1458 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_167 0.1535 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_158 0.1866 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_172 0.1871 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_174 0.1873 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_170 0.1946 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_178 0.1948 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_171 0.1990 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.1529 seconds and 0.8557 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2837.66ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 69.93ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3072, 3200),
              'stride': (3200, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3200, 3328),
              'stride': (3328, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 7.84ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 0.6 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.3 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 282/282 - configs/s
[87s] Initial random population of 100, 5 starting points: error=31 ok=69 min=0.6792 mid=8.9679 max=1982.9161 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[87s] Generation 1 starting: 210 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 11.2 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 210/210 15.9 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 656/656 3135.4         
                                                                 configs/s      
[123s] Generation 1 complete: error=5 ok=210 min=0.3057 mid=0.8229 max=19.4872 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[123s] Generation 2 starting: 180 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 11.7 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 180/180 17.6 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━ 835/835 67.2 configs/s
[166s] Generation 2 complete: error=12 ok=173 min=0.2656 mid=0.3904 max=39.5128 best=Config(block_sizes=[256, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[166s] Generation 3 starting: 176 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 8.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 176/176 18.3 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 917/917 21.3 configs/s
[245s] Generation 3 complete: error=12 ok=169 min=0.2148 mid=0.2943 max=11.3305 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[245s] Generation 4 starting: 184 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 8.9 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 184/184 18.3 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 961/961 21.4 configs/s
[325s] Generation 4 complete: error=11 ok=178 min=0.2168 mid=0.2684 max=9.8771 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[325s] Generation 5 starting: 204 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 204/204 8.3 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 204/204 18.1 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 969/969 21.0 configs/s
[412s] Generation 5 complete: error=13 ok=196 min=0.2049 mid=0.2632 max=9.9219 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[412s] Generation 6 starting: 206 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 8.3 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 206/206 18.0 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 993/993 21.1 configs/s
[500s] Generation 6 complete: error=12 ok=199 min=0.2029 mid=0.2603 max=9.8644 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[500s] Generation 7 starting: 200 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 7.4 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 200/200 16.5 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 23.2 configs/s
[587s] Generation 7 complete: error=12 ok=193 min=0.2008 mid=0.2587 max=9.9012 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[587s] Generation 8 starting: 161 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 9.0 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 18.0 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 31.6 configs/s
[649s] Generation 8 complete: error=9 ok=156 min=0.2017 mid=0.2611 max=9.9055 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[649s] Generation 9 starting: 124 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 6.6 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 124/124 17.8 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 41.1 configs/s
[703s] Generation 9 complete: error=9 ok=119 min=0.2005 mid=0.2525 max=9.9338 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[703s] Generation 10 starting: 121 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.7 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 17.8 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 44.3 configs/s
[754s] Generation 10 complete: error=9 ok=116 min=0.2007 mid=0.2596 max=10.4940 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[754s] Generation 11 starting: 118 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 6.7 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 118/118 17.9 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 45.0 configs/s
[804s] Generation 11 complete: error=9 ok=113 min=0.2006 mid=0.2517 max=10.5556 best=Config(block_sizes=[64, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[804s] Generation 12 starting: 122 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 8.0 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 17.9 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 41.9 configs/s
[854s] Generation 12 complete: error=9 ok=117 min=0.1941 mid=0.2503 max=10.9366 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[854s] Generation 13 starting: 121 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 11.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 18.7 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 42.8 configs/s
[899s] Generation 13 complete: error=8 ok=116 min=0.1987 mid=0.2413 max=6.1780 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[899s] Generation 14 starting: 117 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 10.9 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 117/117 17.9 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 41.4 configs/s
[944s] Generation 14 complete: error=1 ok=119 min=0.2007 mid=0.2357 max=6.3476 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[944s] Generation 15 starting: 122 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 11.0 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 18.1 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 41.7 configs/s
[991s] Generation 15 complete: error=2 ok=123 min=0.2001 mid=0.2243 max=6.2919 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[991s] Generation 16 starting: 115 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 10.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 115/115 18.3 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 49.6 configs/s
[1032s] Generation 16 complete: error=9 ok=109 min=0.1817 mid=0.2203 max=6.4103 best=Config(block_sizes=[256, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[False, None], range_multi_buffers=[True, True], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1032s] Generation 17 starting: 120 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 10.6 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 120/120 18.9 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 48.5 configs/s
[1074s] Generation 17 complete: error=11 ok=112 min=0.1818 mid=0.2223 max=6.4117 best=Config(block_sizes=[256, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, True], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1074s] Generation 18 starting: 80 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 80/80 10.6 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 80/80 19.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 75.3 configs/s
[1102s] Generation 18 complete: error=11 ok=71 min=0.1812 mid=0.2231 max=6.4129 best=Config(block_sizes=[256, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, True], range_multi_buffers=[True, True], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1104s] Generation 19 starting: 84 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 10.3 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 84/84 19.6 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 71.1 configs/s
[1133s] Generation 19 complete: error=11 ok=75 min=0.1817 mid=0.2230 max=6.4099 best=Config(block_sizes=[256, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, True], range_multi_buffers=[True, False], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1133s] Generation 20 starting: 81 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 10.5 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 81/81 19.7 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 75.1 configs/s
[1162s] Generation 20 complete: error=11 ok=72 min=0.1815 mid=0.2231 max=6.4132 best=Config(block_sizes=[256, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, False], range_multi_buffers=[True, False], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1162s] Autotuning complete in 1162.1s after searching 2946 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[256, 256, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='persistent_blocked', range_flattens=[None, False], range_multi_buffers=[True, False], range_num_stages=[2, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 4:
(M, K, N)
------------------
(3072, 3200, 3328)
 38%|███▊      | 5/13 [1:21:31<2:18:15, 1036.88s/it]WARNING:tritonbench.utils.triton_op:Running input ID 5:
(M, K, N)
------------------
(3200, 3328, 3456)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_203", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4", "best_time": 0.15940099954605103, "best_triton_pos": 0}
AUTOTUNE mm(3200x3328, 3328x3456)
strides: [3328, 1], [3456, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_203 0.1594 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_215 0.1600 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_209 0.1610 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_212 0.1633 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_208 0.1899 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_194 0.1966 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_207 0.1991 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_210 0.2100 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_206 0.2154 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_200 0.2183 ms 73.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.1686 seconds and 0.8402 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2603.02ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.05ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3200, 3328),
              'stride': (3328, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3328, 3456),
              'stride': (3456, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 15.18ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 0.8 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.0 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 - configs/s
[89s] Initial random population of 100, 5 starting points: error=31 ok=69 min=0.9504 mid=10.1799 max=2135.7878 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[89s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 11.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 15.1 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 564/564 2645.9         
                                                                 configs/s      
[126s] Generation 1 complete: error=5 ok=202 min=0.3431 mid=1.0222 max=22.1842 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[126s] Generation 2 starting: 182 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 10.4 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 182/182 17.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━ 782/782 79.4 configs/s
[168s] Generation 2 complete: error=12 ok=175 min=0.2866 mid=0.4765 max=39.0468 best=Config(block_sizes=[256, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[168s] Generation 3 starting: 178 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 178/178 10.4 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 178/178 17.5 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 862/862 23.7 configs/s
[237s] Generation 3 complete: error=6 ok=177 min=0.2270 mid=0.3332 max=37.6281 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[237s] Generation 4 starting: 173 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 9.8 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 173/173 18.4 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 880/880 22.8 configs/s
[306s] Generation 4 complete: error=8 ok=170 min=0.2320 mid=0.2798 max=4.4717 best=Config(block_sizes=[64, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[306s] Generation 5 starting: 199 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 10.0 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 199/199 18.4 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 881/881 21.7 configs/s
[383s] Generation 5 complete: error=9 ok=195 min=0.2247 mid=0.2909 max=4.4720 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[383s] Generation 6 starting: 198 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 9.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 198/198 18.4 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 945/945 21.7 configs/s
[464s] Generation 6 complete: error=8 ok=195 min=0.2100 mid=0.2777 max=4.5275 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[464s] Generation 7 starting: 186 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 186/186 9.9 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 186/186 18.4 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 958/958 24.0 configs/s
[537s] Generation 7 complete: error=5 ok=186 min=0.2102 mid=0.2756 max=4.5090 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[537s] Generation 8 starting: 198 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 198/198 10.1 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 198/198 18.6 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 965/965 23.1 configs/s
[614s] Generation 8 complete: error=8 ok=195 min=0.2104 mid=0.2621 max=4.4878 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[615s] Generation 9 starting: 185 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 9.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 185/185 18.4 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 965/965 23.8 configs/s
[689s] Generation 9 complete: error=7 ok=183 min=0.2093 mid=0.2471 max=4.5444 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[689s] Generation 10 starting: 193 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 193/193 10.0 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 193/193 17.0 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 967/967 23.0 configs/s
[766s] Generation 10 complete: error=11 ok=187 min=0.2094 mid=0.2445 max=2.0917 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[766s] Generation 11 starting: 38 neighbors, 1 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38/38 9.4 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 38/38 18.5 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━ 967/967 113.8 configs/s
[782s] Generation 11 complete: ok=39 min=0.2077 mid=0.2559 max=0.3756 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[782s] Generation 12 starting: 39 neighbors, 1 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 12.3 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 39/39 19.4 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━ 967/967 118.0 configs/s
[798s] Generation 12 complete: ok=40 min=0.2077 mid=0.2542 max=0.3757 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[798s] Generation 13 starting: 40 neighbors, 1 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 40/40 12.7 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 40/40 19.9 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 134.1         
                                                                  configs/s     
[813s] Generation 13 complete: error=1 ok=40 min=0.2036 mid=0.2480 max=0.3767 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[813s] Generation 14 starting: 39 neighbors, 1 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 10.3 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 39/39 19.1 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 136.9         
                                                                  configs/s     
[828s] Generation 14 complete: error=1 ok=39 min=0.1984 mid=0.2430 max=0.3734 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[828s] Autotuning complete in 828.6s after searching 2150 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 5:
(M, K, N)
------------------
(3200, 3328, 3456)
 46%|████▌     | 6/13 [1:35:24<1:52:53, 967.68s/it] WARNING:tritonbench.utils.triton_op:Running input ID 6:
(M, K, N)
------------------
(3328, 3456, 3584)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_251", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.15839800238609314, "best_triton_pos": 0}
AUTOTUNE mm(3328x3456, 3456x3584)
strides: [3456, 1], [3584, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_251 0.1584 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_248 0.1666 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_245 0.1679 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_239 0.1689 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_244 0.1960 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_246 0.1997 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_230 0.2065 ms 76.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_242 0.2102 ms 75.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_243 0.2106 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_250 0.2110 ms 75.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.2111 seconds and 0.8627 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2904.60ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.22ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3328, 3456),
              'stride': (3456, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3456, 3584),
              'stride': (3584, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 12.49ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 1.3 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.6 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 214/214 - configs/s
[85s] Initial random population of 100, 5 starting points: error=31 ok=69 min=0.8971 mid=11.3194 max=2341.6240 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[85s] Generation 1 starting: 210 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 10.2 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 210/210 15.5 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 535/535 2320.5         
                                                                 configs/s      
[125s] Generation 1 complete: error=5 ok=210 min=0.3565 mid=1.0391 max=13.9196 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[125s] Generation 2 starting: 180 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 10.2 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 180/180 15.3 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━ 755/755 69.4 configs/s
[170s] Generation 2 complete: error=12 ok=173 min=0.2947 mid=0.4307 max=45.7350 best=Config(block_sizes=[256, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[170s] Generation 3 starting: 179 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 179/179 7.5 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 179/179 18.4 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 838/838 20.0 configs/s
[249s] Generation 3 complete: error=12 ok=172 min=0.2446 mid=0.3392 max=7.0650 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[249s] Generation 4 starting: 184 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 8.1 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 184/184 18.3 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 840/840 18.8 configs/s
[331s] Generation 4 complete: error=11 ok=178 min=0.2457 mid=0.2999 max=11.3624 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[331s] Generation 5 starting: 206 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 9.3 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 206/206 16.6 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 841/841 18.7 configs/s
[414s] Generation 5 complete: error=13 ok=198 min=0.2356 mid=0.2988 max=11.4230 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[414s] Generation 6 starting: 200 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 8.1 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 200/200 18.4 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 848/848 19.9 configs/s
[497s] Generation 6 complete: error=13 ok=192 min=0.2347 mid=0.2992 max=11.3882 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[497s] Generation 7 starting: 205 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 7.8 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 205/205 18.3 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 852/852 19.4 configs/s
[584s] Generation 7 complete: error=12 ok=198 min=0.2341 mid=0.2899 max=11.5839 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[584s] Generation 8 starting: 197 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 197/197 7.8 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 197/197 17.7 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 854/854 19.0 configs/s
[669s] Generation 8 complete: error=5 ok=197 min=0.2330 mid=0.2824 max=11.3740 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[669s] Generation 9 starting: 205 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 205/205 7.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 205/205 17.9 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 951/951 21.6 configs/s
[755s] Generation 9 complete: error=6 ok=204 min=0.2164 mid=0.2830 max=11.3716 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[756s] Generation 10 starting: 159 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 159/159 7.3 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 159/159 17.9 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 27.7 configs/s
[826s] Generation 10 complete: error=6 ok=157 min=0.2158 mid=0.2806 max=11.4205 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[826s] Generation 11 starting: 160 neighbors, 4 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 160/160 6.5 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 160/160 17.7 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 25.7 configs/s
[900s] Generation 11 complete: error=5 ok=159 min=0.2156 mid=0.2788 max=11.4072 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[900s] Generation 12 starting: 124 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 6.2 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 124/124 17.5 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 37.7 configs/s
[956s] Generation 12 complete: error=3 ok=125 min=0.2171 mid=0.2899 max=11.5407 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[956s] Generation 13 starting: 124 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 6.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 124/124 17.3 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 37.1 configs/s
[1015s] Generation 13 complete: error=3 ok=125 min=0.2160 mid=0.2925 max=11.6344 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1015s] Generation 14 starting: 121 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.2 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 17.5 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 39.5 configs/s
[1070s] Generation 14 complete: error=3 ok=122 min=0.2147 mid=0.2961 max=11.4142 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1070s] Generation 15 starting: 121 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 5.9 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 17.3 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 40.1 configs/s
[1126s] Generation 15 complete: error=3 ok=122 min=0.2151 mid=0.2949 max=11.4414 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1126s] Generation 16 starting: 122 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 6.0 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 17.3 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 39.4 configs/s
[1182s] Generation 16 complete: error=3 ok=123 min=0.2148 mid=0.2924 max=11.4728 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1182s] Generation 17 starting: 119 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 6.0 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 119/119 17.3 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 40.5 configs/s
[1237s] Generation 17 complete: error=3 ok=120 min=0.2148 mid=0.2937 max=11.3412 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1237s] Generation 18 starting: 120 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 8.1 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 120/120 17.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 40.1 configs/s
[1287s] Generation 18 complete: error=3 ok=121 min=0.2150 mid=0.2924 max=11.4063 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1287s] Generation 19 starting: 117 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 6.0 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 117/117 17.3 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 42.4 configs/s
[1340s] Generation 19 complete: error=3 ok=118 min=0.2148 mid=0.2979 max=11.3648 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1340s] Generation 20 starting: 118 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 6.3 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 118/118 17.3 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━ 951/951 39.5 configs/s
[1394s] Generation 20 complete: error=3 ok=119 min=0.2144 mid=0.2931 max=12.0178 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1394s] Autotuning complete in 1394.3s after searching 3271 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 6:
(M, K, N)
------------------
(3328, 3456, 3584)
 54%|█████▍    | 7/13 [1:58:43<1:50:52, 1108.80s/it]WARNING:tritonbench.utils.triton_op:Running input ID 7:
(M, K, N)
------------------
(3456, 3584, 3712)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_275", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4", "best_time": 0.17463600635528564, "best_triton_pos": 0}
AUTOTUNE mm(3456x3584, 3584x3712)
strides: [3584, 1], [3712, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_275 0.1746 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_287 0.1818 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_284 0.1818 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_281 0.1853 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_280 0.2064 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_279 0.2227 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_282 0.2239 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_278 0.2324 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_286 0.2334 ms 74.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_273 0.2486 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.2112 seconds and 0.8316 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2645.48ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.10ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3456, 3584),
              'stride': (3584, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3584, 3712),
              'stride': (3712, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 14.81ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 1.5 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 3.3 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 - configs/s
[90s] Initial random population of 100, 5 starting points: error=31 ok=69 min=1.1799 mid=12.6950 max=2529.8413 best=Config(block_sizes=[16, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 2], range_warp_specializes=[], waves_per_eu=1)
[90s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 9.3 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 14.3 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 451/451 3085.5         
                                                                 configs/s      
[131s] Generation 1 complete: error=5 ok=202 min=0.4236 mid=1.3004 max=29.6010 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[131s] Generation 2 starting: 182 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 9.4 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 182/182 16.7 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━ 697/697 189.9 configs/s
[170s] Generation 2 complete: error=12 ok=175 min=0.3029 mid=0.5311 max=51.6345 best=Config(block_sizes=[256, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[170s] Generation 3 starting: 178 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 178/178 8.4 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 178/178 17.0 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 765/765 44.8 configs/s
[224s] Generation 3 complete: error=6 ok=177 min=0.2573 mid=0.4042 max=48.2662 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[224s] Generation 4 starting: 169 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 7.9 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 169/169 18.4 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 777/777 19.6 configs/s
[298s] Generation 4 complete: error=11 ok=163 min=0.2407 mid=0.3302 max=7.1004 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[298s] Generation 5 starting: 193 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 193/193 7.5 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 193/193 18.3 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 830/830 19.7 configs/s
[380s] Generation 5 complete: error=11 ok=187 min=0.2370 mid=0.3185 max=7.1075 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[380s] Generation 6 starting: 196 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 196/196 7.9 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 196/196 18.4 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 843/843 20.9 configs/s
[462s] Generation 6 complete: error=11 ok=190 min=0.2350 mid=0.3110 max=7.0725 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[462s] Generation 7 starting: 161 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 8.3 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 19.1 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 851/851 25.0 configs/s
[527s] Generation 7 complete: error=11 ok=154 min=0.2350 mid=0.3062 max=7.0897 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[527s] Generation 8 starting: 161 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 8.5 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 18.9 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 851/851 25.5 configs/s
[593s] Generation 8 complete: error=11 ok=154 min=0.2336 mid=0.3067 max=7.0876 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[593s] Generation 9 starting: 159 neighbors, 4 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 159/159 8.2 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 159/159 19.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 856/856 26.3 configs/s
[656s] Generation 9 complete: error=11 ok=152 min=0.2338 mid=0.3111 max=7.0788 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[656s] Generation 10 starting: 159 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 159/159 6.7 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 159/159 16.0 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 856/856 25.8 configs/s
[727s] Generation 10 complete: error=11 ok=152 min=0.2347 mid=0.3132 max=7.0749 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[727s] Generation 11 starting: 121 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 7.1 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 19.6 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 856/856 37.0 configs/s
[777s] Generation 11 complete: error=12 ok=112 min=0.2340 mid=0.3053 max=7.0592 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[777s] Generation 12 starting: 118 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 7.3 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 118/118 19.8 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 856/856 38.1 configs/s
[825s] Generation 12 complete: error=12 ok=109 min=0.2341 mid=0.3065 max=7.0552 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[825s] Generation 13 starting: 118 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 7.2 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 118/118 19.4 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 856/856 38.1 configs/s
[874s] Generation 13 complete: error=12 ok=109 min=0.2338 mid=0.3048 max=7.0553 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[874s] Generation 14 starting: 117 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 117/117 7.3 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 117/117 19.8 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━ 856/856 39.0 configs/s
[922s] Generation 14 complete: error=12 ok=108 min=0.2336 mid=0.3054 max=7.0546 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[922s] Generation 15 starting: 118 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 9.1 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 118/118 15.5 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━ 856/856 39.8 configs/s
[968s] Generation 15 complete: error=12 ok=109 min=0.2309 mid=0.3016 max=7.0776 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[968s] Generation 16 starting: 121 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 7.3 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 19.4 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━ 866/866 38.1 configs/s
[1018s] Generation 16 complete: error=12 ok=112 min=0.2302 mid=0.3024 max=7.1203 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[1018s] Generation 17 starting: 115 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 7.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 115/115 19.5 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━ 868/868 41.2 configs/s
[1065s] Generation 17 complete: error=12 ok=106 min=0.2317 mid=0.3094 max=7.1179 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[1065s] Generation 18 starting: 116 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 116/116 7.2 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 116/116 19.5 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━ 868/868 40.6 configs/s
[1113s] Generation 18 complete: error=12 ok=107 min=0.2337 mid=0.3021 max=7.1197 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[1113s] Generation 19 starting: 119 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 119/119 7.2 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 119/119 19.4 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━ 868/868 38.8 configs/s
[1162s] Generation 19 complete: error=12 ok=110 min=0.2339 mid=0.3020 max=7.1201 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[1162s] Generation 20 starting: 111 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 111/111 7.1 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 111/111 19.4 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━ 868/868 42.8 configs/s
[1209s] Generation 20 complete: error=11 ok=103 min=0.2359 mid=0.3033 max=8.6910 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[1209s] Autotuning complete in 1209.7s after searching 3034 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=4, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 7:
(M, K, N)
------------------
(3456, 3584, 3712)
 62%|██████▏   | 8/13 [2:18:58<1:35:12, 1142.45s/it]WARNING:tritonbench.utils.triton_op:Running input ID 8:
(M, K, N)
------------------
(3584, 3712, 3840)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_311", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4", "best_time": 0.1824129968881607, "best_triton_pos": 0}
AUTOTUNE mm(3584x3712, 3712x3840)
strides: [3712, 1], [3840, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_311 0.1824 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_323 0.1831 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_320 0.1890 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_317 0.1916 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_316 0.2112 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_318 0.2205 ms 82.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_322 0.2247 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_314 0.2296 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_315 0.2353 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_309 0.2482 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.2737 seconds and 0.8606 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2761.60ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.23ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3584, 3712),
              'stride': (3712, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3712, 3840),
              'stride': (3840, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 7.18ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 1.5 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 2.7 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193/193 - configs/s
[108s] Initial random population of 100, 5 starting points: error=32 ok=68 min=0.9794 mid=16.7167 max=2853.0713 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[108s] Generation 1 starting: 210 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 210/210 7.6 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 210/210 14.6 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 436/436 2594.9         
                                                                 configs/s      
[155s] Generation 1 complete: error=5 ok=210 min=0.4457 mid=1.2725 max=30.8354 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[155s] Generation 2 starting: 180 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 7.6 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 180/180 16.7 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━ 679/679 216.7 configs/s
[197s] Generation 2 complete: error=12 ok=173 min=0.3087 mid=0.5670 max=58.0279 best=Config(block_sizes=[256, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[197s] Generation 3 starting: 179 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 179/179 6.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 179/179 18.0 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 743/743 26.0 configs/s
[268s] Generation 3 complete: error=12 ok=172 min=0.2787 mid=0.3901 max=14.8955 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[268s] Generation 4 starting: 187 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 187/187 6.2 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 187/187 18.1 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 756/756 17.2 configs/s
[357s] Generation 4 complete: error=12 ok=180 min=0.2720 mid=0.3542 max=13.2131 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[357s] Generation 5 starting: 155 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 155/155 4.2 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 155/155 17.0 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 780/780 23.7 configs/s
[440s] Generation 5 complete: error=5 ok=154 min=0.2687 mid=0.3302 max=13.2237 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[440s] Generation 6 starting: 161 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 5.0 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 17.0 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 780/780 22.9 configs/s
[521s] Generation 6 complete: error=3 ok=162 min=0.2565 mid=0.3210 max=13.2934 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[521s] Generation 7 starting: 162 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 4.7 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 162/162 17.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 780/780 22.8 configs/s
[602s] Generation 7 complete: error=4 ok=162 min=0.2508 mid=0.3158 max=13.7564 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[602s] Generation 8 starting: 159 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 159/159 4.5 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 159/159 18.2 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 797/797 21.6 configs/s
[684s] Generation 8 complete: error=4 ok=159 min=0.2469 mid=0.3168 max=7.8859 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[684s] Generation 9 starting: 121 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 121/121 18.9 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 810/810 30.5 configs/s
[738s] Generation 9 complete: error=4 ok=121 min=0.2461 mid=0.3152 max=3.5788 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[738s] Generation 10 starting: 75 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 6.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 75/75 18.2 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 812/812 56.1 configs/s
[772s] Generation 10 complete: error=1 ok=77 min=0.2468 mid=0.3404 max=3.5755 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[772s] Generation 11 starting: 74 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 4.1 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 74/74 17.7 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 812/812 57.9 configs/s
[812s] Generation 11 complete: error=1 ok=76 min=0.2546 mid=0.3460 max=3.6326 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[812s] Generation 12 starting: 73 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73/73 4.2 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 73/73 17.7 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 812/812 55.0 configs/s
[851s] Generation 12 complete: error=1 ok=75 min=0.2468 mid=0.3370 max=3.6329 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[851s] Generation 13 starting: 74 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 5.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 74/74 18.1 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 812/812 53.9 configs/s
[889s] Generation 13 complete: error=1 ok=76 min=0.2461 mid=0.3363 max=3.6490 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[889s] Generation 14 starting: 75 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 6.9 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 75/75 18.1 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━ 812/812 53.7 configs/s
[921s] Generation 14 complete: error=1 ok=77 min=0.2462 mid=0.3364 max=3.6430 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[921s] Generation 15 starting: 75 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75/75 6.5 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 75/75 18.0 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━ 812/812 54.5 configs/s
[956s] Generation 15 complete: error=2 ok=76 min=0.2457 mid=0.3332 max=3.9799 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[956s] Generation 16 starting: 71 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71/71 6.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 71/71 18.3 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━ 813/813 61.0 configs/s
[988s] Generation 16 complete: error=2 ok=72 min=0.2456 mid=0.3361 max=3.9706 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[988s] Generation 17 starting: 72 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72/72 6.7 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 72/72 18.4 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━ 814/814 58.5 configs/s
[1018s] Generation 17 complete: error=2 ok=73 min=0.2459 mid=0.3361 max=3.9741 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1018s] Generation 18 starting: 74 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74/74 6.1 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 74/74 18.0 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━ 814/814 56.4 configs/s
[1052s] Generation 18 complete: error=2 ok=75 min=0.2476 mid=0.3314 max=3.9773 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1053s] Generation 19 starting: 70 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 70/70 6.1 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 70/70 18.4 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━ 814/814 61.9 configs/s
[1083s] Generation 19 complete: error=2 ok=71 min=0.2496 mid=0.3348 max=3.9715 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1083s] Generation 20 starting: 73 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73/73 6.2 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 73/73 18.4 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━ 814/814 58.5 configs/s
[1117s] Generation 20 complete: error=2 ok=74 min=0.2493 mid=0.3312 max=3.9726 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1117s] Autotuning complete in 1117.5s after searching 2420 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 8:
(M, K, N)
------------------
(3584, 3712, 3840)
 69%|██████▉   | 9/13 [2:37:40<1:15:44, 1136.20s/it]WARNING:tritonbench.utils.triton_op:Running input ID 9:
(M, K, N)
------------------
(3712, 3840, 3968)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_353", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4", "best_time": 0.205144003033638, "best_triton_pos": 0}
AUTOTUNE mm(3712x3840, 3840x3968)
strides: [3840, 1], [3968, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_353 0.2051 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_359 0.2085 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_347 0.2088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_356 0.2088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_352 0.2300 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_354 0.2374 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_351 0.2468 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_358 0.2500 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_350 0.2508 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_345 0.2840 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.2734 seconds and 0.8188 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2799.69ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.16ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3712, 3840),
              'stride': (3840, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3840, 3968),
              'stride': (3968, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 6.51ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 1.5 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 2.5 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136/136 - configs/s
[113s] Initial random population of 100, 5 starting points: error=32 ok=68 min=1.5296 mid=18.1347 max=2949.2371 best=Config(block_sizes=[16, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 2], range_warp_specializes=[], waves_per_eu=1)
[113s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 7.5 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 13.4 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 368/368 2556.5         
                                                                 configs/s      
[160s] Generation 1 complete: error=5 ok=202 min=0.5296 mid=1.6253 max=36.0411 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[160s] Generation 2 starting: 182 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 7.0 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 182/182 16.1 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━ 550/550 69.0 configs/s
[211s] Generation 2 complete: error=12 ok=175 min=0.3655 mid=0.7212 max=69.3645 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[211s] Generation 3 starting: 174 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 7.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 174/174 17.4 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 716/716 97.6 configs/s
[256s] Generation 3 complete: error=9 ok=170 min=0.2986 mid=0.4954 max=59.3617 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[256s] Generation 4 starting: 169 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 6.7 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 169/169 17.6 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 716/716 19.7 configs/s
[331s] Generation 4 complete: error=7 ok=167 min=0.2975 mid=0.3923 max=7.4288 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[331s] Generation 5 starting: 186 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 186/186 5.2 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 186/186 18.1 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 716/716 19.2 configs/s
[418s] Generation 5 complete: error=12 ok=179 min=0.2707 mid=0.3547 max=10.0791 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[418s] Generation 6 starting: 195 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 195/195 5.5 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 195/195 18.9 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 738/738 21.4 configs/s
[503s] Generation 6 complete: error=23 ok=177 min=0.2575 mid=0.3679 max=10.0201 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[503s] Generation 7 starting: 155 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 155/155 7.1 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 155/155 18.6 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 776/776 29.4 configs/s
[564s] Generation 7 complete: error=14 ok=145 min=0.2537 mid=0.3650 max=10.0175 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[564s] Generation 8 starting: 118 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 6.6 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 118/118 18.7 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 788/788 39.4 configs/s
[612s] Generation 8 complete: error=13 ok=108 min=0.2540 mid=0.3441 max=10.0487 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[612s] Generation 9 starting: 121 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.6 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 121/121 18.8 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 788/788 37.6 configs/s
[661s] Generation 9 complete: error=13 ok=111 min=0.2477 mid=0.3429 max=10.0495 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[661s] Generation 10 starting: 120 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 6.5 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 120/120 18.7 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 807/807 38.6 configs/s
[711s] Generation 10 complete: error=13 ok=110 min=0.2498 mid=0.3431 max=10.0404 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[711s] Generation 11 starting: 121 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.7 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 18.8 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 807/807 37.2 configs/s
[761s] Generation 11 complete: error=13 ok=111 min=0.2485 mid=0.3413 max=10.0445 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[761s] Generation 12 starting: 120 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 6.9 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 120/120 19.5 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 807/807 36.9 configs/s
[812s] Generation 12 complete: error=13 ok=110 min=0.2493 mid=0.3406 max=10.0210 best=Config(block_sizes=[128, 128, 64], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[812s] Generation 13 starting: 121 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.9 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 19.5 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 807/807 36.1 configs/s
[862s] Generation 13 complete: error=13 ok=111 min=0.2405 mid=0.3358 max=10.0078 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[862s] Generation 14 starting: 122 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 6.9 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 19.7 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━ 831/831 33.3 configs/s
[914s] Generation 14 complete: error=13 ok=112 min=0.2412 mid=0.3341 max=10.0383 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[914s] Generation 15 starting: 124 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 6.9 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 124/124 19.7 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━ 831/831 32.5 configs/s
[968s] Generation 15 complete: error=13 ok=114 min=0.2395 mid=0.3332 max=10.0245 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[968s] Generation 16 starting: 124 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 124/124 6.9 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 124/124 19.6 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━ 835/835 33.2 configs/s
[1021s] Generation 16 complete: error=13 ok=114 min=0.2417 mid=0.3346 max=10.0152 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1021s] Generation 17 starting: 121 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 121/121 6.9 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 121/121 19.7 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━ 835/835 34.0 configs/s
[1074s] Generation 17 complete: error=13 ok=111 min=0.2407 mid=0.3342 max=10.0366 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1074s] Generation 18 starting: 123 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 6.9 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 123/123 19.6 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━ 835/835 33.0 configs/s
[1127s] Generation 18 complete: error=13 ok=113 min=0.2365 mid=0.3337 max=10.0085 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1127s] Generation 19 starting: 120 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 120/120 6.8 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 120/120 19.7 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━ 845/845 35.3 configs/s
[1180s] Generation 19 complete: error=13 ok=110 min=0.2378 mid=0.3345 max=10.0726 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1180s] Generation 20 starting: 118 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 118/118 6.8 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 118/118 19.8 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━ 845/845 36.7 configs/s
[1230s] Generation 20 complete: error=13 ok=108 min=0.2381 mid=0.3347 max=10.0087 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1230s] Autotuning complete in 1230.7s after searching 2936 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 9:
(M, K, N)
------------------
(3712, 3840, 3968)
 77%|███████▋  | 10/13 [2:58:16<58:20, 1166.91s/it] WARNING:tritonbench.utils.triton_op:Running input ID 10:
(M, K, N)
------------------
(3840, 3968, 4096)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_395", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.19853000342845917, "best_triton_pos": 0}
AUTOTUNE mm(3840x3968, 3968x4096)
strides: [3968, 1], [4096, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_395 0.1985 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_389 0.2136 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_392 0.2149 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_390 0.2362 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_394 0.2419 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_386 0.2579 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_388 0.2928 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_374 0.3015 ms 65.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_387 0.3072 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_383 0.3249 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.3129 seconds and 0.9069 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3032.22ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.13ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3840, 3968),
              'stride': (3968, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3968, 4096),
              'stride': (4096, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 13.91ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 1.7 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 2.2 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154/154 - configs/s
[122s] Initial random population of 100, 5 starting points: error=32 ok=68 min=1.2610 mid=18.4496 max=3379.0603 best=Config(block_sizes=[512, 16, 32], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=4, num_warps=16, pid_type='persistent_blocked', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[0, 1], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[122s] Generation 1 starting: 222 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 222/222 5.4 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 222/222 14.4 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 345/345 331.6 configs/s
[184s] Generation 1 complete: error=10 ok=217 min=0.5525 mid=1.5926 max=45.9558 best=Config(block_sizes=[128, 32, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[184s] Generation 2 starting: 188 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 188/188 5.2 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 188/188 16.7 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━ 521/521 149.4 configs/s
[238s] Generation 2 complete: error=16 ok=177 min=0.3786 mid=0.6771 max=72.7784 best=Config(block_sizes=[128, 64, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[238s] Generation 3 starting: 185 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 185/185 3.7 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 185/185 18.2 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 643/643 20.2 configs/s
[327s] Generation 3 complete: error=22 ok=168 min=0.2962 mid=0.4627 max=79.6161 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[327s] Generation 4 starting: 184 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 184/184 2.5 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 184/184 17.0 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 675/675 20.8 configs/s
[426s] Generation 4 complete: error=15 ok=174 min=0.2908 mid=0.3917 max=25.0725 best=Config(block_sizes=[128, 128, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=8, num_warps=1, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[426s] Generation 5 starting: 115 neighbors, 3 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 115/115 1.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 115/115 16.4 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 716/716 26.8 configs/s
[515s] Generation 5 complete: error=6 ok=113 min=0.2684 mid=0.3678 max=24.7903 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 1], range_warp_specializes=[], waves_per_eu=1)
[515s] Generation 6 starting: 131 neighbors, 3 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 131/131 2.3 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 131/131 19.1 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 745/745 29.3 configs/s
[597s] Generation 6 complete: error=16 ok=118 min=0.2606 mid=0.3584 max=4.6045 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 1], range_warp_specializes=[], waves_per_eu=1)
[597s] Generation 7 starting: 132 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 132/132 3.0 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 132/132 19.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 767/767 30.1 configs/s
[672s] Generation 7 complete: error=17 ok=118 min=0.2547 mid=0.3411 max=5.0543 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[3, 1], range_warp_specializes=[], waves_per_eu=1)
[672s] Generation 8 starting: 92 neighbors, 2 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92/92 5.5 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 92/92 21.0 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 785/785 46.9 configs/s
[714s] Generation 8 complete: error=16 ok=78 min=0.2480 mid=0.3406 max=5.0541 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[4, 1], range_warp_specializes=[], waves_per_eu=1)
[715s] Generation 9 starting: 92 neighbors, 2 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92/92 4.2 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 92/92 21.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 806/806 46.4 configs/s
[762s] Generation 9 complete: error=16 ok=78 min=0.2472 mid=0.3407 max=5.0497 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[3, 3], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[762s] Generation 10 starting: 91 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 3.2 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 91/91 20.4 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 808/808 47.4 configs/s
[816s] Generation 10 complete: error=16 ok=77 min=0.2476 mid=0.3405 max=5.0441 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[816s] Generation 11 starting: 89 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89/89 6.5 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 89/89 21.9 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 808/808 49.2 configs/s
[853s] Generation 11 complete: error=16 ok=75 min=0.2478 mid=0.3409 max=5.0607 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[853s] Generation 12 starting: 91 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 5.4 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 91/91 21.1 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 808/808 47.3 configs/s
[895s] Generation 12 complete: error=16 ok=77 min=0.2487 mid=0.3408 max=5.0533 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[896s] Generation 13 starting: 95 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95/95 5.5 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 95/95 21.2 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 808/808 45.0 configs/s
[939s] Generation 13 complete: error=17 ok=80 min=0.2482 mid=0.3408 max=5.0527 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[939s] Generation 14 starting: 94 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 5.4 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 94/94 21.3 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━ 808/808 47.1 configs/s
[984s] Generation 14 complete: error=17 ok=79 min=0.2477 mid=0.3406 max=5.0532 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[985s] Generation 15 starting: 95 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95/95 3.0 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 95/95 20.8 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━ 808/808 46.1 configs/s
[1042s] Generation 15 complete: error=18 ok=79 min=0.2442 mid=0.3410 max=5.0537 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[1042s] Generation 16 starting: 93 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93/93 5.4 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 93/93 21.4 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━ 818/818 47.0 configs/s
[1085s] Generation 16 complete: error=17 ok=78 min=0.2441 mid=0.3403 max=5.0501 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[1085s] Generation 17 starting: 94 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 5.3 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 94/94 21.4 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━ 822/822 46.8 configs/s
[1130s] Generation 17 complete: error=17 ok=79 min=0.2442 mid=0.3399 max=5.0589 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[2, 0], range_warp_specializes=[], waves_per_eu=1)
[1130s] Generation 18 starting: 92 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92/92 6.7 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 92/92 21.5 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━ 823/823 49.2 configs/s
[1169s] Generation 18 complete: error=17 ok=77 min=0.2439 mid=0.3403 max=5.0559 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[1169s] Generation 19 starting: 94 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 6.6 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 94/94 21.4 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━ 824/824 48.9 configs/s
[1208s] Generation 19 complete: error=17 ok=79 min=0.2450 mid=0.3394 max=5.0576 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[1208s] Generation 20 starting: 90 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 90/90 3.2 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 90/90 20.8 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━ 824/824 49.8 configs/s
[1260s] Generation 20 complete: error=17 ok=75 min=0.2441 mid=0.3399 max=5.0486 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1260s] Autotuning complete in 1260.5s after searching 2459 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[4], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[2, 3], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 10:
(M, K, N)
------------------
(3840, 3968, 4096)
 85%|████████▍ | 11/13 [3:19:22<39:54, 1197.16s/it]WARNING:tritonbench.utils.triton_op:Running input ID 11:
(M, K, N)
------------------
(3968, 4096, 4224)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.01ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_431", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.230322003364563, "best_triton_pos": 0}
AUTOTUNE mm(3968x4096, 4096x4224)
strides: [4096, 1], [4224, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_431 0.2303 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_428 0.2359 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_425 0.2379 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_426 0.2600 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_422 0.2707 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_430 0.2743 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_424 0.3113 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_423 0.3276 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_429 0.3366 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_419 0.3673 ms 62.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 1.2963 seconds and 0.8331 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2960.81ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 70.88ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (3968, 4096),
              'stride': (4096, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4096, 4224),
              'stride': (4224, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 12.03ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 3.3 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 1.8 configs/s
[515s] Initial random population of 100, 5 starting points: error=33 ok=67 min=0.8190 mid=19.0112 max=26439.8750 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 1], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[515s] Generation 1 starting: 209 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 209/209 6.0 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 209/209 15.4 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 429/429 2172.2         
                                                                 configs/s      
[569s] Generation 1 complete: error=14 ok=200 min=0.4744 mid=1.5408 max=21.8053 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[569s] Generation 2 starting: 183 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 5.8 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 183/183 17.0 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━ 536/536 58.6 configs/s
[626s] Generation 2 complete: error=10 ok=178 min=0.3827 mid=0.7699 max=16.6455 best=Config(block_sizes=[128, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[626s] Generation 3 starting: 169 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 169/169 6.2 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 169/169 14.7 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 575/575 16.9 configs/s
[703s] Generation 3 complete: error=9 ok=165 min=0.3235 mid=0.5152 max=7.1578 best=Config(block_sizes=[128, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[703s] Generation 4 starting: 183 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 6.0 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 183/183 17.9 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 640/640 18.5 configs/s
[783s] Generation 4 complete: error=9 ok=179 min=0.3254 mid=0.4185 max=10.1487 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[783s] Generation 5 starting: 194 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 6.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 194/194 18.1 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 650/650 19.7 configs/s
[863s] Generation 5 complete: error=14 ok=185 min=0.3190 mid=0.4200 max=26.1088 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[863s] Generation 6 starting: 194 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 194/194 6.0 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 194/194 18.1 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 651/651 17.2 configs/s
[949s] Generation 6 complete: error=15 ok=184 min=0.3257 mid=0.4088 max=28.2447 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[949s] Generation 7 starting: 123 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 5.9 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 123/123 18.8 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 651/651 28.0 configs/s
[1003s] Generation 7 complete: error=11 ok=116 min=0.3245 mid=0.4015 max=10.2456 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1003s] Generation 8 starting: 77 neighbors, 2 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 5.9 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 77/77 19.7 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 651/651 45.7 configs/s
[1038s] Generation 8 complete: error=11 ok=69 min=0.3202 mid=0.3644 max=10.2512 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1038s] Generation 9 starting: 39 neighbors, 1 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 6.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 39/39 19.4 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 651/651 67.2 configs/s
[1058s] Generation 9 complete: error=1 ok=40 min=0.3182 mid=0.3911 max=0.5362 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1058s] Generation 10 starting: 39 neighbors, 1 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 5.5 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 39/39 17.8 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 651/651 66.6 configs/s
[1080s] Generation 10 complete: ok=41 min=0.3190 mid=0.3814 max=0.5542 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1080s] Generation 11 starting: 39 neighbors, 1 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 5.3 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 39/39 17.8 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 651/651 67.8 configs/s
[1104s] Generation 11 complete: ok=41 min=0.2973 mid=0.3795 max=0.5270 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1104s] Generation 12 starting: 39 neighbors, 1 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 5.3 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 39/39 17.8 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 672/672 68.8 configs/s
[1126s] Generation 12 complete: ok=41 min=0.3160 mid=0.3902 max=0.5268 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1126s] Generation 13 starting: 39 neighbors, 1 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39/39 5.4 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 39/39 17.8 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 672/672 82.9 configs/s
[1147s] Generation 13 complete: ok=41 min=0.3163 mid=0.4122 max=0.5448 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[1147s] Autotuning complete in 1147.4s after searching 1627 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=16, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 11:
(M, K, N)
------------------
(3968, 4096, 4224)
 92%|█████████▏| 12/13 [3:38:34<19:43, 1183.59s/it]WARNING:tritonbench.utils.triton_op:Running input ID 12:
(M, K, N)
------------------
(4096, 4224, 4352)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for triton_matmul
INFO:tritonbench.utils.triton_op:Took 0.07ms to get benchmark function for kernelllm_matmul
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for mako_matmul
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_467", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.23316900432109833, "best_triton_pos": 0}
AUTOTUNE mm(4096x4224, 4224x4352)
strides: [4224, 1], [4352, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_467 0.2332 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_461 0.2409 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_464 0.2450 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_462 0.2572 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=256, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_466 0.2575 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_458 0.2767 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_460 0.3111 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_459 0.3426 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_455 0.3481 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_465 0.3492 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=256, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 1.3667 seconds and 0.8471 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 3020.43ms to get benchmark function for torch_compile_max_matmul
INFO:tritonbench.utils.triton_op:Took 69.91ms to get benchmark function for torch_compile_default_matmul
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4096, 4224),
              'stride': (4224, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4224, 4352),
              'stride': (4352, 1)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 9.68ms to get benchmark function for helion_matmul_tritonbench
[0s] Autotune random seed: 591424575
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[63s] Timeout after 60s compiling Config(block_sizes=[1, 128, 2048], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 0], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[67s] Timeout after 60s compiling Config(block_sizes=[2, 4, 8192], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=1, num_warps=8, pid_type='persistent_blocked', range_flattens=[True, False], range_multi_buffers=[True, False], range_num_stages=[4, 4], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[77s] Timeout after 60s compiling Config(block_sizes=[1, 1, 4096], indexing=['pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=2, pid_type='persistent_blocked', range_flattens=[True, True], range_multi_buffers=[False, False], range_num_stages=[1, 3], range_unroll_factors=[2, 3], range_warp_specializes=[], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 1.6 configs/s
[379s] Initial random population of 100, 5 starting points: error=32 timeout=3 ok=65 min=0.7668 mid=25.4170 max=8303.1211 best=Config(block_sizes=[64, 256, 16], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 1], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[379s] Generation 1 starting: 208 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 208/208 5.7 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 208/208 15.2 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━━ 427/427 1474.6         
                                                                 configs/s      
[435s] Generation 1 complete: error=14 ok=199 min=0.4678 mid=1.7936 max=24.6888 best=Config(block_sizes=[128, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 1], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[435s] Generation 2 starting: 189 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 189/189 5.7 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 189/189 16.8 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━━━ 577/577 51.3 configs/s
[497s] Generation 2 complete: error=11 ok=183 min=0.3539 mid=0.6999 max=19.5010 best=Config(block_sizes=[128, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[497s] Generation 3 starting: 183 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 183/183 6.0 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 183/183 17.9 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━━━ 658/658 25.4 configs/s
[570s] Generation 3 complete: error=9 ok=179 min=0.3013 mid=0.4671 max=9.3795 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[570s] Generation 4 starting: 176 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 5.3 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 176/176 17.0 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━━━ 678/678 22.9 configs/s
[649s] Generation 4 complete: error=12 ok=169 min=0.2916 mid=0.3847 max=26.8373 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[649s] Generation 5 starting: 161 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 5.3 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 17.2 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━━━ 693/693 21.0 configs/s
[727s] Generation 5 complete: error=14 ok=151 min=0.2882 mid=0.3935 max=25.2435 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[727s] Generation 6 starting: 161 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 161/161 3.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 161/161 17.0 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━━━ 694/694 22.8 configs/s
[814s] Generation 6 complete: error=16 ok=149 min=0.2888 mid=0.3753 max=25.2502 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[814s] Generation 7 starting: 167 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 167/167 2.7 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 167/167 16.7 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━━━ 695/695 21.6 configs/s
[908s] Generation 7 complete: error=16 ok=155 min=0.2891 mid=0.3753 max=25.2573 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[908s] Generation 8 starting: 166 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 166/166 3.4 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 166/166 17.1 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━━━ 695/695 21.8 configs/s
[995s] Generation 8 complete: error=16 ok=154 min=0.2879 mid=0.3750 max=25.8464 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[995s] Generation 9 starting: 165 neighbors, 4 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 165/165 5.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 165/165 17.1 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━━━ 695/695 22.0 configs/s
[1070s] Generation 9 complete: error=16 ok=153 min=0.2879 mid=0.3755 max=25.8622 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[1070s] Generation 10 starting: 166 neighbors, 4 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 166/166 5.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 166/166 17.2 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━━━ 695/695 21.7 configs/s
[1145s] Generation 10 complete: error=15 ok=155 min=0.2893 mid=0.3753 max=25.8470 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'pointer'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[1145s] Generation 11 starting: 165 neighbors, 4 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 165/165 4.6 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 165/165 17.7 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━━━ 695/695 23.5 configs/s
[1225s] Generation 11 complete: error=23 ok=146 min=0.2847 mid=0.3732 max=25.8485 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[1225s] Generation 12 starting: 165 neighbors, 4 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 165/165 5.4 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 165/165 17.8 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━━━ 702/702 23.6 configs/s
[1301s] Generation 12 complete: error=24 ok=145 min=0.2849 mid=0.3703 max=26.1028 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1301s] Generation 13 starting: 154 neighbors, 4 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 154/154 5.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 154/154 16.5 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 24.4 configs/s
[1379s] Generation 13 complete: error=13 ok=145 min=0.2835 mid=0.3732 max=26.9133 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, True], range_multi_buffers=[False, False], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1380s] Generation 14 starting: 163 neighbors, 4 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 163/163 5.1 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 163/163 16.7 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 22.5 configs/s
[1459s] Generation 14 complete: error=15 ok=152 min=0.2827 mid=0.3740 max=26.8902 best=Config(block_sizes=[256, 256, 32], indexing=['tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[1459s] Generation 15 starting: 162 neighbors, 4 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 5.1 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 162/162 16.5 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 22.7 configs/s
[1538s] Generation 15 complete: error=15 ok=151 min=0.2836 mid=0.3746 max=26.8740 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, True], range_multi_buffers=[True, False], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1538s] Generation 16 starting: 166 neighbors, 4 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 166/166 5.1 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 166/166 16.5 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 21.3 configs/s
[1620s] Generation 16 complete: error=15 ok=155 min=0.2835 mid=0.3736 max=26.8754 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=8, pid_type='persistent_interleaved', range_flattens=[None, True], range_multi_buffers=[True, True], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1620s] Generation 17 starting: 164 neighbors, 4 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 5.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 164/164 16.7 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 22.0 configs/s
[1701s] Generation 17 complete: error=15 ok=153 min=0.2836 mid=0.3749 max=26.8977 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[True, True], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1701s] Generation 18 starting: 122 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 122/122 4.2 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 122/122 16.6 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 33.5 configs/s
[1764s] Generation 18 complete: error=13 ok=112 min=0.2829 mid=0.3742 max=26.1111 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[True, True], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1764s] Generation 19 starting: 123 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 4.2 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 123/123 16.5 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 32.3 configs/s
[1828s] Generation 19 complete: error=12 ok=114 min=0.2834 mid=0.3738 max=26.0907 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[True, True], range_num_stages=[3, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1828s] Generation 20 starting: 125 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 125/125 4.2 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 125/125 17.5 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━━━ 721/721 32.3 configs/s
[1893s] Generation 20 complete: error=13 ok=115 min=0.2830 mid=0.3742 max=26.1013 best=Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=7, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[True, True], range_num_stages=[3, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[1893s] Autotuning complete in 1893.6s after searching 3348 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[256, 256, 32], indexing=['pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[8], load_eviction_policies=['first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=7, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[True, True], range_num_stages=[3, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 12:
(M, K, N)
------------------
(4096, 4224, 4352)
100%|██████████| 13/13 [4:10:13<00:00, 1400.24s/it]100%|██████████| 13/13 [4:10:13<00:00, 1154.88s/it]
INFO:tritonbench.utils.run_utils:[tritonbench] Output result csv to /tmp/tmpw8139vd7.csv
         (M, K, N)    torch_matmul-latency    torch_matmul-tflops    triton_matmul-latency    triton_matmul-accuracy    triton_matmul-speedup    triton_matmul-tflops    kernelllm_matmul-latency    kernelllm_matmul-accuracy    kernelllm_matmul-speedup    kernelllm_matmul-tflops    mako_matmul-latency    mako_matmul-accuracy    mako_matmul-speedup    mako_matmul-tflops    torch_compile_max_matmul-latency    torch_compile_max_matmul-accuracy    torch_compile_max_matmul-speedup    torch_compile_max_matmul-tflops    torch_compile_default_matmul-latency    torch_compile_default_matmul-accuracy    torch_compile_default_matmul-speedup    torch_compile_default_matmul-tflops    helion_matmul_tritonbench-latency    helion_matmul_tritonbench-accuracy    helion_matmul_tritonbench-speedup    helion_matmul_tritonbench-tflops
------------------  ----------------------  ---------------------  -----------------------  ------------------------  -----------------------  ----------------------  --------------------------  ---------------------------  --------------------------  -------------------------  ---------------------  ----------------------  ---------------------  --------------------  ----------------------------------  -----------------------------------  ----------------------------------  ---------------------------------  --------------------------------------  ---------------------------------------  --------------------------------------  -------------------------------------  -----------------------------------  ------------------------------------  -----------------------------------  ----------------------------------
(2560, 2688, 2816)       0.076135 (±6.53%)                509.035       0.081988 (±11.83%)                         1                 0.928612                 472.696           0.135631 (±2.51%)                            1                    0.561339                    285.741      0.148100 (±4.85%)                       1               0.514078               261.684                   0.077538 (±4.40%)                                    1                            0.981906                            499.824                       0.073890 (±9.33%)                                        1                                1.03038                                 524.501                    0.095013 (±6.12%)                                     1                             0.801311                             407.895
(2688, 2816, 2944)       0.083668 (±9.11%)                532.685        0.100907 (±2.70%)                         1                 0.82916                  441.681           0.143001 (±4.54%)                            1                    0.585087                    311.667      0.176596 (±4.52%)                       1               0.473782               252.376                   0.086435 (±4.82%)                                    1                            0.967988                            515.632                       0.079098 (±8.51%)                                        1                                1.05778                                 563.461                    0.110089 (±4.59%)                                     1                             0.760003                             404.842
(2816, 2944, 3072)      0.091207 (±12.48%)                558.462        0.100146 (±4.92%)                         1                 0.91074                  508.614           0.166657 (±3.10%)                            1                    0.547274                    305.631      0.170947 (±4.22%)                       1               0.53354                297.962                   0.094895 (±6.59%)                                    1                            0.961136                            536.758                       0.091126 (±7.26%)                                        1                                1.00089                                 558.958                    0.113617 (±7.30%)                                     1                             0.802758                             448.31
(2944, 3072, 3200)       0.113016 (±9.54%)                512.152        0.117185 (±5.88%)                         1                 0.964424                 493.932           0.188425 (±2.43%)                            1                    0.599793                    307.185      0.217091 (±3.93%)                       1               0.520593               266.623                   0.103595 (±9.02%)                                    1                            1.09094                             558.728                      0.113416 (±11.98%)                                        1                                0.996473                                510.346                   0.130375 (±10.36%)                                     1                             0.866853                             443.961
(3072, 3200, 3328)      0.110089 (±17.48%)                594.348        0.174755 (±6.91%)                         1                 0.629962                 374.416           0.194881 (±4.46%)                            1                    0.564904                    335.749      0.209995 (±4.33%)                       1               0.524246               311.584                   0.141320 (±7.83%)                                    1                            0.779005                            463                           0.110731 (±6.30%)                                        1                                0.994202                                590.902                    0.180368 (±9.51%)                                     1                             0.610358                             362.765
(3200, 3328, 3456)       0.138273 (±8.52%)                532.353        0.203220 (±3.08%)                         1                 0.68041                  362.218           0.224508 (±4.98%)                            1                    0.615893                    327.873      0.219937 (±4.52%)                       1               0.628694               334.687                   0.155833 (±7.95%)                                    1                            0.887315                            472.365                      0.138553 (±10.30%)                                        1                                0.997979                                531.277                    0.208152 (±5.10%)                                     1                             0.664289                             353.636
(3328, 3456, 3584)       0.130014 (±5.64%)                634.11         0.197006 (±3.03%)                         1                 0.659949                 418.481           0.228798 (±2.70%)                            1                    0.568248                    360.332      0.237298 (±5.88%)                       1               0.547893               347.425                   0.151263 (±4.93%)                                    1                            0.859523                            545.032                       0.128732 (±5.92%)                                        1                                1.00996                                 640.425                    0.223386 (±4.22%)                                     1                             0.582015                             369.062
(3456, 3584, 3712)       0.158118 (±4.21%)                581.565        0.234211 (±3.47%)                         1                 0.675109                 392.62            0.270372 (±2.37%)                            1                    0.584816                    340.109      0.252813 (±4.46%)                       1               0.625435               363.731                   0.189228 (±5.47%)                                    1                            0.835595                            485.953                      0.160724 (±17.54%)                                        1                                0.983786                                572.136                    0.236776 (±5.43%)                                     1                             0.667796                             388.367
(3584, 3712, 3840)       0.160002 (±5.44%)                638.575        0.231645 (±2.01%)                         1                 0.690721                 441.077           0.263798 (±2.60%)                            1                    0.606532                    387.316      0.258787 (±4.23%)                       1               0.618277               394.816                   0.192436 (±4.29%)                                    1                            0.831456                            530.947                       0.171548 (±5.37%)                                        1                                0.932695                                595.596                    0.244635 (±4.51%)                                     1                             0.654044                             417.656
(3712, 3840, 3968)      0.208593 (±13.36%)                542.302        0.257664 (±3.28%)                         1                 0.809554                 439.023           0.309622 (±5.55%)                            1                    0.673702                    365.35       0.298316 (±4.03%)                       1               0.699235               379.196                  0.199452 (±10.77%)                                    1                            1.04583                             567.156                      0.202900 (±10.85%)                                        1                                1.02806                                 557.518                    0.245235 (±3.91%)                                     1                             0.850584                             461.273
(3840, 3968, 4096)       0.204143 (±7.36%)                611.446        0.252492 (±1.97%)                         1                 0.808513                 494.362           0.311947 (±2.39%)                            1                    0.654416                    400.14      0.464693 (±14.62%)                       1               0.439307               268.613                   0.189709 (±9.59%)                                    1                            1.07609                             657.968                       0.200535 (±5.76%)                                        1                                1.01799                                 622.447                    0.244555 (±4.61%)                                     1                             0.834753                             510.407
(3968, 4096, 4224)       0.245677 (±3.57%)                558.883        0.289577 (±3.42%)                         1                 0.8484                   474.156           0.359255 (±2.86%)                            1                    0.683851                    382.193      0.576828 (±3.38%)                       1               0.42591                238.034                   0.235975 (±8.21%)                                    1                            1.04111                             581.861                       0.242670 (±8.81%)                                        1                                1.01239                                 565.808                    0.313270 (±3.38%)                                     1                             0.784234                             438.295
(4096, 4224, 4352)      0.239503 (±10.18%)                628.77         0.272779 (±1.68%)                         1                 0.878011                 552.067           0.362662 (±3.58%)                            1                    0.660403                    415.241      0.528237 (±4.14%)                       1               0.453401               285.085                  0.234891 (±10.15%)                                    1                            1.01963                             641.116                      0.220780 (±13.66%)                                        1                                1.0848                                  682.092                    0.287171 (±3.73%)                                     1                             0.834008                             524.399
           average     0.15064907532471877                571.899      0.19335192327315992                         1                 0.793351                 451.18           0.2430428438461744                            1                    0.608174                    348.041    0.28920292395811814                       1               0.538799               307.832                  0.1578899988761315                                    1                            0.952118                            542.795                     0.14882330711071307                                        1                                1.01134                                 578.113                  0.20251092314720154                                     1                             0.747154                             425.451INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency
INFO:__main__:ignoring torch_matmul-latency
INFO:__main__:ignoring torch_matmul-tflops
INFO:__main__:ignoring triton_matmul-latency
INFO:__main__:ignoring kernelllm_matmul-latency
INFO:__main__:ignoring mako_matmul-latency
INFO:__main__:ignoring torch_compile_max_matmul-latency
INFO:__main__:ignoring torch_compile_default_matmul-latency
INFO:__main__:ignoring helion_matmul_tritonbench-latency

INFO:tritonbench.utils.run_utils:[tritonbench] Running helion benchmark: /home/hotaisle/myenv/bin/python benchmarks/run.py --device=cuda --precision bf16 --kernel-config ../custom_kernel_config.yaml --kernel bf16_gemm_gelu --output benchmark_bf16_gemm_gelu.json
Loading custom kernel configuration from: ../custom_kernel_config.yaml
Loaded 3 kernel mapping(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Loaded metric mappings for 3 kernel(s): bf16_gemm_gelu, bf16_layernorm, bf16_matmul
Using num_inputs=20 for bf16_gemm_gelu
Running bf16_gemm_gelu benchmark with Helion implementation...

WARNING:tritonbench.utils.triton_op:First-k mode: Selected 5 sequential inputs starting from index 0 (total available: 5)
WARNING:tritonbench.utils.triton_op:Input IDs to run: [0, 1, 2, 3, 4]
  0%|          | 0/5 [00:00<?, ?it/s]WARNING:tritonbench.utils.triton_op:Running input ID 0:
(M, K, N)
------------
(32, 64, 16)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 12, "num_triton_choices": 12, "best_kernel": "triton_mm_2", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2", "best_time": 0.010463999584317207, "best_triton_pos": 0}
AUTOTUNE mm(32x64, 64x16)
strides: [64, 1], [16, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_2 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_0 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=8, num_stages=2, num_warps=1
  triton_mm_11 0.0107 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_8 0.0109 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_7 0.0115 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_5 0.0115 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_6 0.0125 ms 83.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_1 0.0130 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_9 0.0131 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=4, num_stages=2, num_warps=2
  triton_mm_3 0.0134 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 0.0866 seconds and 0.1068 seconds precompiling for 12 choices
INFO:tritonbench.utils.triton_op:Took 1855.37ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 235.08ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.03ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (32, 64),
              'stride': (64, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (64, 16),
              'stride': (16, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (16,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 0.62ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 606477989
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━ 100/100 22.0 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 17.2 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 204.2 configs/s
[18s] Initial random population of 100, 5 starting points: error=18 ok=82 min=0.0072 mid=0.0130 max=0.0173 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=8, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[18s] Generation 1 starting: 176 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 176/176 8.9 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 176/176 15.7 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 113.1         
                                                                  configs/s     
[63s] Generation 1 complete: ok=181 min=0.0062 mid=0.0096 max=0.0167 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[63s] Generation 2 starting: 133 neighbors, 4 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 133/133 11.1 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 133/133 15.9 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 154.9         
                                                                  configs/s     
[94s] Generation 2 complete: ok=137 min=0.0063 mid=0.0068 max=0.0154 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 3], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[94s] Generation 3 starting: 130 neighbors, 4 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 130/130 8.6 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 130/130 15.7 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 160.9         
                                                                  configs/s     
[128s] Generation 3 complete: ok=134 min=0.0063 mid=0.0071 max=0.0151 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[128s] Generation 4 starting: 132 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 132/132 10.4 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 132/132 15.4 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 160.2         
                                                                  configs/s     
[159s] Generation 4 complete: ok=136 min=0.0062 mid=0.0071 max=0.0150 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[159s] Generation 5 starting: 92 neighbors, 3 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92/92 8.8 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 92/92 15.7 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 183.2         
                                                                  configs/s     
[184s] Generation 5 complete: ok=95 min=0.0062 mid=0.0065 max=0.0155 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[184s] Generation 6 starting: 94 neighbors, 3 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 12.0 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 94/94 15.9 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 177.8         
                                                                  configs/s     
[206s] Generation 6 complete: ok=97 min=0.0063 mid=0.0065 max=0.0168 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[206s] Generation 7 starting: 95 neighbors, 3 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 95/95 13.3 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 95/95 16.0 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 178.4         
                                                                  configs/s     
[228s] Generation 7 complete: ok=98 min=0.0063 mid=0.0065 max=0.0153 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=4, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[228s] Generation 8 starting: 92 neighbors, 3 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92/92 9.7 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 92/92 15.2 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 182.9         
                                                                  configs/s     
[252s] Generation 8 complete: ok=95 min=0.0063 mid=0.0065 max=0.0154 best=Config(block_sizes=[1, 1, 16], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[252s] Generation 9 starting: 94 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 8.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 94/94 15.8 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 180.5         
                                                                  configs/s     
[277s] Generation 9 complete: ok=97 min=0.0061 mid=0.0064 max=0.0153 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[277s] Generation 10 starting: 91 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 9.1 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 91/91 15.7 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 186.6         
                                                                  configs/s     
[301s] Generation 10 complete: ok=94 min=0.0061 mid=0.0063 max=0.0153 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[16], load_eviction_policies=['', 'last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[301s] Generation 11 starting: 94 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 8.8 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 94/94 15.8 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 180.1         
                                                                  configs/s     
[326s] Generation 11 complete: ok=97 min=0.0061 mid=0.0064 max=0.0152 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[326s] Generation 12 starting: 97 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97/97 8.9 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 97/97 15.7 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 174.1         
                                                                  configs/s     
[353s] Generation 12 complete: ok=100 min=0.0061 mid=0.0064 max=0.0156 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'last', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[353s] Generation 13 starting: 96 neighbors, 3 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96/96 9.5 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 96/96 15.7 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 174.5         
                                                                  configs/s     
[377s] Generation 13 complete: ok=99 min=0.0061 mid=0.0064 max=0.0160 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first', 'last'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[377s] Generation 14 starting: 96 neighbors, 3 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 96/96 10.2 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 96/96 15.7 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 176.1         
                                                                  configs/s     
[402s] Generation 14 complete: ok=99 min=0.0062 mid=0.0063 max=0.0159 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[402s] Generation 15 starting: 94 neighbors, 3 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94/94 8.7 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 94/94 15.7 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 179.0         
                                                                  configs/s     
[428s] Generation 15 complete: ok=97 min=0.0061 mid=0.0063 max=0.0157 best=Config(block_sizes=[1, 1, 32], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[428s] Generation 16 starting: 96 neighbors, 3 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 96/96 10.2 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 96/96 15.6 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 173.6         
                                                                  configs/s     
[453s] Generation 16 complete: ok=99 min=0.0062 mid=0.0065 max=0.0163 best=Config(block_sizes=[1, 1, 16], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[2], load_eviction_policies=['last', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 1], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[453s] Generation 17 starting: 97 neighbors, 3 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 97/97 11.2 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 97/97 15.8 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 175.3         
                                                                  configs/s     
[476s] Generation 17 complete: ok=100 min=0.0061 mid=0.0064 max=0.0153 best=Config(block_sizes=[1, 1, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[8], load_eviction_policies=['', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[476s] Generation 18 starting: 88 neighbors, 3 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88/88 8.9 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 88/88 15.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 193.8         
                                                                  configs/s     
[500s] Generation 18 complete: ok=91 min=0.0061 mid=0.0064 max=0.0158 best=Config(block_sizes=[1, 1, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[500s] Generation 19 starting: 93 neighbors, 3 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93/93 8.7 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 93/93 15.6 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 179.1         
                                                                  configs/s     
[525s] Generation 19 complete: ok=96 min=0.0061 mid=0.0065 max=0.0171 best=Config(block_sizes=[1, 1, 16], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=2, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 3], range_warp_specializes=[], waves_per_eu=1)
[525s] Generation 20 starting: 91 neighbors, 3 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 91/91 11.5 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 91/91 15.8 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 188.8         
                                                                  configs/s     
[547s] Generation 20 complete: ok=94 min=0.0061 mid=0.0064 max=0.0153 best=Config(block_sizes=[1, 1, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[547s] Autotuning complete in 547.3s after searching 2171 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[1, 1, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
TritonBench accuracy check failed with Helion kernel config: @helion.kernel(config=helion.Config(block_sizes=[1, 1, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[4], load_eviction_policies=['last', 'first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=1, pid_type='xyz', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 3], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
WARNING:tritonbench.utils.triton_op:Completed input ID 0:
(M, K, N)
------------
(32, 64, 16)
 20%|██        | 1/5 [09:18<37:14, 558.56s/it]WARNING:tritonbench.utils.triton_op:Running input ID 1:
(M, K, N)
--------------
(128, 256, 64)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.12ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.06ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 30, "num_triton_choices": 30, "best_kernel": "triton_mm_27", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.011064999736845493, "best_triton_pos": 0}
AUTOTUNE mm(128x256, 256x64)
strides: [256, 1], [64, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_27 0.0111 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_30 0.0111 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_12 0.0113 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=16, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=8, num_stages=2, num_warps=1
  triton_mm_21 0.0113 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_39 0.0113 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_17 0.0114 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_22 0.0114 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_28 0.0114 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_15 0.0115 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_32 0.0117 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3164 seconds and 0.7345 seconds precompiling for 30 choices
INFO:tritonbench.utils.triton_op:Took 2055.34ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 240.02ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (128, 256),
              'stride': (256, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (256, 64),
              'stride': (64, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (64,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 4.88ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 606477989
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Accuracy check failed!
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 5.9 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 20.4 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 717.8 configs/s
[25s] Initial random population of 100, 5 starting points: error=22 ok=78 min=0.0097 mid=0.0167 max=0.9879 best=Config(block_sizes=[1, 32, 256], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[1, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[25s] Generation 1 starting: 230 neighbors, 5 active search path(s)
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: error: 'tt.load' op operation destroyed but still has uses
        a_tile = x_desc.load([offset_0, offset_2])
                            ^
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: note: - use: %97 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x64xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [1], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<256> : tensor<1x64xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x64xi64, #blocked>
    %cst_1 = arith.constant dense<128> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<256> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x64xbf16, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c4096_i32 = arith.constant 4096 : i32
    %cst_5 = arith.constant dense<64> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x1xf32, #blocked1>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x1xf32, #blocked1>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x1xf32, #blocked1>
    %cst_9 = arith.constant dense<64> : tensor<64x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x1xf32, #blocked1>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c256_i32 = arith.constant 256 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c4096_i32 : i32
    %2 = arith.muli %1, %c64_i32 : i32
    %3 = arith.subi %c8_i32, %2 : i32
    %4 = arith.minsi %3, %c64_i32 : i32
    %5 = arith.remsi %0, %c4096_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c16_i32 : i32
    %10 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %11 = tt.splat %9 : i32 -> tensor<16xi32, #blocked2>
    %12 = arith.addi %11, %10 : tensor<16xi32, #blocked2>
    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked2>
    %14 = arith.extsi %9 : i32 to i64
    %15 = tt.splat %14 : i64 -> tensor<16xi64, #blocked2>
    %16 = arith.extsi %10 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2>
    %17 = arith.addi %15, %16 : tensor<16xi64, #blocked2>
    %18 = ttg.convert_layout %17 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %19 = tt.expand_dims %18 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi64, #blocked3>
    %20 = ttg.convert_layout %19 : tensor<16x1xi64, #blocked3> -> tensor<16x1xi64, #blocked1>
    %21 = arith.extsi %13 : tensor<64xi32, #blocked2> to tensor<64xi64, #blocked2>
    %22 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x64x!tt.ptr<bf16>, #blocked>
    %23 = arith.muli %20, %cst_3 : tensor<16x1xi64, #blocked1>
    %24 = tt.broadcast %23 : tensor<16x1xi64, #blocked1> -> tensor<16x64xi64, #blocked1>
    %25 = ttg.convert_layout %24 : tensor<16x64xi64, #blocked1> -> tensor<16x64xi64, #blocked>
    %26 = arith.cmpi sge, %20, %cst_2 : tensor<16x1xi64, #blocked1>
    %27 = arith.cmpi slt, %20, %cst_1 : tensor<16x1xi64, #blocked1>
    %28 = arith.andi %26, %27 : tensor<16x1xi1, #blocked1>
    %29 = tt.broadcast %28 : tensor<16x1xi1, #blocked1> -> tensor<16x64xi1, #blocked1>
    %30 = ttg.convert_layout %29 : tensor<16x64xi1, #blocked1> -> tensor<16x64xi1, #blocked>
    %31 = tt.splat %8 : i32 -> tensor<64x1xi32, #blocked1>
    %32 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>, #blocked1>
    %c128_i32 = arith.constant 128 : i32
    %33 = scf.for %arg4 = %c0_i32 to %c256_i32 step %c128_i32 iter_args(%arg5 = %cst_10) -> (tensor<16x1xf32, #blocked1>)  : i32 {
      %57 = tt.splat %arg4 : i32 -> tensor<64xi32, #blocked2>
      %58 = arith.addi %57, %13 : tensor<64xi32, #blocked2>
      %59 = arith.extsi %arg4 : i32 to i64
      %60 = tt.splat %59 : i64 -> tensor<64xi64, #blocked2>
      %61 = arith.addi %60, %21 : tensor<64xi64, #blocked2>
      %62 = ttg.convert_layout %61 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %63 = tt.expand_dims %62 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %64 = ttg.convert_layout %63 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %65 = tt.broadcast %64 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %66 = arith.addi %25, %65 : tensor<16x64xi64, #blocked>
      %67 = tt.addptr %22, %66 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %68 = arith.cmpi sge, %64, %cst_0 : tensor<1x64xi64, #blocked>
      %69 = arith.cmpi slt, %64, %cst : tensor<1x64xi64, #blocked>
      %70 = arith.andi %68, %69 : tensor<1x64xi1, #blocked>
      %71 = tt.broadcast %70 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %72 = arith.andi %30, %71 : tensor<16x64xi1, #blocked>
      %73 = tt.load %67, %72, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %74 = ttg.convert_layout %58 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %75 = tt.expand_dims %74 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %76 = ttg.convert_layout %75 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %77 = arith.muli %76, %cst_9 : tensor<64x1xi32, #blocked1>
      %78 = arith.addi %77, %31 : tensor<64x1xi32, #blocked1>
      %79 = tt.addptr %32, %78 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %80 = tt.load %79 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %81 = ttg.convert_layout %73 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %82 = ttg.convert_layout %80 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %83 = ttg.convert_layout %arg5 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %84 = tt.dot %81, %82, %83, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      %c1_i32 = arith.constant 1 : i32
      %85 = arith.muli %c64_i32, %c1_i32 : i32
      %86 = arith.addi %arg4, %85 : i32
      %87 = tt.splat %86 : i32 -> tensor<64xi32, #blocked2>
      %88 = arith.addi %87, %13 : tensor<64xi32, #blocked2>
      %89 = arith.extsi %86 : i32 to i64
      %90 = tt.splat %89 : i64 -> tensor<64xi64, #blocked2>
      %91 = arith.addi %90, %21 : tensor<64xi64, #blocked2>
      %92 = ttg.convert_layout %91 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %95 = tt.broadcast %94 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %96 = arith.addi %25, %95 : tensor<16x64xi64, #blocked>
      %97 = tt.addptr %22, %96 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %98 = arith.cmpi sge, %94, %cst_0 : tensor<1x64xi64, #blocked>
      %99 = arith.cmpi slt, %94, %cst : tensor<1x64xi64, #blocked>
      %100 = arith.andi %98, %99 : tensor<1x64xi1, #blocked>
      %101 = tt.broadcast %100 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %102 = arith.andi %30, %101 : tensor<16x64xi1, #blocked>
      %103 = tt.load %97, %102, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %104 = ttg.convert_layout %88 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %105 = tt.expand_dims %104 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %107 = arith.muli %106, %cst_9 : tensor<64x1xi32, #blocked1>
      %108 = arith.addi %107, %31 : tensor<64x1xi32, #blocked1>
      %109 = tt.addptr %32, %108 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %110 = tt.load %109 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %111 = ttg.convert_layout %103 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %112 = ttg.convert_layout %110 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %113 = ttg.convert_layout %84 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %114 = tt.dot %111, %112, %113, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      scf.yield %114 : tensor<16x1xf32, #blocked1>
    } {tt.disallow_acc_multi_buffer}
    %34 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %35 = tt.splat %34 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked2>
    %36 = tt.load %35 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked2>
    %37 = ttg.convert_layout %36 : tensor<1xbf16, #blocked2> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %38 = tt.expand_dims %37 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %39 = ttg.convert_layout %38 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked1>
    %40 = arith.extf %39 : tensor<1x1xbf16, #blocked1> to tensor<1x1xf32, #blocked1>
    %41 = tt.broadcast %40 : tensor<1x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
    %42 = arith.addf %33, %41 : tensor<16x1xf32, #blocked1>
    %43 = arith.mulf %42, %cst_8 : tensor<16x1xf32, #blocked1>
    %44 = arith.mulf %42, %cst_7 : tensor<16x1xf32, #blocked1>
    %45 = tt.extern_elementwise %44 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked1>
    %46 = arith.addf %45, %cst_6 : tensor<16x1xf32, #blocked1>
    %47 = arith.mulf %43, %46 : tensor<16x1xf32, #blocked1>
    %48 = arith.truncf %47 : tensor<16x1xf32, #blocked1> to tensor<16x1xbf16, #blocked1>
    %49 = ttg.convert_layout %12 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %50 = tt.expand_dims %49 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
    %51 = ttg.convert_layout %50 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked1>
    %52 = arith.muli %51, %cst_5 : tensor<16x1xi32, #blocked1>
    %53 = tt.splat %8 : i32 -> tensor<16x1xi32, #blocked1>
    %54 = arith.addi %52, %53 : tensor<16x1xi32, #blocked1>
    %55 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x1x!tt.ptr<bf16>, #blocked1>
    %56 = tt.addptr %55, %54 : tensor<16x1x!tt.ptr<bf16>, #blocked1>, tensor<16x1xi32, #blocked1>
    tt.store %56, %48 : tensor<16x1x!tt.ptr<bf16>, #blocked1>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=4}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 1, 64], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 230/230 24.2 configs/s
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: error: 'tt.load' op operation destroyed but still has uses
        a_tile = x_desc.load([offset_0, offset_2])
                            ^
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: note: - use: %97 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x64xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [1], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<256> : tensor<1x64xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x64xi64, #blocked>
    %cst_1 = arith.constant dense<128> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<256> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x64xbf16, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c4096_i32 = arith.constant 4096 : i32
    %cst_5 = arith.constant dense<64> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x1xf32, #blocked1>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x1xf32, #blocked1>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x1xf32, #blocked1>
    %cst_9 = arith.constant dense<64> : tensor<64x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x1xf32, #blocked1>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c256_i32 = arith.constant 256 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c4096_i32 : i32
    %2 = arith.muli %1, %c64_i32 : i32
    %3 = arith.subi %c8_i32, %2 : i32
    %4 = arith.minsi %3, %c64_i32 : i32
    %5 = arith.remsi %0, %c4096_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c16_i32 : i32
    %10 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %11 = tt.splat %9 : i32 -> tensor<16xi32, #blocked2>
    %12 = arith.addi %11, %10 : tensor<16xi32, #blocked2>
    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked2>
    %14 = arith.extsi %9 : i32 to i64
    %15 = tt.splat %14 : i64 -> tensor<16xi64, #blocked2>
    %16 = arith.extsi %10 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2>
    %17 = arith.addi %15, %16 : tensor<16xi64, #blocked2>
    %18 = ttg.convert_layout %17 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %19 = tt.expand_dims %18 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi64, #blocked3>
    %20 = ttg.convert_layout %19 : tensor<16x1xi64, #blocked3> -> tensor<16x1xi64, #blocked1>
    %21 = arith.extsi %13 : tensor<64xi32, #blocked2> to tensor<64xi64, #blocked2>
    %22 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x64x!tt.ptr<bf16>, #blocked>
    %23 = arith.muli %20, %cst_3 : tensor<16x1xi64, #blocked1>
    %24 = tt.broadcast %23 : tensor<16x1xi64, #blocked1> -> tensor<16x64xi64, #blocked1>
    %25 = ttg.convert_layout %24 : tensor<16x64xi64, #blocked1> -> tensor<16x64xi64, #blocked>
    %26 = arith.cmpi sge, %20, %cst_2 : tensor<16x1xi64, #blocked1>
    %27 = arith.cmpi slt, %20, %cst_1 : tensor<16x1xi64, #blocked1>
    %28 = arith.andi %26, %27 : tensor<16x1xi1, #blocked1>
    %29 = tt.broadcast %28 : tensor<16x1xi1, #blocked1> -> tensor<16x64xi1, #blocked1>
    %30 = ttg.convert_layout %29 : tensor<16x64xi1, #blocked1> -> tensor<16x64xi1, #blocked>
    %31 = tt.splat %8 : i32 -> tensor<64x1xi32, #blocked1>
    %32 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>, #blocked1>
    %c128_i32 = arith.constant 128 : i32
    %33 = scf.for %arg4 = %c0_i32 to %c256_i32 step %c128_i32 iter_args(%arg5 = %cst_10) -> (tensor<16x1xf32, #blocked1>)  : i32 {
      %57 = tt.splat %arg4 : i32 -> tensor<64xi32, #blocked2>
      %58 = arith.addi %57, %13 : tensor<64xi32, #blocked2>
      %59 = arith.extsi %arg4 : i32 to i64
      %60 = tt.splat %59 : i64 -> tensor<64xi64, #blocked2>
      %61 = arith.addi %60, %21 : tensor<64xi64, #blocked2>
      %62 = ttg.convert_layout %61 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %63 = tt.expand_dims %62 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %64 = ttg.convert_layout %63 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %65 = tt.broadcast %64 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %66 = arith.addi %25, %65 : tensor<16x64xi64, #blocked>
      %67 = tt.addptr %22, %66 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %68 = arith.cmpi sge, %64, %cst_0 : tensor<1x64xi64, #blocked>
      %69 = arith.cmpi slt, %64, %cst : tensor<1x64xi64, #blocked>
      %70 = arith.andi %68, %69 : tensor<1x64xi1, #blocked>
      %71 = tt.broadcast %70 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %72 = arith.andi %30, %71 : tensor<16x64xi1, #blocked>
      %73 = tt.load %67, %72, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %74 = ttg.convert_layout %58 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %75 = tt.expand_dims %74 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %76 = ttg.convert_layout %75 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %77 = arith.muli %76, %cst_9 : tensor<64x1xi32, #blocked1>
      %78 = arith.addi %77, %31 : tensor<64x1xi32, #blocked1>
      %79 = tt.addptr %32, %78 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %80 = tt.load %79 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %81 = ttg.convert_layout %73 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %82 = ttg.convert_layout %80 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %83 = ttg.convert_layout %arg5 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %84 = tt.dot %81, %82, %83, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      %c1_i32 = arith.constant 1 : i32
      %85 = arith.muli %c64_i32, %c1_i32 : i32
      %86 = arith.addi %arg4, %85 : i32
      %87 = tt.splat %86 : i32 -> tensor<64xi32, #blocked2>
      %88 = arith.addi %87, %13 : tensor<64xi32, #blocked2>
      %89 = arith.extsi %86 : i32 to i64
      %90 = tt.splat %89 : i64 -> tensor<64xi64, #blocked2>
      %91 = arith.addi %90, %21 : tensor<64xi64, #blocked2>
      %92 = ttg.convert_layout %91 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %95 = tt.broadcast %94 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %96 = arith.addi %25, %95 : tensor<16x64xi64, #blocked>
      %97 = tt.addptr %22, %96 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %98 = arith.cmpi sge, %94, %cst_0 : tensor<1x64xi64, #blocked>
      %99 = arith.cmpi slt, %94, %cst : tensor<1x64xi64, #blocked>
      %100 = arith.andi %98, %99 : tensor<1x64xi1, #blocked>
      %101 = tt.broadcast %100 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %102 = arith.andi %30, %101 : tensor<16x64xi1, #blocked>
      %103 = tt.load %97, %102, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %104 = ttg.convert_layout %88 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %105 = tt.expand_dims %104 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %107 = arith.muli %106, %cst_9 : tensor<64x1xi32, #blocked1>
      %108 = arith.addi %107, %31 : tensor<64x1xi32, #blocked1>
      %109 = tt.addptr %32, %108 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %110 = tt.load %109 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %111 = ttg.convert_layout %103 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %112 = ttg.convert_layout %110 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %113 = ttg.convert_layout %84 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %114 = tt.dot %111, %112, %113, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      scf.yield %114 : tensor<16x1xf32, #blocked1>
    } {tt.disallow_acc_multi_buffer}
    %34 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %35 = tt.splat %34 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked2>
    %36 = tt.load %35 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked2>
    %37 = ttg.convert_layout %36 : tensor<1xbf16, #blocked2> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %38 = tt.expand_dims %37 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %39 = ttg.convert_layout %38 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked1>
    %40 = arith.extf %39 : tensor<1x1xbf16, #blocked1> to tensor<1x1xf32, #blocked1>
    %41 = tt.broadcast %40 : tensor<1x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
    %42 = arith.addf %33, %41 : tensor<16x1xf32, #blocked1>
    %43 = arith.mulf %42, %cst_8 : tensor<16x1xf32, #blocked1>
    %44 = arith.mulf %42, %cst_7 : tensor<16x1xf32, #blocked1>
    %45 = tt.extern_elementwise %44 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked1>
    %46 = arith.addf %45, %cst_6 : tensor<16x1xf32, #blocked1>
    %47 = arith.mulf %43, %46 : tensor<16x1xf32, #blocked1>
    %48 = arith.truncf %47 : tensor<16x1xf32, #blocked1> to tensor<16x1xbf16, #blocked1>
    %49 = ttg.convert_layout %12 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %50 = tt.expand_dims %49 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
    %51 = ttg.convert_layout %50 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked1>
    %52 = arith.muli %51, %cst_5 : tensor<16x1xi32, #blocked1>
    %53 = tt.splat %8 : i32 -> tensor<16x1xi32, #blocked1>
    %54 = arith.addi %52, %53 : tensor<16x1xi32, #blocked1>
    %55 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x1x!tt.ptr<bf16>, #blocked1>
    %56 = tt.addptr %55, %54 : tensor<16x1x!tt.ptr<bf16>, #blocked1>, tensor<16x1xi32, #blocked1>
    tt.store %56, %48 : tensor<16x1x!tt.ptr<bf16>, #blocked1>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=4}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[50s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 1, 64], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['first', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 230/230 15.5 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 80.5 configs/s
[69s] Generation 1 complete: error=3 ok=232 min=0.0096 mid=0.0111 max=0.0550 best=Config(block_sizes=[1, 32, 256], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[1, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[69s] Generation 2 starting: 180 neighbors, 4 active search path(s)
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: error: 'tt.load' op operation destroyed but still has uses
        a_tile = x_desc.load([offset_0, offset_2])
                            ^
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: note: - use: %97 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x64xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [1], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<256> : tensor<1x64xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x64xi64, #blocked>
    %cst_1 = arith.constant dense<128> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<256> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x64xbf16, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c4096_i32 = arith.constant 4096 : i32
    %cst_5 = arith.constant dense<64> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x1xf32, #blocked1>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x1xf32, #blocked1>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x1xf32, #blocked1>
    %cst_9 = arith.constant dense<64> : tensor<64x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x1xf32, #blocked1>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c256_i32 = arith.constant 256 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c4096_i32 : i32
    %2 = arith.muli %1, %c64_i32 : i32
    %3 = arith.subi %c8_i32, %2 : i32
    %4 = arith.minsi %3, %c64_i32 : i32
    %5 = arith.remsi %0, %c4096_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c16_i32 : i32
    %10 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %11 = tt.splat %9 : i32 -> tensor<16xi32, #blocked2>
    %12 = arith.addi %11, %10 : tensor<16xi32, #blocked2>
    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked2>
    %14 = arith.extsi %9 : i32 to i64
    %15 = tt.splat %14 : i64 -> tensor<16xi64, #blocked2>
    %16 = arith.extsi %10 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2>
    %17 = arith.addi %15, %16 : tensor<16xi64, #blocked2>
    %18 = ttg.convert_layout %17 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %19 = tt.expand_dims %18 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi64, #blocked3>
    %20 = ttg.convert_layout %19 : tensor<16x1xi64, #blocked3> -> tensor<16x1xi64, #blocked1>
    %21 = arith.extsi %13 : tensor<64xi32, #blocked2> to tensor<64xi64, #blocked2>
    %22 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x64x!tt.ptr<bf16>, #blocked>
    %23 = arith.muli %20, %cst_3 : tensor<16x1xi64, #blocked1>
    %24 = tt.broadcast %23 : tensor<16x1xi64, #blocked1> -> tensor<16x64xi64, #blocked1>
    %25 = ttg.convert_layout %24 : tensor<16x64xi64, #blocked1> -> tensor<16x64xi64, #blocked>
    %26 = arith.cmpi sge, %20, %cst_2 : tensor<16x1xi64, #blocked1>
    %27 = arith.cmpi slt, %20, %cst_1 : tensor<16x1xi64, #blocked1>
    %28 = arith.andi %26, %27 : tensor<16x1xi1, #blocked1>
    %29 = tt.broadcast %28 : tensor<16x1xi1, #blocked1> -> tensor<16x64xi1, #blocked1>
    %30 = ttg.convert_layout %29 : tensor<16x64xi1, #blocked1> -> tensor<16x64xi1, #blocked>
    %31 = tt.splat %8 : i32 -> tensor<64x1xi32, #blocked1>
    %32 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>, #blocked1>
    %c128_i32 = arith.constant 128 : i32
    %33 = scf.for %arg4 = %c0_i32 to %c256_i32 step %c128_i32 iter_args(%arg5 = %cst_10) -> (tensor<16x1xf32, #blocked1>)  : i32 {
      %57 = tt.splat %arg4 : i32 -> tensor<64xi32, #blocked2>
      %58 = arith.addi %57, %13 : tensor<64xi32, #blocked2>
      %59 = arith.extsi %arg4 : i32 to i64
      %60 = tt.splat %59 : i64 -> tensor<64xi64, #blocked2>
      %61 = arith.addi %60, %21 : tensor<64xi64, #blocked2>
      %62 = ttg.convert_layout %61 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %63 = tt.expand_dims %62 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %64 = ttg.convert_layout %63 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %65 = tt.broadcast %64 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %66 = arith.addi %25, %65 : tensor<16x64xi64, #blocked>
      %67 = tt.addptr %22, %66 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %68 = arith.cmpi sge, %64, %cst_0 : tensor<1x64xi64, #blocked>
      %69 = arith.cmpi slt, %64, %cst : tensor<1x64xi64, #blocked>
      %70 = arith.andi %68, %69 : tensor<1x64xi1, #blocked>
      %71 = tt.broadcast %70 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %72 = arith.andi %30, %71 : tensor<16x64xi1, #blocked>
      %73 = tt.load %67, %72, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %74 = ttg.convert_layout %58 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %75 = tt.expand_dims %74 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %76 = ttg.convert_layout %75 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %77 = arith.muli %76, %cst_9 : tensor<64x1xi32, #blocked1>
      %78 = arith.addi %77, %31 : tensor<64x1xi32, #blocked1>
      %79 = tt.addptr %32, %78 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %80 = tt.load %79 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %81 = ttg.convert_layout %73 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %82 = ttg.convert_layout %80 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %83 = ttg.convert_layout %arg5 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %84 = tt.dot %81, %82, %83, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      %c1_i32 = arith.constant 1 : i32
      %85 = arith.muli %c64_i32, %c1_i32 : i32
      %86 = arith.addi %arg4, %85 : i32
      %87 = tt.splat %86 : i32 -> tensor<64xi32, #blocked2>
      %88 = arith.addi %87, %13 : tensor<64xi32, #blocked2>
      %89 = arith.extsi %86 : i32 to i64
      %90 = tt.splat %89 : i64 -> tensor<64xi64, #blocked2>
      %91 = arith.addi %90, %21 : tensor<64xi64, #blocked2>
      %92 = ttg.convert_layout %91 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %95 = tt.broadcast %94 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %96 = arith.addi %25, %95 : tensor<16x64xi64, #blocked>
      %97 = tt.addptr %22, %96 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %98 = arith.cmpi sge, %94, %cst_0 : tensor<1x64xi64, #blocked>
      %99 = arith.cmpi slt, %94, %cst : tensor<1x64xi64, #blocked>
      %100 = arith.andi %98, %99 : tensor<1x64xi1, #blocked>
      %101 = tt.broadcast %100 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %102 = arith.andi %30, %101 : tensor<16x64xi1, #blocked>
      %103 = tt.load %97, %102, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %104 = ttg.convert_layout %88 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %105 = tt.expand_dims %104 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %107 = arith.muli %106, %cst_9 : tensor<64x1xi32, #blocked1>
      %108 = arith.addi %107, %31 : tensor<64x1xi32, #blocked1>
      %109 = tt.addptr %32, %108 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %110 = tt.load %109 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %111 = ttg.convert_layout %103 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %112 = ttg.convert_layout %110 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %113 = ttg.convert_layout %84 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %114 = tt.dot %111, %112, %113, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      scf.yield %114 : tensor<16x1xf32, #blocked1>
    } {tt.disallow_acc_multi_buffer}
    %34 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %35 = tt.splat %34 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked2>
    %36 = tt.load %35 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked2>
    %37 = ttg.convert_layout %36 : tensor<1xbf16, #blocked2> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %38 = tt.expand_dims %37 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %39 = ttg.convert_layout %38 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked1>
    %40 = arith.extf %39 : tensor<1x1xbf16, #blocked1> to tensor<1x1xf32, #blocked1>
    %41 = tt.broadcast %40 : tensor<1x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
    %42 = arith.addf %33, %41 : tensor<16x1xf32, #blocked1>
    %43 = arith.mulf %42, %cst_8 : tensor<16x1xf32, #blocked1>
    %44 = arith.mulf %42, %cst_7 : tensor<16x1xf32, #blocked1>
    %45 = tt.extern_elementwise %44 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked1>
    %46 = arith.addf %45, %cst_6 : tensor<16x1xf32, #blocked1>
    %47 = arith.mulf %43, %46 : tensor<16x1xf32, #blocked1>
    %48 = arith.truncf %47 : tensor<16x1xf32, #blocked1> to tensor<16x1xbf16, #blocked1>
    %49 = ttg.convert_layout %12 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %50 = tt.expand_dims %49 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
    %51 = ttg.convert_layout %50 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked1>
    %52 = arith.muli %51, %cst_5 : tensor<16x1xi32, #blocked1>
    %53 = tt.splat %8 : i32 -> tensor<16x1xi32, #blocked1>
    %54 = arith.addi %52, %53 : tensor<16x1xi32, #blocked1>
    %55 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x1x!tt.ptr<bf16>, #blocked1>
    %56 = tt.addptr %55, %54 : tensor<16x1x!tt.ptr<bf16>, #blocked1>, tensor<16x1xi32, #blocked1>
    tt.store %56, %48 : tensor<16x1x!tt.ptr<bf16>, #blocked1>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=4}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 1, 64], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 180/180 32.4 configs/s
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: error: 'tt.load' op operation destroyed but still has uses
        a_tile = x_desc.load([offset_0, offset_2])
                            ^
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:46:29: note: - use: %97 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x64xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [8, 8], warpsPerCTA = [1, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [1], order = [0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [1, 1], order = [0, 1]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 1], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 1 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<256> : tensor<1x64xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x64xi64, #blocked>
    %cst_1 = arith.constant dense<128> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<256> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x64xbf16, #blocked>
    %c8_i32 = arith.constant 8 : i32
    %c0_i32 = arith.constant 0 : i32
    %c4096_i32 = arith.constant 4096 : i32
    %cst_5 = arith.constant dense<64> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x1xf32, #blocked1>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x1xf32, #blocked1>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x1xf32, #blocked1>
    %cst_9 = arith.constant dense<64> : tensor<64x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x1xf32, #blocked1>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c256_i32 = arith.constant 256 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.divsi %0, %c4096_i32 : i32
    %2 = arith.muli %1, %c64_i32 : i32
    %3 = arith.subi %c8_i32, %2 : i32
    %4 = arith.minsi %3, %c64_i32 : i32
    %5 = arith.remsi %0, %c4096_i32 : i32
    %6 = arith.remsi %5, %4 : i32
    %7 = arith.addi %2, %6 : i32
    %8 = arith.divsi %5, %4 : i32
    %9 = arith.muli %7, %c16_i32 : i32
    %10 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>
    %11 = tt.splat %9 : i32 -> tensor<16xi32, #blocked2>
    %12 = arith.addi %11, %10 : tensor<16xi32, #blocked2>
    %13 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked2>
    %14 = arith.extsi %9 : i32 to i64
    %15 = tt.splat %14 : i64 -> tensor<16xi64, #blocked2>
    %16 = arith.extsi %10 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2>
    %17 = arith.addi %15, %16 : tensor<16xi64, #blocked2>
    %18 = ttg.convert_layout %17 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %19 = tt.expand_dims %18 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi64, #blocked3>
    %20 = ttg.convert_layout %19 : tensor<16x1xi64, #blocked3> -> tensor<16x1xi64, #blocked1>
    %21 = arith.extsi %13 : tensor<64xi32, #blocked2> to tensor<64xi64, #blocked2>
    %22 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x64x!tt.ptr<bf16>, #blocked>
    %23 = arith.muli %20, %cst_3 : tensor<16x1xi64, #blocked1>
    %24 = tt.broadcast %23 : tensor<16x1xi64, #blocked1> -> tensor<16x64xi64, #blocked1>
    %25 = ttg.convert_layout %24 : tensor<16x64xi64, #blocked1> -> tensor<16x64xi64, #blocked>
    %26 = arith.cmpi sge, %20, %cst_2 : tensor<16x1xi64, #blocked1>
    %27 = arith.cmpi slt, %20, %cst_1 : tensor<16x1xi64, #blocked1>
    %28 = arith.andi %26, %27 : tensor<16x1xi1, #blocked1>
    %29 = tt.broadcast %28 : tensor<16x1xi1, #blocked1> -> tensor<16x64xi1, #blocked1>
    %30 = ttg.convert_layout %29 : tensor<16x64xi1, #blocked1> -> tensor<16x64xi1, #blocked>
    %31 = tt.splat %8 : i32 -> tensor<64x1xi32, #blocked1>
    %32 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<64x1x!tt.ptr<bf16>, #blocked1>
    %c128_i32 = arith.constant 128 : i32
    %33 = scf.for %arg4 = %c0_i32 to %c256_i32 step %c128_i32 iter_args(%arg5 = %cst_10) -> (tensor<16x1xf32, #blocked1>)  : i32 {
      %57 = tt.splat %arg4 : i32 -> tensor<64xi32, #blocked2>
      %58 = arith.addi %57, %13 : tensor<64xi32, #blocked2>
      %59 = arith.extsi %arg4 : i32 to i64
      %60 = tt.splat %59 : i64 -> tensor<64xi64, #blocked2>
      %61 = arith.addi %60, %21 : tensor<64xi64, #blocked2>
      %62 = ttg.convert_layout %61 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %63 = tt.expand_dims %62 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %64 = ttg.convert_layout %63 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %65 = tt.broadcast %64 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %66 = arith.addi %25, %65 : tensor<16x64xi64, #blocked>
      %67 = tt.addptr %22, %66 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %68 = arith.cmpi sge, %64, %cst_0 : tensor<1x64xi64, #blocked>
      %69 = arith.cmpi slt, %64, %cst : tensor<1x64xi64, #blocked>
      %70 = arith.andi %68, %69 : tensor<1x64xi1, #blocked>
      %71 = tt.broadcast %70 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %72 = arith.andi %30, %71 : tensor<16x64xi1, #blocked>
      %73 = tt.load %67, %72, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %74 = ttg.convert_layout %58 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %75 = tt.expand_dims %74 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %76 = ttg.convert_layout %75 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %77 = arith.muli %76, %cst_9 : tensor<64x1xi32, #blocked1>
      %78 = arith.addi %77, %31 : tensor<64x1xi32, #blocked1>
      %79 = tt.addptr %32, %78 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %80 = tt.load %79 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %81 = ttg.convert_layout %73 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %82 = ttg.convert_layout %80 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %83 = ttg.convert_layout %arg5 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %84 = tt.dot %81, %82, %83, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      %c1_i32 = arith.constant 1 : i32
      %85 = arith.muli %c64_i32, %c1_i32 : i32
      %86 = arith.addi %arg4, %85 : i32
      %87 = tt.splat %86 : i32 -> tensor<64xi32, #blocked2>
      %88 = arith.addi %87, %13 : tensor<64xi32, #blocked2>
      %89 = arith.extsi %86 : i32 to i64
      %90 = tt.splat %89 : i64 -> tensor<64xi64, #blocked2>
      %91 = arith.addi %90, %21 : tensor<64xi64, #blocked2>
      %92 = ttg.convert_layout %91 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x64xi64, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<1x64xi64, #blocked4> -> tensor<1x64xi64, #blocked>
      %95 = tt.broadcast %94 : tensor<1x64xi64, #blocked> -> tensor<16x64xi64, #blocked>
      %96 = arith.addi %25, %95 : tensor<16x64xi64, #blocked>
      %97 = tt.addptr %22, %96 : tensor<16x64x!tt.ptr<bf16>, #blocked>, tensor<16x64xi64, #blocked>
      %98 = arith.cmpi sge, %94, %cst_0 : tensor<1x64xi64, #blocked>
      %99 = arith.cmpi slt, %94, %cst : tensor<1x64xi64, #blocked>
      %100 = arith.andi %98, %99 : tensor<1x64xi1, #blocked>
      %101 = tt.broadcast %100 : tensor<1x64xi1, #blocked> -> tensor<16x64xi1, #blocked>
      %102 = arith.andi %30, %101 : tensor<16x64xi1, #blocked>
      %103 = tt.load %97, %102, %cst_4 : tensor<16x64x!tt.ptr<bf16>, #blocked>
      %104 = ttg.convert_layout %88 : tensor<64xi32, #blocked2> -> tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
      %105 = tt.expand_dims %104 {axis = 1 : i32} : tensor<64xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<64x1xi32, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<64x1xi32, #blocked3> -> tensor<64x1xi32, #blocked1>
      %107 = arith.muli %106, %cst_9 : tensor<64x1xi32, #blocked1>
      %108 = arith.addi %107, %31 : tensor<64x1xi32, #blocked1>
      %109 = tt.addptr %32, %108 : tensor<64x1x!tt.ptr<bf16>, #blocked1>, tensor<64x1xi32, #blocked1>
      %110 = tt.load %109 : tensor<64x1x!tt.ptr<bf16>, #blocked1>
      %111 = ttg.convert_layout %103 : tensor<16x64xbf16, #blocked> -> tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>>
      %112 = ttg.convert_layout %110 : tensor<64x1xbf16, #blocked1> -> tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>>
      %113 = ttg.convert_layout %84 : tensor<16x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
      %114 = tt.dot %111, %112, %113, inputPrecision = tf32 : tensor<16x64xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked1}>> * tensor<64x1xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked1}>> -> tensor<16x1xf32, #blocked1>
      scf.yield %114 : tensor<16x1xf32, #blocked1>
    } {tt.disallow_acc_multi_buffer}
    %34 = tt.addptr %arg2, %8 : !tt.ptr<bf16>, i32
    %35 = tt.splat %34 : !tt.ptr<bf16> -> tensor<1x!tt.ptr<bf16>, #blocked2>
    %36 = tt.load %35 evictionPolicy = evict_first : tensor<1x!tt.ptr<bf16>, #blocked2>
    %37 = ttg.convert_layout %36 : tensor<1xbf16, #blocked2> -> tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>>
    %38 = tt.expand_dims %37 {axis = 0 : i32} : tensor<1xbf16, #ttg.slice<{dim = 0, parent = #blocked4}>> -> tensor<1x1xbf16, #blocked4>
    %39 = ttg.convert_layout %38 : tensor<1x1xbf16, #blocked4> -> tensor<1x1xbf16, #blocked1>
    %40 = arith.extf %39 : tensor<1x1xbf16, #blocked1> to tensor<1x1xf32, #blocked1>
    %41 = tt.broadcast %40 : tensor<1x1xf32, #blocked1> -> tensor<16x1xf32, #blocked1>
    %42 = arith.addf %33, %41 : tensor<16x1xf32, #blocked1>
    %43 = arith.mulf %42, %cst_8 : tensor<16x1xf32, #blocked1>
    %44 = arith.mulf %42, %cst_7 : tensor<16x1xf32, #blocked1>
    %45 = tt.extern_elementwise %44 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x1xf32, #blocked1>) -> tensor<16x1xf32, #blocked1>
    %46 = arith.addf %45, %cst_6 : tensor<16x1xf32, #blocked1>
    %47 = arith.mulf %43, %46 : tensor<16x1xf32, #blocked1>
    %48 = arith.truncf %47 : tensor<16x1xf32, #blocked1> to tensor<16x1xbf16, #blocked1>
    %49 = ttg.convert_layout %12 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>>
    %50 = tt.expand_dims %49 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16x1xi32, #blocked3>
    %51 = ttg.convert_layout %50 : tensor<16x1xi32, #blocked3> -> tensor<16x1xi32, #blocked1>
    %52 = arith.muli %51, %cst_5 : tensor<16x1xi32, #blocked1>
    %53 = tt.splat %8 : i32 -> tensor<16x1xi32, #blocked1>
    %54 = arith.addi %52, %53 : tensor<16x1xi32, #blocked1>
    %55 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x1x!tt.ptr<bf16>, #blocked1>
    %56 = tt.addptr %55, %54 : tensor<16x1x!tt.ptr<bf16>, #blocked1>, tensor<16x1xi32, #blocked1>
    tt.store %56, %48 : tensor<16x1x!tt.ptr<bf16>, #blocked1>
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=4}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/li/cli6px2ovpjarur5al6b7ppqpy5sld3ajd6as4a6sbd3isfi3fwu.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[88s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 1, 64], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, False], range_num_stages=[0, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 180/180 15.7 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 109.9         
                                                                  configs/s     
[100s] Generation 2 complete: error=1 ok=183 min=0.0097 mid=0.0108 max=0.0374 best=Config(block_sizes=[1, 32, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[1, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[100s] Generation 3 starting: 173 neighbors, 4 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 173/173 28.4 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 173/173 15.3 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 155.7         
                                                                  configs/s     
[128s] Generation 3 complete: ok=177 min=0.0090 mid=0.0103 max=0.0374 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[1, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[128s] Generation 4 starting: 174 neighbors, 4 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 27.9 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 174/174 15.6 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 118.1         
                                                                  configs/s     
[159s] Generation 4 complete: ok=178 min=0.0095 mid=0.0105 max=0.0375 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[1, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[159s] Generation 5 starting: 175 neighbors, 4 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 28.1 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 175/175 15.7 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 120.8         
                                                                  configs/s     
[190s] Generation 5 complete: ok=179 min=0.0093 mid=0.0104 max=0.0374 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[2, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[190s] Generation 6 starting: 182 neighbors, 4 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 182/182 19.5 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 182/182 15.7 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 106.5         
                                                                  configs/s     
[225s] Generation 6 complete: ok=186 min=0.0100 mid=0.0117 max=0.0219 best=Config(block_sizes=[32, 1, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, False], range_num_stages=[1, 1], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[225s] Generation 7 starting: 174 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 18.6 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 174/174 15.2 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 204.4         
                                                                  configs/s     
[255s] Generation 7 complete: ok=178 min=0.0094 mid=0.0104 max=0.0216 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[2, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[255s] Generation 8 starting: 177 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 177/177 31.8 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 177/177 15.4 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 205.4         
                                                                  configs/s     
[281s] Generation 8 complete: ok=181 min=0.0094 mid=0.0103 max=0.0223 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[3, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[281s] Generation 9 starting: 82 neighbors, 2 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82/82 12.8 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━━ 82/82 14.6 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 295.6         
                                                                  configs/s     
[298s] Generation 9 complete: ok=84 min=0.0088 mid=0.0101 max=0.0218 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[3, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[298s] Generation 10 starting: 81 neighbors, 2 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 13.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 81/81 15.4 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 223.7         
                                                                  configs/s     
[316s] Generation 10 complete: ok=83 min=0.0091 mid=0.0101 max=0.0235 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 1], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[316s] Generation 11 starting: 86 neighbors, 2 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 86/86 33.3 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 86/86 15.7 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 219.3         
                                                                  configs/s     
[331s] Generation 11 complete: ok=88 min=0.0094 mid=0.0104 max=0.0238 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 1], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[331s] Generation 12 starting: 87 neighbors, 2 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/87 33.9 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 87/87 15.8 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 217.6         
                                                                  configs/s     
[346s] Generation 12 complete: ok=89 min=0.0089 mid=0.0102 max=0.0238 best=Config(block_sizes=[1, 32, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[1], load_eviction_policies=['', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[346s] Generation 13 starting: 89 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 89/89 33.0 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 89/89 14.8 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 210.5         
                                                                  configs/s     
[362s] Generation 13 complete: ok=91 min=0.0092 mid=0.0105 max=0.0239 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 2], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[362s] Generation 14 starting: 86 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 86/86 29.4 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 86/86 15.7 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 221.8         
                                                                  configs/s     
[378s] Generation 14 complete: ok=88 min=0.0094 mid=0.0107 max=0.0239 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[378s] Generation 15 starting: 85 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 85/85 29.0 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 85/85 15.7 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 225.0         
                                                                  configs/s     
[393s] Generation 15 complete: ok=87 min=0.0089 mid=0.0100 max=0.0238 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[393s] Generation 16 starting: 89 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 89/89 23.4 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 89/89 15.6 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 211.8         
                                                                  configs/s     
[410s] Generation 16 complete: ok=91 min=0.0089 mid=0.0101 max=0.0237 best=Config(block_sizes=[1, 32, 256], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[3, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[410s] Generation 17 starting: 87 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 87/87 32.3 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 87/87 15.8 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 215.9         
                                                                  configs/s     
[426s] Generation 17 complete: ok=89 min=0.0094 mid=0.0104 max=0.0238 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 4], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[426s] Generation 18 starting: 84 neighbors, 2 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 31.7 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 84/84 15.7 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 225.7         
                                                                  configs/s     
[441s] Generation 18 complete: ok=86 min=0.0094 mid=0.0103 max=0.0238 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 4], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[441s] Generation 19 starting: 86 neighbors, 2 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 86/86 32.7 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 86/86 15.7 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 219.3         
                                                                  configs/s     
[456s] Generation 19 complete: ok=88 min=0.0090 mid=0.0103 max=0.0239 best=Config(block_sizes=[1, 32, 256], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[1], load_eviction_policies=['last', 'last', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=4, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[4, 2], range_unroll_factors=[3, 0], range_warp_specializes=[], waves_per_eu=1)
[456s] Generation 20 starting: 86 neighbors, 2 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 86/86 31.9 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 86/86 15.7 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 225.1         
                                                                  configs/s     
[471s] Generation 20 complete: ok=88 min=0.0089 mid=0.0099 max=0.0238 best=Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1)
[471s] Autotuning complete in 471.8s after searching 2593 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[16, 2, 32], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', '', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=5, num_warps=1, pid_type='persistent_interleaved', range_flattens=[True, None], range_multi_buffers=[None, None], range_num_stages=[1, 3], range_unroll_factors=[4, 3], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 1:
(M, K, N)
--------------
(128, 256, 64)
 40%|████      | 2/5 [17:15<25:31, 510.43s/it]WARNING:tritonbench.utils.triton_op:Running input ID 2:
(M, K, N)
----------------
(512, 1024, 128)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.09ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 34, "num_triton_choices": 34, "best_kernel": "triton_mm_55", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.011385999619960785, "best_triton_pos": 0}
AUTOTUNE mm(512x1024, 1024x128)
strides: [1024, 1], [128, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_55 0.0114 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_54 0.0115 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_47 0.0126 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_51 0.0135 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_43 0.0137 ms 83.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_49 0.0144 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_50 0.0145 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_45 0.0146 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_58 0.0150 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_63 0.0152 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.3627 seconds and 0.6582 seconds precompiling for 34 choices
INFO:tritonbench.utils.triton_op:Took 2173.04ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 244.56ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512, 1024),
              'stride': (1024, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024, 128),
              'stride': (128, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (128,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 4.95ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 606477989
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 9.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 20.5 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 1956.1 configs/s
[18s] Initial random population of 100, 5 starting points: error=20 ok=80 min=0.0112 mid=0.0819 max=4.2749 best=Config(block_sizes=[16, 16, 64], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=1, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[True, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[18s] Generation 1 starting: 239 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 239/239 13.8 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 239/239 16.7 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 105.5         
                                                                  configs/s     
[67s] Generation 1 complete: error=6 ok=238 min=0.0119 mid=0.0143 max=0.1210 best=Config(block_sizes=[16, 16, 64], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[64], load_eviction_policies=['', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=1, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[True, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[67s] Generation 2 starting: 235 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 235/235 14.6 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 235/235 16.3 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 98.6 configs/s
[113s] Generation 2 complete: error=6 ok=234 min=0.0102 mid=0.0136 max=0.1217 best=Config(block_sizes=[16, 16, 128], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[False, None], range_num_stages=[3, 3], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1)
[113s] Generation 3 starting: 217 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 217/217 13.1 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 217/217 16.6 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 100.3         
                                                                  configs/s     
[160s] Generation 3 complete: error=9 ok=213 min=0.0102 mid=0.0121 max=0.1087 best=Config(block_sizes=[16, 16, 64], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[True, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[160s] Generation 4 starting: 227 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 227/227 26.3 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 227/227 17.0 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 99.6 configs/s
[199s] Generation 4 complete: error=9 ok=223 min=0.0099 mid=0.0122 max=0.1088 best=Config(block_sizes=[16, 16, 128], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[3, 3], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1)
[199s] Generation 5 starting: 224 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 224/224 13.9 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 224/224 17.0 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 100.0         
                                                                  configs/s     
[244s] Generation 5 complete: error=9 ok=220 min=0.0106 mid=0.0123 max=0.1089 best=Config(block_sizes=[16, 16, 128], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[244s] Generation 6 starting: 226 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 226/226 22.7 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 226/226 16.8 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 102.1         
                                                                  configs/s     
[284s] Generation 6 complete: error=9 ok=222 min=0.0098 mid=0.0121 max=0.1089 best=Config(block_sizes=[16, 16, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[284s] Generation 7 starting: 224 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 224/224 24.7 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 224/224 16.9 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 103.0         
                                                                  configs/s     
[323s] Generation 7 complete: error=9 ok=220 min=0.0097 mid=0.0121 max=0.1089 best=Config(block_sizes=[16, 16, 64], indexing=['pointer', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[323s] Generation 8 starting: 220 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 220/220 24.7 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 220/220 17.3 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 102.3         
                                                                  configs/s     
[361s] Generation 8 complete: error=11 ok=214 min=0.0101 mid=0.0122 max=0.1089 best=Config(block_sizes=[16, 16, 256], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[361s] Generation 9 starting: 228 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 228/228 24.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 228/228 17.0 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 100.6         
                                                                  configs/s     
[400s] Generation 9 complete: error=11 ok=222 min=0.0100 mid=0.0122 max=0.1089 best=Config(block_sizes=[16, 16, 64], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[400s] Generation 10 starting: 223 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 223/223 13.6 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 223/223 16.4 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 102.2         
                                                                  configs/s     
[446s] Generation 10 complete: error=11 ok=217 min=0.0106 mid=0.0125 max=0.1089 best=Config(block_sizes=[32, 16, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[None, True], range_num_stages=[0, 3], range_unroll_factors=[1, 1], range_warp_specializes=[], waves_per_eu=1)
[446s] Generation 11 starting: 225 neighbors, 5 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 225/225 22.4 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 225/225 17.1 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 105.1         
                                                                  configs/s     
[485s] Generation 11 complete: error=11 ok=219 min=0.0099 mid=0.0121 max=0.1088 best=Config(block_sizes=[16, 16, 256], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[485s] Generation 12 starting: 213 neighbors, 5 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 24.2 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 213/213 17.3 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 107.8         
                                                                  configs/s     
[522s] Generation 12 complete: error=12 ok=206 min=0.0098 mid=0.0122 max=0.1088 best=Config(block_sizes=[16, 32, 128], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[522s] Generation 13 starting: 213 neighbors, 5 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 213/213 17.4 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 213/213 17.5 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 118.9         
                                                                  configs/s     
[561s] Generation 13 complete: error=15 ok=203 min=0.0096 mid=0.0121 max=0.1573 best=Config(block_sizes=[16, 16, 512], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[561s] Generation 14 starting: 231 neighbors, 5 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 231/231 9.8 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 231/231 16.7 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 106.5         
                                                                  configs/s     
[614s] Generation 14 complete: error=17 ok=219 min=0.0098 mid=0.0121 max=0.2161 best=Config(block_sizes=[16, 16, 512], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=6, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[614s] Generation 15 starting: 229 neighbors, 5 active search path(s)
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<32x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<32x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<32x16xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<32x16xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<32x16xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<32x16xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<32x16xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<32x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<32x16xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %132, %120 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x32xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<16x32xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<16x32xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<16x32xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<16x32xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<16x32xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<16x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<16x32xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %132, %120 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 229/229 14.3 configs/s
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[635s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[637s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<32x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<32x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<32x16xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<32x16xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<32x16xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<32x16xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<32x16xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<32x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<32x16xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %132, %120 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[637s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[638s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x32xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<16x32xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<16x32xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<16x32xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<16x32xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<16x32xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<16x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<16x32xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %132, %120 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[638s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 229/229 17.4 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 108.5         
                                                                  configs/s     
[659s] Generation 15 complete: error=22 ok=212 min=0.0105 mid=0.0119 max=0.2166 best=Config(block_sizes=[16, 16, 256], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1)
[659s] Generation 16 starting: 225 neighbors, 5 active search path(s)
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<32x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<32x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<32x16xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<32x16xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<32x16xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<32x16xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<32x16xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<32x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<32x16xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %132, %120 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x32xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<16x32xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<16x32xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<16x32xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<16x32xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<16x32xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<16x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<16x32xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %132, %120 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 225/225 15.1 configs/s
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/az/cazbqrg75t7p65ukdlburnzbymlqgcc4g6uud63yuynexmucll4k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[679s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/u3/cu3tww7tlfwub3pf3zqi6tpt37jlsapw2gz5agklml3wph3zw57e.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[681s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<32x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<32x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<32x16xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<32x16xf32, #blocked6> -> tensor<32x16xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<32x16xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<32x16xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<32x16xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<32x16xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<32x16xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<32x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<32x16xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %132, %120 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/qb/cqbb3zqrcnqoqwg5433fm6rdotvjrlm2hbzh2irmnlmizjvuz3dt.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[681s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [2, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/gz/cgz5ftfuaqqp543dwljq5f6geavdcim7xmmu2ffmogg4eivc7a5k.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[681s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [1, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [2, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [2], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [2, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 2], order = [0, 1]}>
#blocked6 = #ttg.blocked<{sizePerThread = [2, 2], threadsPerWarp = [4, 16], warpsPerCTA = [2, 1], order = [1, 0]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 2 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %73 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %5 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %7 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %34, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %8, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %39, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %99 = arith.addi %98, %43 : tensor<512x32xi32, #blocked2>
      %100 = tt.addptr %9, %99 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>
      %103 = ttg.convert_layout %101 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>
      %104 = ttg.convert_layout %73 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked6>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x32xf32, #blocked6>
      %106 = ttg.convert_layout %105 : tensor<16x32xf32, #blocked6> -> tensor<16x32xf32, #blocked2>
      %107 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %108 = tt.load %107 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %109 = ttg.convert_layout %108 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %110 = tt.expand_dims %109 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %111 = ttg.convert_layout %110 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %112 = arith.extf %111 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %113 = tt.broadcast %112 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %114 = arith.addf %106, %113 : tensor<16x32xf32, #blocked2>
      %115 = arith.mulf %114, %cst_8 : tensor<16x32xf32, #blocked2>
      %116 = arith.mulf %114, %cst_7 : tensor<16x32xf32, #blocked2>
      %117 = tt.extern_elementwise %116 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %118 = arith.addf %117, %cst_6 : tensor<16x32xf32, #blocked2>
      %119 = arith.mulf %115, %118 : tensor<16x32xf32, #blocked2>
      %120 = arith.truncf %119 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %121 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %122 = tt.expand_dims %121 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %123 = ttg.convert_layout %122 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %124 = arith.muli %123, %cst_5 : tensor<16x1xi32, #blocked1>
      %125 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %126 = tt.expand_dims %125 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %127 = ttg.convert_layout %126 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %128 = tt.broadcast %124 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %129 = ttg.convert_layout %128 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %130 = tt.broadcast %127 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %131 = arith.addi %129, %130 : tensor<16x32xi32, #blocked2>
      %132 = tt.addptr %11, %131 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %132, %120 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/by/cbywe7adkre7lvk2dpmn7gr2ye2dvey3w4cc3lxh3nf6zlrprk43.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[681s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 225/225 17.5 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 119.1         
                                                                  configs/s     
[702s] Generation 16 complete: error=24 ok=206 min=0.0109 mid=0.0122 max=0.2159 best=Config(block_sizes=[16, 32, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=2, pid_type='persistent_interleaved', range_flattens=[False, True], range_multi_buffers=[True, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[702s] Generation 17 starting: 241 neighbors, 5 active search path(s)
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<32x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<32x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x16xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<32x16xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<32x16xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<32x16xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<32x16xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<32x16xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<32x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<32x16xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %130, %118 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<16x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<16x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x32xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<16x32xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<16x32xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<16x32xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<16x32xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<16x32xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<16x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<16x32xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %130, %118 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 241/241 15.0 configs/s
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[724s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[726s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<32x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<32x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x16xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<32x16xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<32x16xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<32x16xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<32x16xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<32x16xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<32x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<32x16xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %130, %118 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[726s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[726s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<16x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<16x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x32xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<16x32xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<16x32xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<16x32xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<16x32xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<16x32xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<16x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<16x32xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %130, %118 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[726s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 241/241 17.4 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 102.7         
                                                                  configs/s     
[749s] Generation 17 complete: error=26 ok=220 min=0.0098 mid=0.0116 max=0.2497 best=Config(block_sizes=[16, 16, 512], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=8, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, None], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[749s] Generation 18 starting: 236 neighbors, 5 active search path(s)
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<32x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<32x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x16xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<32x16xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<32x16xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<32x16xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<32x16xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<32x16xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<32x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<32x16xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %130, %118 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<16x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<16x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x32xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<16x32xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<16x32xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<16x32xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<16x32xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<16x32xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<16x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<16x32xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %130, %118 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 236/236 17.3 configs/s
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[767s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[770s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<32x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<32x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x16xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<32x16xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<32x16xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<32x16xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<32x16xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<32x16xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<32x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<32x16xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %130, %118 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[770s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[770s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<16x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<16x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x32xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<16x32xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<16x32xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<16x32xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<16x32xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<16x32xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<16x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<16x32xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %130, %118 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[770s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['last', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 236/236 17.0 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 121.5         
                                                                  configs/s     
[790s] Generation 18 complete: error=26 ok=215 min=0.0101 mid=0.0117 max=0.2487 best=Config(block_sizes=[16, 32, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[16], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=2, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[0, 3], range_unroll_factors=[0, 1], range_warp_specializes=[], waves_per_eu=1)
[790s] Generation 19 starting: 235 neighbors, 5 active search path(s)
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<32x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<32x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x16xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<32x16xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<32x16xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<32x16xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<32x16xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<32x16xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<32x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<32x16xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %130, %118 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<16x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<16x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x32xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<16x32xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<16x32xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<16x32xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<16x32xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<16x32xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<16x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<16x32xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %130, %118 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 235/235 12.8 configs/s
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c256_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %5 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %6 = arith.extsi %4 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %7 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %8 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %9 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %10 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %11 = arith.divsi %arg4, %c512_i32 : i32
      %12 = arith.muli %11, %c64_i32 : i32
      %13 = arith.subi %c32_i32, %12 : i32
      %14 = arith.minsi %13, %c64_i32 : i32
      %15 = arith.remsi %arg4, %c512_i32 : i32
      %16 = arith.remsi %15, %14 : i32
      %17 = arith.addi %12, %16 : i32
      %18 = arith.divsi %15, %14 : i32
      %19 = arith.muli %17, %c16_i32 : i32
      %20 = tt.splat %19 : i32 -> tensor<16xi32, #blocked3>
      %21 = arith.addi %20, %3 : tensor<16xi32, #blocked3>
      %22 = arith.muli %18, %c16_i32 : i32
      %23 = tt.splat %22 : i32 -> tensor<16xi32, #blocked3>
      %24 = arith.addi %23, %3 : tensor<16xi32, #blocked3>
      %25 = arith.extsi %19 : i32 to i64
      %26 = tt.splat %25 : i64 -> tensor<16xi64, #blocked3>
      %27 = arith.addi %26, %5 : tensor<16xi64, #blocked3>
      %28 = ttg.convert_layout %27 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %29 = tt.expand_dims %28 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %30 = ttg.convert_layout %29 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %31 = arith.muli %30, %cst_3 : tensor<16x1xi64, #blocked1>
      %32 = tt.broadcast %31 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %33 = ttg.convert_layout %32 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %34 = arith.cmpi sge, %30, %cst_2 : tensor<16x1xi64, #blocked1>
      %35 = arith.cmpi slt, %30, %cst_1 : tensor<16x1xi64, #blocked1>
      %36 = arith.andi %34, %35 : tensor<16x1xi1, #blocked1>
      %37 = tt.broadcast %36 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %38 = ttg.convert_layout %37 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %39 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %40 = tt.expand_dims %39 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %41 = ttg.convert_layout %40 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %42 = tt.broadcast %41 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %43 = arith.addi %cst_12, %4 : tensor<512xi32, #blocked3>
      %44 = arith.extsi %c0_i32 : i32 to i64
      %45 = tt.splat %44 : i64 -> tensor<512xi64, #blocked3>
      %46 = arith.addi %45, %6 : tensor<512xi64, #blocked3>
      %47 = ttg.convert_layout %46 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %48 = tt.expand_dims %47 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %49 = ttg.convert_layout %48 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %50 = tt.broadcast %49 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %51 = arith.addi %33, %50 : tensor<16x512xi64, #blocked>
      %52 = tt.addptr %7, %51 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %53 = arith.cmpi sge, %49, %cst_0 : tensor<1x512xi64, #blocked>
      %54 = arith.cmpi slt, %49, %cst : tensor<1x512xi64, #blocked>
      %55 = arith.andi %53, %54 : tensor<1x512xi1, #blocked>
      %56 = tt.broadcast %55 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %57 = arith.andi %38, %56 : tensor<16x512xi1, #blocked>
      %58 = tt.load %52, %57, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %59 = ttg.convert_layout %43 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %60 = tt.expand_dims %59 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %61 = ttg.convert_layout %60 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %62 = arith.muli %61, %cst_9 : tensor<512x1xi32, #blocked1>
      %63 = tt.broadcast %62 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %64 = ttg.convert_layout %63 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %65 = arith.addi %64, %42 : tensor<512x16xi32, #blocked2>
      %66 = tt.addptr %8, %65 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %67 = tt.load %66 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %68 = ttg.convert_layout %58 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %69 = ttg.convert_layout %67 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %70 = ttg.convert_layout %cst_10 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %71 = tt.dot %68, %69, %70, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %72 = arith.muli %c512_i32, %c1_i32_13 : i32
      %73 = arith.addi %c0_i32, %72 : i32
      %74 = tt.splat %73 : i32 -> tensor<512xi32, #blocked3>
      %75 = arith.addi %74, %4 : tensor<512xi32, #blocked3>
      %76 = arith.extsi %73 : i32 to i64
      %77 = tt.splat %76 : i64 -> tensor<512xi64, #blocked3>
      %78 = arith.addi %77, %6 : tensor<512xi64, #blocked3>
      %79 = ttg.convert_layout %78 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %80 = tt.expand_dims %79 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %81 = ttg.convert_layout %80 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %82 = tt.broadcast %81 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %83 = arith.addi %33, %82 : tensor<16x512xi64, #blocked>
      %84 = tt.addptr %7, %83 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %85 = arith.cmpi sge, %81, %cst_0 : tensor<1x512xi64, #blocked>
      %86 = arith.cmpi slt, %81, %cst : tensor<1x512xi64, #blocked>
      %87 = arith.andi %85, %86 : tensor<1x512xi1, #blocked>
      %88 = tt.broadcast %87 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %89 = arith.andi %38, %88 : tensor<16x512xi1, #blocked>
      %90 = tt.load %84, %89, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %91 = ttg.convert_layout %75 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %92 = tt.expand_dims %91 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %93 = ttg.convert_layout %92 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %94 = arith.muli %93, %cst_9 : tensor<512x1xi32, #blocked1>
      %95 = tt.broadcast %94 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %96 = ttg.convert_layout %95 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %97 = arith.addi %96, %42 : tensor<512x16xi32, #blocked2>
      %98 = tt.addptr %8, %97 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %99 = tt.load %98 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %100 = ttg.convert_layout %90 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %101 = ttg.convert_layout %99 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %102 = ttg.convert_layout %71 : tensor<16x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %103 = tt.dot %100, %101, %102, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x16xf32, #blocked2>
      %104 = tt.addptr %9, %24 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %105 = tt.load %104 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %106 = ttg.convert_layout %105 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %107 = tt.expand_dims %106 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %108 = ttg.convert_layout %107 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %109 = arith.extf %108 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %110 = tt.broadcast %109 : tensor<1x16xf32, #blocked2> -> tensor<16x16xf32, #blocked2>
      %111 = arith.addf %103, %110 : tensor<16x16xf32, #blocked2>
      %112 = arith.mulf %111, %cst_8 : tensor<16x16xf32, #blocked2>
      %113 = arith.mulf %111, %cst_7 : tensor<16x16xf32, #blocked2>
      %114 = tt.extern_elementwise %113 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x16xf32, #blocked2>) -> tensor<16x16xf32, #blocked2>
      %115 = arith.addf %114, %cst_6 : tensor<16x16xf32, #blocked2>
      %116 = arith.mulf %112, %115 : tensor<16x16xf32, #blocked2>
      %117 = arith.truncf %116 : tensor<16x16xf32, #blocked2> to tensor<16x16xbf16, #blocked2>
      %118 = ttg.convert_layout %21 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %119 = tt.expand_dims %118 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %120 = ttg.convert_layout %119 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %121 = arith.muli %120, %cst_5 : tensor<16x1xi32, #blocked1>
      %122 = ttg.convert_layout %24 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %123 = tt.expand_dims %122 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %124 = ttg.convert_layout %123 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %125 = tt.broadcast %121 : tensor<16x1xi32, #blocked1> -> tensor<16x16xi32, #blocked1>
      %126 = ttg.convert_layout %125 : tensor<16x16xi32, #blocked1> -> tensor<16x16xi32, #blocked2>
      %127 = tt.broadcast %124 : tensor<1x16xi32, #blocked2> -> tensor<16x16xi32, #blocked2>
      %128 = arith.addi %126, %127 : tensor<16x16xi32, #blocked2>
      %129 = tt.addptr %10, %128 : tensor<16x16x!tt.ptr<bf16>, #blocked2>, tensor<16x16xi32, #blocked2>
      tt.store %129, %117 : tensor<16x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2l/c2l5qjhr5h65n3s2t3donhavyjdd27ag5jnkanhlayrsbfzxk4nh.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[814s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:55:33: note: - use: %99 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<8x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<8x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<8x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<8x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<8x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<8x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<8x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<8x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<8x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<8x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<8x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c8_i32 = arith.constant 8 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %5 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<8xi32, #blocked3> to tensor<8xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<8x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<8x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c512_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c64_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c512_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c8_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<8xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<8xi32, #blocked3>
      %24 = arith.muli %20, %c16_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<16xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<16xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<8xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<8xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<8xi64, #blocked3> -> tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<8xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<8x1xi64, #blocked4> -> tensor<8x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<8x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<8x1xi64, #blocked1> -> tensor<8x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<8x512xi64, #blocked1> -> tensor<8x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<8x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<8x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<8x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<8x1xi1, #blocked1> -> tensor<8x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<8x512xi1, #blocked1> -> tensor<8x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<8x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<8x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x16xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<8x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<8x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<8x512x!tt.ptr<bf16>, #blocked>, tensor<8x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<8x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<8x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<8x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x16xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<8x512xbf16, #blocked> -> tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<8x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<8x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<8x16xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x16xf32, #blocked2> -> tensor<8x16xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<8x16xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<8x16xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<8x16xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<8x16xf32, #blocked2>) -> tensor<8x16xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<8x16xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<8x16xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<8x16xf32, #blocked2> to tensor<8x16xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<8xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<8x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<8x1xi32, #blocked4> -> tensor<8x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<8x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<8x1xi32, #blocked1> -> tensor<8x16xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<8x16xi32, #blocked1> -> tensor<8x16xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x16xi32, #blocked2> -> tensor<8x16xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<8x16xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<8x16x!tt.ptr<bf16>, #blocked2>, tensor<8x16xi32, #blocked2>
      tt.store %131, %119 : tensor<8x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/fz/cfzwks7lq2vab5jatd3lwnh23dn4vh32kq47ar6kuc6xluwhfi5x.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[816s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[8, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<16xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x16xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [2, 2], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<32x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<32x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<32x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<32x512xbf16, #blocked>
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<32x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<32x16xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<32x16xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<32x16xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<32x16xf32, #blocked2>
    %c16_i32 = arith.constant 16 : i32
    %c32_i32 = arith.constant 32 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<32xi32, #blocked3> to tensor<32xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<32x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x16x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<16x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<32x16x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c512_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c16_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c512_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c32_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<32xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<32xi32, #blocked3>
      %23 = arith.muli %19, %c16_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<16xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<16xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<32xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<32xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<32xi64, #blocked3> -> tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<32xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<32x1xi64, #blocked4> -> tensor<32x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<32x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<32x1xi64, #blocked1> -> tensor<32x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<32x512xi64, #blocked1> -> tensor<32x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<32x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<32x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<32x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<32x1xi1, #blocked1> -> tensor<32x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<32x512xi1, #blocked1> -> tensor<32x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x16xi32, #blocked2> -> tensor<512x16xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<32x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<32x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x16xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<32x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<32x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<32x512x!tt.ptr<bf16>, #blocked>, tensor<32x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<32x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<32x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<32x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x16xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x16xi32, #blocked1> -> tensor<512x16xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x16xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x16x!tt.ptr<bf16>, #blocked2>, tensor<512x16xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x16x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<32x512xbf16, #blocked> -> tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x16xbf16, #blocked2> -> tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<32x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<32x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x16xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<32x16xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<16x!tt.ptr<bf16>, #blocked3>, tensor<16xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<16x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<16xbf16, #blocked3> -> tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<16xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x16xbf16, #blocked5> -> tensor<1x16xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x16xbf16, #blocked2> to tensor<1x16xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x16xf32, #blocked2> -> tensor<32x16xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<32x16xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<32x16xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<32x16xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<32x16xf32, #blocked2>) -> tensor<32x16xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<32x16xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<32x16xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<32x16xf32, #blocked2> to tensor<32x16xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<32xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<32x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<32x1xi32, #blocked4> -> tensor<32x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<32x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x16xi32, #blocked5> -> tensor<1x16xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<32x1xi32, #blocked1> -> tensor<32x16xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<32x16xi32, #blocked1> -> tensor<32x16xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x16xi32, #blocked2> -> tensor<32x16xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<32x16xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<32x16x!tt.ptr<bf16>, #blocked2>, tensor<32x16xi32, #blocked2>
      tt.store %130, %118 : tensor<32x16x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/nd/cndomhzcktkax7fu45h25nx7caaujnlv5o2xv45uauu7d2heae5r.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[816s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[32, 16, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: error: 'tt.load' op operation destroyed but still has uses
            a_tile = x_desc.load([offset_0, offset_2])
                                ^
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:55:33: note: - use: %101 = "tt.fp_to_fp"(<<UNKNOWN SSA VALUE>>) <{rounding = 1 : i32}> : (tensor<16x512xbf16, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>) -> tensor<16x512xf32, #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [1, 64], warpsPerCTA = [4, 1], order = [1, 0]}>>

LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [8, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c2_i32 = arith.constant 2 : i32
    %c32_i32 = arith.constant 32 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x8xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x8xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x8xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x8xf32, #blocked2>
    %c8_i32 = arith.constant 8 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.muli %0, %c2_i32 : i32
    %2 = arith.addi %1, %c2_i32 : i32
    %3 = arith.minsi %2, %c512_i32 : i32
    %4 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %5 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #blocked3>
    %6 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %7 = arith.extsi %4 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %8 = arith.extsi %6 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %9 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %10 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x8x!tt.ptr<bf16>, #blocked2>
    %11 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<8x!tt.ptr<bf16>, #blocked3>
    %12 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x8x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %1 to %3 step %c1_i32  : i32 {
      %13 = arith.divsi %arg4, %c1024_i32 : i32
      %14 = arith.muli %13, %c64_i32 : i32
      %15 = arith.subi %c32_i32, %14 : i32
      %16 = arith.minsi %15, %c64_i32 : i32
      %17 = arith.remsi %arg4, %c1024_i32 : i32
      %18 = arith.remsi %17, %16 : i32
      %19 = arith.addi %14, %18 : i32
      %20 = arith.divsi %17, %16 : i32
      %21 = arith.muli %19, %c16_i32 : i32
      %22 = tt.splat %21 : i32 -> tensor<16xi32, #blocked3>
      %23 = arith.addi %22, %4 : tensor<16xi32, #blocked3>
      %24 = arith.muli %20, %c8_i32 : i32
      %25 = tt.splat %24 : i32 -> tensor<8xi32, #blocked3>
      %26 = arith.addi %25, %5 : tensor<8xi32, #blocked3>
      %27 = arith.extsi %21 : i32 to i64
      %28 = tt.splat %27 : i64 -> tensor<16xi64, #blocked3>
      %29 = arith.addi %28, %7 : tensor<16xi64, #blocked3>
      %30 = ttg.convert_layout %29 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %31 = tt.expand_dims %30 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %32 = ttg.convert_layout %31 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %33 = arith.muli %32, %cst_3 : tensor<16x1xi64, #blocked1>
      %34 = tt.broadcast %33 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %35 = ttg.convert_layout %34 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %36 = arith.cmpi sge, %32, %cst_2 : tensor<16x1xi64, #blocked1>
      %37 = arith.cmpi slt, %32, %cst_1 : tensor<16x1xi64, #blocked1>
      %38 = arith.andi %36, %37 : tensor<16x1xi1, #blocked1>
      %39 = tt.broadcast %38 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %40 = ttg.convert_layout %39 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %41 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %42 = tt.expand_dims %41 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %43 = ttg.convert_layout %42 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %44 = tt.broadcast %43 : tensor<1x8xi32, #blocked2> -> tensor<512x8xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %45 = arith.addi %cst_12, %6 : tensor<512xi32, #blocked3>
      %46 = arith.extsi %c0_i32 : i32 to i64
      %47 = tt.splat %46 : i64 -> tensor<512xi64, #blocked3>
      %48 = arith.addi %47, %8 : tensor<512xi64, #blocked3>
      %49 = ttg.convert_layout %48 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %50 = tt.expand_dims %49 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %51 = ttg.convert_layout %50 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %52 = tt.broadcast %51 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %53 = arith.addi %35, %52 : tensor<16x512xi64, #blocked>
      %54 = tt.addptr %9, %53 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %55 = arith.cmpi sge, %51, %cst_0 : tensor<1x512xi64, #blocked>
      %56 = arith.cmpi slt, %51, %cst : tensor<1x512xi64, #blocked>
      %57 = arith.andi %55, %56 : tensor<1x512xi1, #blocked>
      %58 = tt.broadcast %57 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %59 = arith.andi %40, %58 : tensor<16x512xi1, #blocked>
      %60 = tt.load %54, %59, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %61 = ttg.convert_layout %45 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %62 = tt.expand_dims %61 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %63 = ttg.convert_layout %62 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %64 = arith.muli %63, %cst_9 : tensor<512x1xi32, #blocked1>
      %65 = tt.broadcast %64 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %66 = ttg.convert_layout %65 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %67 = arith.addi %66, %44 : tensor<512x8xi32, #blocked2>
      %68 = tt.addptr %10, %67 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %69 = tt.load %68 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %70 = ttg.convert_layout %60 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %71 = ttg.convert_layout %69 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %72 = ttg.convert_layout %cst_10 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %73 = tt.dot %70, %71, %72, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %74 = arith.muli %c512_i32, %c1_i32_13 : i32
      %75 = arith.addi %c0_i32, %74 : i32
      %76 = tt.splat %75 : i32 -> tensor<512xi32, #blocked3>
      %77 = arith.addi %76, %6 : tensor<512xi32, #blocked3>
      %78 = arith.extsi %75 : i32 to i64
      %79 = tt.splat %78 : i64 -> tensor<512xi64, #blocked3>
      %80 = arith.addi %79, %8 : tensor<512xi64, #blocked3>
      %81 = ttg.convert_layout %80 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %82 = tt.expand_dims %81 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %83 = ttg.convert_layout %82 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %84 = tt.broadcast %83 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %85 = arith.addi %35, %84 : tensor<16x512xi64, #blocked>
      %86 = tt.addptr %9, %85 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %87 = arith.cmpi sge, %83, %cst_0 : tensor<1x512xi64, #blocked>
      %88 = arith.cmpi slt, %83, %cst : tensor<1x512xi64, #blocked>
      %89 = arith.andi %87, %88 : tensor<1x512xi1, #blocked>
      %90 = tt.broadcast %89 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %91 = arith.andi %40, %90 : tensor<16x512xi1, #blocked>
      %92 = tt.load %86, %91, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %93 = ttg.convert_layout %77 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %94 = tt.expand_dims %93 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %95 = ttg.convert_layout %94 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %96 = arith.muli %95, %cst_9 : tensor<512x1xi32, #blocked1>
      %97 = tt.broadcast %96 : tensor<512x1xi32, #blocked1> -> tensor<512x8xi32, #blocked1>
      %98 = ttg.convert_layout %97 : tensor<512x8xi32, #blocked1> -> tensor<512x8xi32, #blocked2>
      %99 = arith.addi %98, %44 : tensor<512x8xi32, #blocked2>
      %100 = tt.addptr %10, %99 : tensor<512x8x!tt.ptr<bf16>, #blocked2>, tensor<512x8xi32, #blocked2>
      %101 = tt.load %100 evictionPolicy = evict_first : tensor<512x8x!tt.ptr<bf16>, #blocked2>
      %102 = ttg.convert_layout %92 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %103 = ttg.convert_layout %101 : tensor<512x8xbf16, #blocked2> -> tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %104 = ttg.convert_layout %73 : tensor<16x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %105 = tt.dot %102, %103, %104, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x8xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x8xf32, #blocked2>
      %106 = tt.addptr %11, %26 : tensor<8x!tt.ptr<bf16>, #blocked3>, tensor<8xi32, #blocked3>
      %107 = tt.load %106 evictionPolicy = evict_first : tensor<8x!tt.ptr<bf16>, #blocked3>
      %108 = ttg.convert_layout %107 : tensor<8xbf16, #blocked3> -> tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %109 = tt.expand_dims %108 {axis = 0 : i32} : tensor<8xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xbf16, #blocked5>
      %110 = ttg.convert_layout %109 : tensor<1x8xbf16, #blocked5> -> tensor<1x8xbf16, #blocked2>
      %111 = arith.extf %110 : tensor<1x8xbf16, #blocked2> to tensor<1x8xf32, #blocked2>
      %112 = tt.broadcast %111 : tensor<1x8xf32, #blocked2> -> tensor<16x8xf32, #blocked2>
      %113 = arith.addf %105, %112 : tensor<16x8xf32, #blocked2>
      %114 = arith.mulf %113, %cst_8 : tensor<16x8xf32, #blocked2>
      %115 = arith.mulf %113, %cst_7 : tensor<16x8xf32, #blocked2>
      %116 = tt.extern_elementwise %115 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x8xf32, #blocked2>) -> tensor<16x8xf32, #blocked2>
      %117 = arith.addf %116, %cst_6 : tensor<16x8xf32, #blocked2>
      %118 = arith.mulf %114, %117 : tensor<16x8xf32, #blocked2>
      %119 = arith.truncf %118 : tensor<16x8xf32, #blocked2> to tensor<16x8xbf16, #blocked2>
      %120 = ttg.convert_layout %23 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %122 = ttg.convert_layout %121 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %123 = arith.muli %122, %cst_5 : tensor<16x1xi32, #blocked1>
      %124 = ttg.convert_layout %26 : tensor<8xi32, #blocked3> -> tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %125 = tt.expand_dims %124 {axis = 0 : i32} : tensor<8xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x8xi32, #blocked5>
      %126 = ttg.convert_layout %125 : tensor<1x8xi32, #blocked5> -> tensor<1x8xi32, #blocked2>
      %127 = tt.broadcast %123 : tensor<16x1xi32, #blocked1> -> tensor<16x8xi32, #blocked1>
      %128 = ttg.convert_layout %127 : tensor<16x8xi32, #blocked1> -> tensor<16x8xi32, #blocked2>
      %129 = tt.broadcast %126 : tensor<1x8xi32, #blocked2> -> tensor<16x8xi32, #blocked2>
      %130 = arith.addi %128, %129 : tensor<16x8xi32, #blocked2>
      %131 = tt.addptr %12, %130 : tensor<16x8x!tt.ptr<bf16>, #blocked2>, tensor<16x8xi32, #blocked2>
      tt.store %131, %119 : tensor<16x8x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/42/c42ee73nxgbwpdzhh6oadr3u7nhwckku2kws45cogtaz67gau6xp.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[816s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 8, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:61:45: error: 'tt.load' op operation destroyed but still has uses
        load = tl.load(bias + indices_1 * 1, None, eviction_policy='evict_first')
                                            ^
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:62:19: note: - use: %128 = "tt.expand_dims"(<<UNKNOWN SSA VALUE>>) <{axis = 0 : i32}> : (tensor<32xbf16, #ttg.slice<{dim = 0, parent = #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>}>>) -> tensor<1x32xbf16, #ttg.amd_mfma<{version = 3, warpsPerCTA = [4, 1], instrShape = [16, 16], isTransposed = true}>>

        v_0 = load[None, :]
                  ^
LLVM ERROR: operation destroyed but still has uses
#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked2 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 32], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked3 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>
#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [64, 1], warpsPerCTA = [4, 1], order = [0, 1]}>
#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 64], warpsPerCTA = [1, 4], order = [0, 1]}>
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx942", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_helion_gemm_gelu(%arg0: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<bf16> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<bf16> {tt.divisibility = 16 : i32}) attributes {noinline = false} {
    %cst = arith.constant dense<1024> : tensor<1x512xi64, #blocked>
    %cst_0 = arith.constant dense<0> : tensor<1x512xi64, #blocked>
    %cst_1 = arith.constant dense<512> : tensor<16x1xi64, #blocked1>
    %cst_2 = arith.constant dense<0> : tensor<16x1xi64, #blocked1>
    %cst_3 = arith.constant dense<1024> : tensor<16x1xi64, #blocked1>
    %cst_4 = arith.constant dense<0.000000e+00> : tensor<16x512xbf16, #blocked>
    %c256_i32 = arith.constant 256 : i32
    %c128_i32 = arith.constant 128 : i32
    %c0_i32 = arith.constant 0 : i32
    %c1_i32 = arith.constant 1 : i32
    %cst_5 = arith.constant dense<128> : tensor<16x1xi32, #blocked1>
    %cst_6 = arith.constant dense<1.000000e+00> : tensor<16x32xf32, #blocked2>
    %cst_7 = arith.constant dense<0.707106769> : tensor<16x32xf32, #blocked2>
    %cst_8 = arith.constant dense<5.000000e-01> : tensor<16x32xf32, #blocked2>
    %cst_9 = arith.constant dense<128> : tensor<512x1xi32, #blocked1>
    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16x32xf32, #blocked2>
    %c32_i32 = arith.constant 32 : i32
    %c16_i32 = arith.constant 16 : i32
    %c64_i32 = arith.constant 64 : i32
    %c512_i32 = arith.constant 512 : i32
    %c1024_i32 = arith.constant 1024 : i32
    %0 = tt.get_program_id x : i32
    %1 = arith.addi %0, %c1_i32 : i32
    %2 = arith.minsi %1, %c128_i32 : i32
    %3 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked3>
    %4 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #blocked3>
    %5 = tt.make_range {end = 512 : i32, start = 0 : i32} : tensor<512xi32, #blocked3>
    %6 = arith.extsi %3 : tensor<16xi32, #blocked3> to tensor<16xi64, #blocked3>
    %7 = arith.extsi %5 : tensor<512xi32, #blocked3> to tensor<512xi64, #blocked3>
    %8 = tt.splat %arg0 : !tt.ptr<bf16> -> tensor<16x512x!tt.ptr<bf16>, #blocked>
    %9 = tt.splat %arg1 : !tt.ptr<bf16> -> tensor<512x32x!tt.ptr<bf16>, #blocked2>
    %10 = tt.splat %arg2 : !tt.ptr<bf16> -> tensor<32x!tt.ptr<bf16>, #blocked3>
    %11 = tt.splat %arg3 : !tt.ptr<bf16> -> tensor<16x32x!tt.ptr<bf16>, #blocked2>
    scf.for %arg4 = %0 to %2 step %c1_i32  : i32 {
      %12 = arith.divsi %arg4, %c256_i32 : i32
      %13 = arith.muli %12, %c64_i32 : i32
      %14 = arith.subi %c32_i32, %13 : i32
      %15 = arith.minsi %14, %c64_i32 : i32
      %16 = arith.remsi %arg4, %c256_i32 : i32
      %17 = arith.remsi %16, %15 : i32
      %18 = arith.addi %13, %17 : i32
      %19 = arith.divsi %16, %15 : i32
      %20 = arith.muli %18, %c16_i32 : i32
      %21 = tt.splat %20 : i32 -> tensor<16xi32, #blocked3>
      %22 = arith.addi %21, %3 : tensor<16xi32, #blocked3>
      %23 = arith.muli %19, %c32_i32 : i32
      %24 = tt.splat %23 : i32 -> tensor<32xi32, #blocked3>
      %25 = arith.addi %24, %4 : tensor<32xi32, #blocked3>
      %26 = arith.extsi %20 : i32 to i64
      %27 = tt.splat %26 : i64 -> tensor<16xi64, #blocked3>
      %28 = arith.addi %27, %6 : tensor<16xi64, #blocked3>
      %29 = ttg.convert_layout %28 : tensor<16xi64, #blocked3> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %30 = tt.expand_dims %29 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>
      %31 = ttg.convert_layout %30 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked1>
      %32 = arith.muli %31, %cst_3 : tensor<16x1xi64, #blocked1>
      %33 = tt.broadcast %32 : tensor<16x1xi64, #blocked1> -> tensor<16x512xi64, #blocked1>
      %34 = ttg.convert_layout %33 : tensor<16x512xi64, #blocked1> -> tensor<16x512xi64, #blocked>
      %35 = arith.cmpi sge, %31, %cst_2 : tensor<16x1xi64, #blocked1>
      %36 = arith.cmpi slt, %31, %cst_1 : tensor<16x1xi64, #blocked1>
      %37 = arith.andi %35, %36 : tensor<16x1xi1, #blocked1>
      %38 = tt.broadcast %37 : tensor<16x1xi1, #blocked1> -> tensor<16x512xi1, #blocked1>
      %39 = ttg.convert_layout %38 : tensor<16x512xi1, #blocked1> -> tensor<16x512xi1, #blocked>
      %40 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %41 = tt.expand_dims %40 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %42 = ttg.convert_layout %41 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %43 = tt.broadcast %42 : tensor<1x32xi32, #blocked2> -> tensor<512x32xi32, #blocked2>
      %c1024_i32_11 = arith.constant 1024 : i32
      %cst_12 = arith.constant dense<0> : tensor<512xi32, #blocked3>
      %44 = arith.addi %cst_12, %5 : tensor<512xi32, #blocked3>
      %45 = arith.extsi %c0_i32 : i32 to i64
      %46 = tt.splat %45 : i64 -> tensor<512xi64, #blocked3>
      %47 = arith.addi %46, %7 : tensor<512xi64, #blocked3>
      %48 = ttg.convert_layout %47 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %49 = tt.expand_dims %48 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %50 = ttg.convert_layout %49 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %51 = tt.broadcast %50 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %52 = arith.addi %34, %51 : tensor<16x512xi64, #blocked>
      %53 = tt.addptr %8, %52 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %54 = arith.cmpi sge, %50, %cst_0 : tensor<1x512xi64, #blocked>
      %55 = arith.cmpi slt, %50, %cst : tensor<1x512xi64, #blocked>
      %56 = arith.andi %54, %55 : tensor<1x512xi1, #blocked>
      %57 = tt.broadcast %56 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %58 = arith.andi %39, %57 : tensor<16x512xi1, #blocked>
      %59 = tt.load %53, %58, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %60 = ttg.convert_layout %44 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %61 = tt.expand_dims %60 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %62 = ttg.convert_layout %61 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %63 = arith.muli %62, %cst_9 : tensor<512x1xi32, #blocked1>
      %64 = tt.broadcast %63 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %65 = ttg.convert_layout %64 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %66 = arith.addi %65, %43 : tensor<512x32xi32, #blocked2>
      %67 = tt.addptr %9, %66 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %68 = tt.load %67 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %69 = ttg.convert_layout %59 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %70 = ttg.convert_layout %68 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %71 = ttg.convert_layout %cst_10 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %72 = tt.dot %69, %70, %71, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %c1_i32_13 = arith.constant 1 : i32
      %73 = arith.muli %c512_i32, %c1_i32_13 : i32
      %74 = arith.addi %c0_i32, %73 : i32
      %75 = tt.splat %74 : i32 -> tensor<512xi32, #blocked3>
      %76 = arith.addi %75, %5 : tensor<512xi32, #blocked3>
      %77 = arith.extsi %74 : i32 to i64
      %78 = tt.splat %77 : i64 -> tensor<512xi64, #blocked3>
      %79 = arith.addi %78, %7 : tensor<512xi64, #blocked3>
      %80 = ttg.convert_layout %79 : tensor<512xi64, #blocked3> -> tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %81 = tt.expand_dims %80 {axis = 0 : i32} : tensor<512xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x512xi64, #blocked5>
      %82 = ttg.convert_layout %81 : tensor<1x512xi64, #blocked5> -> tensor<1x512xi64, #blocked>
      %83 = tt.broadcast %82 : tensor<1x512xi64, #blocked> -> tensor<16x512xi64, #blocked>
      %84 = arith.addi %34, %83 : tensor<16x512xi64, #blocked>
      %85 = tt.addptr %8, %84 : tensor<16x512x!tt.ptr<bf16>, #blocked>, tensor<16x512xi64, #blocked>
      %86 = arith.cmpi sge, %82, %cst_0 : tensor<1x512xi64, #blocked>
      %87 = arith.cmpi slt, %82, %cst : tensor<1x512xi64, #blocked>
      %88 = arith.andi %86, %87 : tensor<1x512xi1, #blocked>
      %89 = tt.broadcast %88 : tensor<1x512xi1, #blocked> -> tensor<16x512xi1, #blocked>
      %90 = arith.andi %39, %89 : tensor<16x512xi1, #blocked>
      %91 = tt.load %85, %90, %cst_4 : tensor<16x512x!tt.ptr<bf16>, #blocked>
      %92 = ttg.convert_layout %76 : tensor<512xi32, #blocked3> -> tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %93 = tt.expand_dims %92 {axis = 1 : i32} : tensor<512xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<512x1xi32, #blocked4>
      %94 = ttg.convert_layout %93 : tensor<512x1xi32, #blocked4> -> tensor<512x1xi32, #blocked1>
      %95 = arith.muli %94, %cst_9 : tensor<512x1xi32, #blocked1>
      %96 = tt.broadcast %95 : tensor<512x1xi32, #blocked1> -> tensor<512x32xi32, #blocked1>
      %97 = ttg.convert_layout %96 : tensor<512x32xi32, #blocked1> -> tensor<512x32xi32, #blocked2>
      %98 = arith.addi %97, %43 : tensor<512x32xi32, #blocked2>
      %99 = tt.addptr %9, %98 : tensor<512x32x!tt.ptr<bf16>, #blocked2>, tensor<512x32xi32, #blocked2>
      %100 = tt.load %99 evictionPolicy = evict_first : tensor<512x32x!tt.ptr<bf16>, #blocked2>
      %101 = ttg.convert_layout %91 : tensor<16x512xbf16, #blocked> -> tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>>
      %102 = ttg.convert_layout %100 : tensor<512x32xbf16, #blocked2> -> tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>>
      %103 = ttg.convert_layout %72 : tensor<16x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %104 = tt.dot %101, %102, %103, inputPrecision = tf32 : tensor<16x512xbf16, #ttg.dot_op<{opIdx = 0, parent = #blocked2}>> * tensor<512x32xbf16, #ttg.dot_op<{opIdx = 1, parent = #blocked2}>> -> tensor<16x32xf32, #blocked2>
      %105 = tt.addptr %10, %25 : tensor<32x!tt.ptr<bf16>, #blocked3>, tensor<32xi32, #blocked3>
      %106 = tt.load %105 evictionPolicy = evict_first : tensor<32x!tt.ptr<bf16>, #blocked3>
      %107 = ttg.convert_layout %106 : tensor<32xbf16, #blocked3> -> tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %108 = tt.expand_dims %107 {axis = 0 : i32} : tensor<32xbf16, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xbf16, #blocked5>
      %109 = ttg.convert_layout %108 : tensor<1x32xbf16, #blocked5> -> tensor<1x32xbf16, #blocked2>
      %110 = arith.extf %109 : tensor<1x32xbf16, #blocked2> to tensor<1x32xf32, #blocked2>
      %111 = tt.broadcast %110 : tensor<1x32xf32, #blocked2> -> tensor<16x32xf32, #blocked2>
      %112 = arith.addf %104, %111 : tensor<16x32xf32, #blocked2>
      %113 = arith.mulf %112, %cst_8 : tensor<16x32xf32, #blocked2>
      %114 = arith.mulf %112, %cst_7 : tensor<16x32xf32, #blocked2>
      %115 = tt.extern_elementwise %114 {libname = "", libpath = "", pure = true, symbol = "__ocml_erf_f32"} : (tensor<16x32xf32, #blocked2>) -> tensor<16x32xf32, #blocked2>
      %116 = arith.addf %115, %cst_6 : tensor<16x32xf32, #blocked2>
      %117 = arith.mulf %113, %116 : tensor<16x32xf32, #blocked2>
      %118 = arith.truncf %117 : tensor<16x32xf32, #blocked2> to tensor<16x32xbf16, #blocked2>
      %119 = ttg.convert_layout %22 : tensor<16xi32, #blocked3> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>
      %120 = tt.expand_dims %119 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>
      %121 = ttg.convert_layout %120 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked1>
      %122 = arith.muli %121, %cst_5 : tensor<16x1xi32, #blocked1>
      %123 = ttg.convert_layout %25 : tensor<32xi32, #blocked3> -> tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>>
      %124 = tt.expand_dims %123 {axis = 0 : i32} : tensor<32xi32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x32xi32, #blocked5>
      %125 = ttg.convert_layout %124 : tensor<1x32xi32, #blocked5> -> tensor<1x32xi32, #blocked2>
      %126 = tt.broadcast %122 : tensor<16x1xi32, #blocked1> -> tensor<16x32xi32, #blocked1>
      %127 = ttg.convert_layout %126 : tensor<16x32xi32, #blocked1> -> tensor<16x32xi32, #blocked2>
      %128 = tt.broadcast %125 : tensor<1x32xi32, #blocked2> -> tensor<16x32xi32, #blocked2>
      %129 = arith.addi %127, %128 : tensor<16x32xi32, #blocked2>
      %130 = tt.addptr %11, %129 : tensor<16x32x!tt.ptr<bf16>, #blocked2>, tensor<16x32xi32, #blocked2>
      tt.store %130, %118 : tensor<16x32x!tt.ptr<bf16>, #blocked2>
    } {tt.disallow_acc_multi_buffer, tt.num_stages = 4 : i32}
    tt.return
  }
}

{-#
  external_resources: {
    mlir_reproducer: {
      pipeline: "builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx942 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=2 use_async_copy=false use_pingpong=true}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tt.func(tritonamdgpu-in-thread-transpose), tritongpu-remove-layout-conversions, tritonamdgpu-reorder-instructions, tritonamdgpu-block-pingpong{num-stages=2}, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)",
      disable_threading: false,
      verify_each: true
    }
  }
#-}
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: error: Failures have been detected while processing an MLIR pass pipeline
/tmp/torchinductor_hotaisle/2t/c2tnk5c7ry44maomodgaqybhy62o6yzpx7satrue673qon3o57qw.py:18:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`
[816s] Triton compile failed. This likely indicates a bug in Triton. Skipping failing config.
Config: @helion.kernel(config=helion.Config(block_sizes=[16, 32, 512], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'pointer'], l2_groupings=[64], load_eviction_policies=['first', 'first', 'first'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[4, 0], range_unroll_factors=[0, 2], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)
Error: RuntimeError: PassManager::run failed
Enable HELION_AUTOTUNE_LOG_LEVEL=DEBUG to log generated Triton code.
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 235/235 17.6 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 106.5         
                                                                  configs/s     
[838s] Generation 19 complete: error=26 ok=214 min=0.0106 mid=0.0119 max=0.2487 best=Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=8, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[838s] Generation 20 starting: 230 neighbors, 5 active search path(s)
Generation 20: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 230/230 18.2 configs/s
Generation 20: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 230/230 17.6 configs/s
Generation 20: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 124.1         
                                                                  configs/s     
[879s] Generation 20 complete: error=21 ok=214 min=0.0101 mid=0.0118 max=0.2591 best=Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=8, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1)
[879s] Autotuning complete in 879.5s after searching 4641 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[16, 16, 512], indexing=['tensor_descriptor', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['last', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=8, num_warps=4, pid_type='persistent_interleaved', range_flattens=[None, None], range_multi_buffers=[True, False], range_num_stages=[3, 3], range_unroll_factors=[1, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 2:
(M, K, N)
----------------
(512, 1024, 128)
 60%|██████    | 3/5 [31:59<22:42, 681.03s/it]WARNING:tritonbench.utils.triton_op:Running input ID 3:
(M, K, N)
-----------------
(1024, 2048, 256)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.10ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_79", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4", "best_time": 0.013028999790549278, "best_triton_pos": 0}
AUTOTUNE mm(1024x2048, 2048x256)
strides: [2048, 1], [256, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_79 0.0130 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_83 0.0163 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_80 0.0164 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_77 0.0172 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=32, BLOCK_N=16, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=2
  triton_mm_81 0.0176 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=16, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
  triton_mm_85 0.0184 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_89 0.0190 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_88 0.0191 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_82 0.0218 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_84 0.0223 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.4039 seconds and 0.8055 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2275.73ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 241.30ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (1024, 2048),
              'stride': (2048, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048, 256),
              'stride': (256, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (256,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 10.08ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 606477989
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
[62s] Timeout after 60s compiling Config(block_sizes=[16, 8, 512], indexing=['pointer', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[16], load_eviction_policies=['last', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=7, num_warps=1, pid_type='persistent_blocked', range_flattens=[False, False], range_multi_buffers=[False, False], range_num_stages=[0, 2], range_unroll_factors=[4, 4], range_warp_specializes=[], waves_per_eu=1)
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━ 100/100 15.1 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7939.0 configs/s
[69s] Initial random population of 100, 5 starting points: error=27 timeout=1 ok=72 min=0.0361 mid=0.5533 max=90.3036 best=Config(block_sizes=[32, 16, 64], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=1, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[69s] Generation 1 starting: 202 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 20.7 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 17.4 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 308.9         
                                                                  configs/s     
[99s] Generation 1 complete: error=1 ok=206 min=0.0278 mid=0.0548 max=0.6707 best=Config(block_sizes=[32, 16, 64], indexing=['tensor_descriptor', 'tensor_descriptor', 'pointer', 'pointer'], l2_groupings=[32], load_eviction_policies=['', '', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=6, num_warps=2, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 1], range_unroll_factors=[0, 4], range_warp_specializes=[], waves_per_eu=1)
[99s] Generation 2 starting: 187 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 187/187 18.9 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 187/187 16.3 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 158.6         
                                                                  configs/s     
[132s] Generation 2 complete: error=1 ok=191 min=0.0243 mid=0.0370 max=0.5386 best=Config(block_sizes=[64, 16, 128], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[132s] Generation 3 starting: 174 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 174/174 18.3 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 174/174 16.9 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 165.7         
                                                                  configs/s     
[163s] Generation 3 complete: error=1 ok=178 min=0.0190 mid=0.0289 max=1.2473 best=Config(block_sizes=[32, 16, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[163s] Generation 4 starting: 199 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 199/199 17.9 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 199/199 16.9 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 107.4         
                                                                  configs/s     
[202s] Generation 4 complete: error=1 ok=203 min=0.0187 mid=0.0269 max=1.2160 best=Config(block_sizes=[32, 16, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[202s] Generation 5 starting: 202 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 13.7 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 202/202 16.9 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 97.1 configs/s
[246s] Generation 5 complete: error=3 ok=204 min=0.0184 mid=0.0241 max=1.2154 best=Config(block_sizes=[32, 16, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[246s] Generation 6 starting: 187 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 187/187 12.2 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 187/187 16.8 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 105.8         
                                                                  configs/s     
[288s] Generation 6 complete: error=2 ok=190 min=0.0172 mid=0.0226 max=2.1493 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[288s] Generation 7 starting: 190 neighbors, 5 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 190/190 9.9 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 190/190 16.9 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 118.1         
                                                                  configs/s     
[334s] Generation 7 complete: error=3 ok=192 min=0.0172 mid=0.0227 max=2.1505 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[334s] Generation 8 starting: 209 neighbors, 5 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 209/209 9.2 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 209/209 17.1 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 106.2         
                                                                  configs/s     
[384s] Generation 8 complete: error=5 ok=209 min=0.0172 mid=0.0227 max=2.3372 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[384s] Generation 9 starting: 200 neighbors, 5 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 200/200 13.9 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 200/200 17.2 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 107.7         
                                                                  configs/s     
[425s] Generation 9 complete: error=4 ok=201 min=0.0172 mid=0.0225 max=2.3412 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[425s] Generation 10 starting: 202 neighbors, 5 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 202/202 13.3 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 202/202 17.4 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 109.6         
                                                                  configs/s     
[467s] Generation 10 complete: error=8 ok=199 min=0.0173 mid=0.0227 max=2.3400 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, False], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[467s] Generation 11 starting: 206 neighbors, 5 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 8.3 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 206/206 17.3 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 110.4         
                                                                  configs/s     
[518s] Generation 11 complete: error=8 ok=203 min=0.0173 mid=0.0227 max=2.3353 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[518s] Generation 12 starting: 123 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 123/123 9.0 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 123/123 17.6 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 188.7         
                                                                  configs/s     
[548s] Generation 12 complete: error=6 ok=120 min=0.0170 mid=0.0199 max=2.1490 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[548s] Generation 13 starting: 84 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 84/84 11.7 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 84/84 17.9 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 289.3         
                                                                  configs/s     
[566s] Generation 13 complete: error=5 ok=82 min=0.0172 mid=0.0197 max=1.4421 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[566s] Generation 14 starting: 83 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83/83 6.1 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 83/83 18.0 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 295.1         
                                                                  configs/s     
[590s] Generation 14 complete: error=5 ok=81 min=0.0172 mid=0.0197 max=1.4435 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[590s] Generation 15 starting: 82 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 82/82 18.2 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 82/82 18.4 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 292.3         
                                                                  configs/s     
[604s] Generation 15 complete: error=5 ok=80 min=0.0172 mid=0.0197 max=1.4421 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[605s] Generation 16 starting: 42 neighbors, 1 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 42/42 18.9 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 42/42 19.1 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 548.1         
                                                                  configs/s     
[612s] Generation 16 complete: error=4 ok=40 min=0.0172 mid=0.0196 max=1.2633 best=Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[612s] Autotuning complete in 612.7s after searching 2671 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[16, 32, 256], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'first', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=2, pid_type='flat', range_flattens=[None, None], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 3:
(M, K, N)
-----------------
(1024, 2048, 256)
 80%|████████  | 4/5 [42:16<10:55, 655.90s/it]WARNING:tritonbench.utils.triton_op:Running input ID 4:
(M, K, N)
-----------------
(512, 2048, 4096)
INFO:tritonbench.utils.triton_op:Took 0.00ms to get benchmark function for torch_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.13ms to get benchmark function for mako_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.05ms to get benchmark function for kernelllm_gemm_gelu
Autotune Choices Stats:
{"num_choices": 36, "num_triton_choices": 36, "best_kernel": "triton_mm_134", "best_kernel_desc": "ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8", "best_time": 0.03199300169944763, "best_triton_pos": 0}
AUTOTUNE mm(512x2048, 2048x4096)
strides: [2048, 1], [4096, 1]
dtypes: torch.bfloat16, torch.bfloat16
  triton_mm_134 0.0320 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_129 0.0326 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_123 0.0332 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_140 0.0335 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_128 0.0350 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_139 0.0380 ms 84.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_133 0.0394 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=4
  triton_mm_124 0.0397 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=16, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_125 0.0411 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=256, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=0, num_stages=2, num_warps=8
  triton_mm_126 0.0412 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=4, USE_FAST_ACCUM=False, kpack=2, matrix_instr_nonkdim=16, waves_per_eu=2, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4904 seconds and 0.8163 seconds precompiling for 36 choices
INFO:tritonbench.utils.triton_op:Took 2600.23ms to get benchmark function for torch_compile_max_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 263.99ms to get benchmark function for torch_compile_default_gemm_gelu
INFO:tritonbench.utils.triton_op:Took 0.02ms to get benchmark function for triton_gemm_gelu_kernel
WARNING:__main__:Input tensor metadata:
{ 'args': ( { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (512, 2048),
              'stride': (2048, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (2048, 4096),
              'stride': (4096, 1)},
            { 'device': 'cuda:0',
              'dtype': 'torch.bfloat16',
              'shape': (4096,),
              'stride': (1,)}),
  'kwargs': {}}
INFO:tritonbench.utils.triton_op:Took 13.50ms to get benchmark function for helion_gemm_gelu_tritonbench
[0s] Autotune random seed: 606477989
[0s] Starting PatternSearch with initial_population=100, copies=5, max_generations=20
Initial population precompiling 100% ━━━━━━━━━━━━━━━━━━━━━ 100/100 0.1 configs/s
Initial population exploring neighbors 100% ━━━━━━━━━━━━━━ 100/100 4.1 configs/s
Verifying initial results 100% ━━━━━━━━━━━━━━━━━━━━━━ 1000/1000 7579.5 configs/s
[71s] Initial random population of 100, 5 starting points: error=22 ok=78 min=0.0572 mid=2.2056 max=738.3278 best=Config(block_sizes=[32, 256, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[71s] Generation 1 starting: 206 neighbors, 5 active search path(s)
Generation 1: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 16.8 configs/s
Generation 1: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 206/206 17.0 configs/s
Generation 1: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 289.8         
                                                                  configs/s     
[105s] Generation 1 complete: error=2 ok=209 min=0.0422 mid=0.0943 max=8.5850 best=Config(block_sizes=[32, 128, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 1], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[105s] Generation 2 starting: 175 neighbors, 5 active search path(s)
Generation 2: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 175/175 12.9 configs/s
Generation 2: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 175/175 17.5 configs/s
Generation 2: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 197.4         
                                                                  configs/s     
[138s] Generation 2 complete: error=3 ok=177 min=0.0414 mid=0.0672 max=3.7033 best=Config(block_sizes=[32, 128, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[138s] Generation 3 starting: 186 neighbors, 5 active search path(s)
Generation 3: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 186/186 14.2 configs/s
Generation 3: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 186/186 18.4 configs/s
Generation 3: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 139.1         
                                                                  configs/s     
[175s] Generation 3 complete: error=12 ok=179 min=0.0392 mid=0.0627 max=5.9108 best=Config(block_sizes=[64, 64, 64], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[175s] Generation 4 starting: 189 neighbors, 5 active search path(s)
Generation 4: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 189/189 15.7 configs/s
Generation 4: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 189/189 17.6 configs/s
Generation 4: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 137.5         
                                                                  configs/s     
[211s] Generation 4 complete: error=3 ok=191 min=0.0377 mid=0.0558 max=0.3764 best=Config(block_sizes=[64, 64, 128], indexing=['tensor_descriptor', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[2], load_eviction_policies=['first', 'last', 'last'], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=2, num_warps=4, pid_type='flat', range_flattens=[None, True], range_multi_buffers=[None, True], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[211s] Generation 5 starting: 191 neighbors, 5 active search path(s)
Generation 5: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 191/191 15.3 configs/s
Generation 5: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 191/191 18.3 configs/s
Generation 5: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 138.1         
                                                                  configs/s     
[246s] Generation 5 complete: error=11 ok=185 min=0.0358 mid=0.0557 max=0.3705 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', ''], loop_orders=[[0, 1]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[246s] Generation 6 starting: 206 neighbors, 5 active search path(s)
Generation 6: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 206/206 14.9 configs/s
Generation 6: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 206/206 18.6 configs/s
Generation 6: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 149.2         
                                                                  configs/s     
[284s] Generation 6 complete: error=13 ok=198 min=0.0349 mid=0.0544 max=0.3703 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='flat', range_flattens=[None, False], range_multi_buffers=[None, None], range_num_stages=[0, 2], range_unroll_factors=[0, 0], range_warp_specializes=[], waves_per_eu=1)
[284s] Generation 7 starting: 162 neighbors, 4 active search path(s)
Generation 7: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 162/162 9.8 configs/s
Generation 7: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 162/162 18.7 configs/s
Generation 7: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 152.0         
                                                                  configs/s     
[320s] Generation 7 complete: error=13 ok=153 min=0.0346 mid=0.0531 max=0.2292 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[2, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[320s] Generation 8 starting: 170 neighbors, 4 active search path(s)
Generation 8: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 170/170 7.2 configs/s
Generation 8: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 170/170 18.7 configs/s
Generation 8: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 140.2         
                                                                  configs/s     
[365s] Generation 8 complete: error=13 ok=161 min=0.0340 mid=0.0525 max=0.2294 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'pointer', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[3, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[365s] Generation 9 starting: 130 neighbors, 3 active search path(s)
Generation 9: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 130/130 17.0 configs/s
Generation 9: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━ 130/130 19.6 configs/s
Generation 9: verifying top configs 100% ━━━━━━━━━━━━━━ 1000/1000 136.8         
                                                                  configs/s     
[391s] Generation 9 complete: error=12 ok=121 min=0.0344 mid=0.0467 max=0.1315 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', '', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[3, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[391s] Generation 10 starting: 130 neighbors, 3 active search path(s)
Generation 10: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 130/130 16.9 configs/s
Generation 10: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 130/130 19.6 configs/s
Generation 10: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 136.8         
                                                                  configs/s     
[417s] Generation 10 complete: error=12 ok=121 min=0.0344 mid=0.0467 max=0.1316 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[3, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[417s] Generation 11 starting: 129 neighbors, 3 active search path(s)
Generation 11: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━ 129/129 6.1 configs/s
Generation 11: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 129/129 19.1 configs/s
Generation 11: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 138.3         
                                                                  configs/s     
[455s] Generation 11 complete: error=12 ok=120 min=0.0340 mid=0.0465 max=0.1320 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[455s] Generation 12 starting: 126 neighbors, 3 active search path(s)
Generation 12: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━ 126/126 16.6 configs/s
Generation 12: exploring neighbors 100% ━━━━━━━━━━━━━━━━━ 126/126 19.7 configs/s
Generation 12: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 143.7         
                                                                  configs/s     
[480s] Generation 12 complete: error=12 ok=117 min=0.0344 mid=0.0465 max=0.1319 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[480s] Generation 13 starting: 77 neighbors, 2 active search path(s)
Generation 13: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77/77 5.4 configs/s
Generation 13: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 77/77 18.7 configs/s
Generation 13: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 241.0         
                                                                  configs/s     
[504s] Generation 13 complete: error=4 ok=76 min=0.0344 mid=0.0492 max=0.1317 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[504s] Generation 14 starting: 81 neighbors, 2 active search path(s)
Generation 14: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 17.0 configs/s
Generation 14: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 81/81 18.6 configs/s
Generation 14: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 221.4         
                                                                  configs/s     
[521s] Generation 14 complete: error=4 ok=80 min=0.0344 mid=0.0485 max=0.1321 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[521s] Generation 15 starting: 78 neighbors, 2 active search path(s)
Generation 15: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78/78 5.3 configs/s
Generation 15: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 78/78 18.7 configs/s
Generation 15: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 234.1         
                                                                  configs/s     
[545s] Generation 15 complete: error=4 ok=77 min=0.0343 mid=0.0489 max=0.1327 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[545s] Generation 16 starting: 81 neighbors, 2 active search path(s)
Generation 16: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81/81 5.5 configs/s
Generation 16: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 81/81 18.6 configs/s
Generation 16: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 222.0         
                                                                  configs/s     
[571s] Generation 16 complete: error=4 ok=80 min=0.0344 mid=0.0484 max=0.1325 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[571s] Generation 17 starting: 79 neighbors, 2 active search path(s)
Generation 17: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 79/79 16.0 configs/s
Generation 17: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 79/79 19.1 configs/s
Generation 17: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 230.0         
                                                                  configs/s     
[586s] Generation 17 complete: error=4 ok=78 min=0.0344 mid=0.0490 max=0.1325 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[587s] Generation 18 starting: 41 neighbors, 1 active search path(s)
Generation 18: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 41/41 14.4 configs/s
Generation 18: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 41/41 18.4 configs/s
Generation 18: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 452.5         
                                                                  configs/s     
[596s] Generation 18 complete: error=2 ok=41 min=0.0344 mid=0.0430 max=0.1321 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[596s] Generation 19 starting: 40 neighbors, 1 active search path(s)
Generation 19: precompiling 100% ━━━━━━━━━━━━━━━━━━━━━━━━━━ 40/40 16.7 configs/s
Generation 19: exploring neighbors 100% ━━━━━━━━━━━━━━━━━━━ 40/40 19.2 configs/s
Generation 19: verifying top configs 100% ━━━━━━━━━━━━━ 1000/1000 467.2         
                                                                  configs/s     
[604s] Generation 19 complete: error=2 ok=40 min=0.0344 mid=0.0441 max=0.1325 best=Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1)
[604s] Autotuning complete in 604.7s after searching 2577 configs.
One can hardcode the best config and skip autotuning with:
    @helion.kernel(config=helion.Config(block_sizes=[128, 64, 128], indexing=['pointer', 'pointer', 'tensor_descriptor', 'tensor_descriptor'], l2_groupings=[32], load_eviction_policies=['', 'last', ''], loop_orders=[[1, 0]], matrix_instr_nonkdim=0, num_stages=3, num_warps=8, pid_type='persistent_interleaved', range_flattens=[False, False], range_multi_buffers=[True, None], range_num_stages=[4, 2], range_unroll_factors=[4, 0], range_warp_specializes=[], waves_per_eu=1), static_shapes=True)

WARNING:tritonbench.utils.triton_op:Completed input ID 4:
(M, K, N)
-----------------
(512, 2048, 4096)
100%|██████████| 5/5 [52:26<00:00, 639.24s/it]100%|██████████| 5/5 [52:26<00:00, 629.28s/it]
INFO:tritonbench.utils.run_utils:[tritonbench] Output result csv to /tmp/tmp1j9iryem.csv
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
INFO:__main__:ignoring torch_gemm_gelu-latency
INFO:__main__:ignoring torch_gemm_gelu-tflops
INFO:__main__:ignoring mako_gemm_gelu-latency
INFO:__main__:ignoring kernelllm_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_max_gemm_gelu-latency
INFO:__main__:ignoring torch_compile_default_gemm_gelu-latency
INFO:__main__:ignoring triton_gemm_gelu_kernel-latency
INFO:__main__:ignoring helion_gemm_gelu_tritonbench-latency
        (M, K, N)    torch_gemm_gelu-latency    torch_gemm_gelu-tflops    mako_gemm_gelu-latency    mako_gemm_gelu-accuracy    mako_gemm_gelu-speedup    mako_gemm_gelu-tflops    kernelllm_gemm_gelu-latency    kernelllm_gemm_gelu-accuracy    kernelllm_gemm_gelu-speedup    kernelllm_gemm_gelu-tflops    torch_compile_max_gemm_gelu-latency    torch_compile_max_gemm_gelu-accuracy    torch_compile_max_gemm_gelu-speedup    torch_compile_max_gemm_gelu-tflops    torch_compile_default_gemm_gelu-latency    torch_compile_default_gemm_gelu-accuracy    torch_compile_default_gemm_gelu-speedup    torch_compile_default_gemm_gelu-tflops    triton_gemm_gelu_kernel-latency    triton_gemm_gelu_kernel-accuracy    triton_gemm_gelu_kernel-speedup    triton_gemm_gelu_kernel-tflops    helion_gemm_gelu_tritonbench-latency    helion_gemm_gelu_tritonbench-accuracy    helion_gemm_gelu_tritonbench-speedup    helion_gemm_gelu_tritonbench-tflops
-----------------  -------------------------  ------------------------  ------------------------  -------------------------  ------------------------  -----------------------  -----------------------------  ------------------------------  -----------------------------  ----------------------------  -------------------------------------  --------------------------------------  -------------------------------------  ------------------------------------  -----------------------------------------  ------------------------------------------  -----------------------------------------  ----------------------------------------  ---------------------------------  ----------------------------------  ---------------------------------  --------------------------------  --------------------------------------  ---------------------------------------  --------------------------------------  -------------------------------------
     (32, 64, 16)         0.015516 (±58.39%)                0.00438876        0.068676 (±33.57%)                          1                  0.22593               0.000991555             0.037846 (±15.25%)                               1                       0.409977                    0.00179929                     0.054845 (±13.01%)                                       1                               0.282906                            0.00124161                          0.086717 (±8.46%)                                           1                                   0.178927                               0.000785267                 0.030550 (±15.09%)                                   1                           0.507889                          0.002229                      0.038447 (±14.60%)                                      0                                  0.403569                             0.00177117
   (128, 256, 64)         0.014192 (±21.48%)                0.298426          0.065468 (±11.33%)                          1                  0.216778              0.0646921               0.041253 (±14.87%)                               1                       0.344023                    0.102666                       0.058693 (±13.25%)                                       1                               0.241801                            0.0721596                          0.089082 (±10.89%)                                           1                                   0.159314                               0.0475434                   0.032514 (±15.41%)                                   1                           0.436489                          0.13026                       0.045022 (±12.20%)                                      1                                  0.315224                             0.094071
 (512, 1024, 128)         0.016558 (±17.43%)                8.1257            0.063824 (±11.12%)                          1                  0.259432              2.10807                 0.041735 (±14.41%)                               1                       0.396741                    3.2238                         0.059936 (±12.78%)                                       1                               0.276261                            2.24482                            0.090606 (±10.13%)                                           1                                   0.182747                               1.48495                     0.032233 (±13.93%)                                   1                           0.513697                          4.17415                       0.044662 (±12.48%)                                      1                                  0.37074                              3.01253
(1024, 2048, 256)          0.023653 (±7.80%)               45.451             0.064347 (±10.03%)                          1                  0.367585             16.7071                  0.041855 (±13.79%)                               1                       0.565118                   25.6852                         0.059375 (±13.10%)                                       1                               0.398366                           18.1061                             0.089804 (±10.18%)                                           1                                   0.263385                              11.9711                      0.037686 (±12.98%)                                   1                           0.627634                         28.5266                        0.046706 (±10.47%)                                      1                                  0.506423                            23.0174
(512, 2048, 4096)          0.047308 (±3.81%)              181.796             0.071241 (±12.04%)                          1                  0.664056            120.723                   0.040011 (±14.73%)                               1                       1.18237                   214.951                          0.055607 (±15.93%)                                       1                               0.850756                          154.664                              0.088722 (±10.80%)                                           1                                   0.533216                              96.9367                       0.045584 (±4.75%)                                   1                           1.03782                         188.672                         0.050434 (±13.51%)                                      1                                  0.938018                           170.528
          average       0.023445400595664977               47.1352           0.06671119928359985                          1                  0.346756             27.9208                0.040540000051259996                               1                       0.579647                   48.793                        0.057691199332475664                                       1                               0.410018                           35.0177                            0.08898620009422302                                           1                                   0.263518                              22.0882                     0.03571339994668961                                   1                           0.624706                         44.301                       0.045054199546575545                                      0.8                                0.506795                            39.3308
INFO:__main__:Benchmark run completed
INFO:__main__:Post-processing 3 output file(s)...
INFO:__main__:Processing output for benchmark 'bf16_layernorm': benchmark_bf16_layernorm.json
INFO:__main__:Processing results from: /home/hotaisle/KernelGeneration/helion/benchmark_bf16_layernorm.json
INFO:__main__:Successfully moved output to: /home/hotaisle/KernelGeneration/results/AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-_20251215_180748/benchmark_bf16_layernorm_AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-.json
INFO:__main__:Processing output for benchmark 'bf16_matmul': benchmark_bf16_matmul.json
INFO:__main__:Processing results from: /home/hotaisle/KernelGeneration/helion/benchmark_bf16_matmul.json
INFO:__main__:Successfully moved output to: /home/hotaisle/KernelGeneration/results/AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-_20251215_180748/benchmark_bf16_matmul_AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-.json
INFO:__main__:Processing output for benchmark 'bf16_gemm_gelu': benchmark_bf16_gemm_gelu.json
INFO:__main__:Processing results from: /home/hotaisle/KernelGeneration/helion/benchmark_bf16_gemm_gelu.json
INFO:__main__:Successfully moved output to: /home/hotaisle/KernelGeneration/results/AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-_20251215_180748/benchmark_bf16_gemm_gelu_AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-.json
INFO:__main__:==================================================
INFO:__main__:POST-PROCESSING SUMMARY
INFO:__main__:==================================================
INFO:__main__:Total: 3 | Success: 3 | Failed: 0
INFO:__main__:Successful:
INFO:__main__:  ✓ bf16_layernorm -> /home/hotaisle/KernelGeneration/results/AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-_20251215_180748/benchmark_bf16_layernorm_AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-.json
INFO:__main__:  ✓ bf16_matmul -> /home/hotaisle/KernelGeneration/results/AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-_20251215_180748/benchmark_bf16_matmul_AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-.json
INFO:__main__:  ✓ bf16_gemm_gelu -> /home/hotaisle/KernelGeneration/results/AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-_20251215_180748/benchmark_bf16_gemm_gelu_AMD_Instinct_MI300X_VF_gfx942_sramecc_xnack-.json
